[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GRÓ-FTP ARAM - Population Models",
    "section": "",
    "text": "For this part of the GRO-FTP ARAM line, you are expected to already have some fluency in using Rstudio and tidyverse. At this point you should also start thinking of how you plan to organize your files and work into a project, or even start using Rmarkdown files to take notes and do exploratory studies and plots."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "GRÓ-FTP ARAM - Population Models",
    "section": "",
    "text": "For this part of the GRO-FTP ARAM line, you are expected to already have some fluency in using Rstudio and tidyverse. At this point you should also start thinking of how you plan to organize your files and work into a project, or even start using Rmarkdown files to take notes and do exploratory studies and plots."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "GRÓ-FTP ARAM - Population Models",
    "section": "Schedule",
    "text": "Schedule\nThe population dynamics section of teaching is split into roughly 8 half-day sessions:\nSimple population models\nReview-what are models?\nFitting a logistic growth curve\nSurplus production models\nFitting a surplus production model\nSPiCT exercise\n[Components of an age-based model 1](\n[Components of an age-based model 2]\n[Age-based models 1]\n[Age-based models 2]"
  },
  {
    "objectID": "Fitting_logistic_curve.html",
    "href": "Fitting_logistic_curve.html",
    "title": "Fitting a logistic curve",
    "section": "",
    "text": "Remember this equation from the Simple Population Models section?\n\\(N_t = N_{t-1} + rN_{t-1}(1-N_{t-1}/K)\\)\nLet’s review.\n\nWhat is the functional form?\nWhat are the parameters?\nWhat does each parameter control?\n\n\n\nIn this section, building on the basics we have learned on how statistical models are fit, we are going to fit a non-linear model. The model describes logistic growth of an elephant population. Note: here we use sum of squares as our objective function, which is not the best way to fit a non-linear model, but is used for demonstration.\nTake a look at this website below to get familiar with the system. Then continue with the exercise.\n\n\n\n\nYrs&lt;-c(1903, 1905, 1907, 1926, 1928, 1932, 1935, 1939, 1946, 1948, 1961, 1964, 1967, 1970, 1972, 1974, 1976, 1979, 1982, 1985, 1988, 1990, 1993, 1996)\nData&lt;-c(12, 15, 17, 25, 27, 30, 37, 42, 52, 108, 156, 219, 7563, 8642, 7378, 7245, 6983, 7367, 7689, 8503, 7654, 6803, 7204, 7343)\nlength(Yrs); length(Data)\n\n[1] 24\n\n\n[1] 24\n\n#pars, Years, and Numbers are vector arguments\nLogisticGrowthModel&lt;-function(pars, Years, Numbers, ReturnResults=F){\n  \n  #print(Years)\n  \n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]#assign parameters\n  \n  TT&lt;-Years[1]:Years[length(Years)] #stretch out years within model so no missing years\n\n  #print(TT)\n  \n  PP&lt;-vector(mode = \"numeric\", length = length(TT));  #empty population size vector\n  \n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); #empty population growth vector\n  \n  II&lt;-rr*BB*(1-BB/KK) #initial growth to reach year 1 from year 0\n  \n  PP[1]&lt;-BB+II # population assignment at year 1, based on parameter BB (initial population)\n  \n  for(i in 2:length(PP)){ \n    \n    DD[i-1] &lt;- rr*PP[i-1]*(1-PP[i-1]/KK); #this is dN/dt - amount the population grows based\n    #on conditions in the previous time step\n    \n    PP[i] &lt;- PP[i-1] + DD[i-1] #update population size\n    \n    } \n\n  #print(DD)\n  #print(PP)\n  \n   Predictions&lt;-data.frame(Years = TT, Pred = PP); #create predictions\n   \n   Observations&lt;-data.frame(Years, Numbers) #store observations in data frame\n   \n   Results&lt;-merge(Predictions, Observations, by = \"Years\") #match Predictions and Observations by Year\n  \n   SS&lt;-sum((Results$Numbers - Results$Pred)^2) #calculate sum of squares: sum((obs - pred)^2)\n  \n   #print('Results')\n   #print(Results)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(SS)}\n   #controls options for what is returned from function\n\n   }\n\n#LogisticGrowthModel(c(7500,0.5), Yrs, Data, t=0, ReturnResults=T) ERROR\nLogisticGrowthModel(c(7500,0.5,1), Years=Yrs, Numbers=Data, ReturnResults=T)\n\n[[1]]\n   Years        Pred\n1   1903    1.499933\n2   1904    2.249750\n3   1905    3.374288\n4   1906    5.060672\n5   1907    7.589301\n6   1908   11.380112\n7   1909   17.061534\n8   1910   25.572895\n9   1911   38.315744\n10  1912   57.375743\n11  1913   85.844149\n12  1914  128.274942\n13  1915  191.315449\n14  1916  284.533067\n15  1917  421.402329\n16  1918  620.264832\n17  1919  904.748684\n18  1920 1302.551681\n19  1921 1840.718129\n20  1922 2535.194312\n21  1923 3374.310788\n22  1924 4302.401296\n23  1925 5219.558149\n24  1926 6013.084739\n25  1927 6609.147903\n26  1928 7001.666121\n27  1929 7234.277284\n28  1930 7362.431404\n29  1931 7429.954028\n30  1932 7464.649918\n31  1933 7482.241650\n32  1934 7491.099801\n33  1935 7495.544620\n34  1936 7497.770987\n35  1937 7498.885162\n36  1938 7499.442498\n37  1939 7499.721228\n38  1940 7499.860609\n39  1941 7499.930303\n40  1942 7499.965151\n41  1943 7499.982576\n42  1944 7499.991288\n43  1945 7499.995644\n44  1946 7499.997822\n45  1947 7499.998911\n46  1948 7499.999455\n47  1949 7499.999728\n48  1950 7499.999864\n49  1951 7499.999932\n50  1952 7499.999966\n51  1953 7499.999983\n52  1954 7499.999991\n53  1955 7499.999996\n54  1956 7499.999998\n55  1957 7499.999999\n56  1958 7499.999999\n57  1959 7500.000000\n58  1960 7500.000000\n59  1961 7500.000000\n60  1962 7500.000000\n61  1963 7500.000000\n62  1964 7500.000000\n63  1965 7500.000000\n64  1966 7500.000000\n65  1967 7500.000000\n66  1968 7500.000000\n67  1969 7500.000000\n68  1970 7500.000000\n69  1971 7500.000000\n70  1972 7500.000000\n71  1973 7500.000000\n72  1974 7500.000000\n73  1975 7500.000000\n74  1976 7500.000000\n75  1977 7500.000000\n76  1978 7500.000000\n77  1979 7500.000000\n78  1980 7500.000000\n79  1981 7500.000000\n80  1982 7500.000000\n81  1983 7500.000000\n82  1984 7500.000000\n83  1985 7500.000000\n84  1986 7500.000000\n85  1987 7500.000000\n86  1988 7500.000000\n87  1989 7500.000000\n88  1990 7500.000000\n89  1991 7500.000000\n90  1992 7500.000000\n91  1993 7500.000000\n92  1994 7500.000000\n93  1995 7500.000000\n94  1996 7500.000000\n\n[[2]]\n[1] 471423138\n\n\nNow we would like to fit this model by minimizing the objective function using algorithms implemented in the ‘optim’ function.\n\n?optim\n# this function tries different parameter values to minimize the output of \n#LogisticGrowthModel. The output is the sum of squares.\n\n\nOptPars1&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000))\nOptPars1\n\n$par\n[1] 8040.6746970    0.1581262    0.8000000\n\n$value\n[1] 42519063\n\n$counts\nfunction gradient \n      60       60 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\"\n\nOptPars2&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, Years = Yrs, Numbers = Data, control = list(maxit = 50000))\nOptPars2\n\n$par\n[1] 7.529675e+03 1.835725e+00 5.138202e-26\n\n$value\n[1] 3428472\n\n$counts\nfunction gradient \n    2531       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\nPredictions1&lt;-LogisticGrowthModel(OptPars1$par, Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModel(OptPars2$par, Yrs, Data, ReturnResults=T)[[1]]\n\n#tibble(Yrs, Data) %&gt;% \n#  ggplot(aes(Yrs, Data)) + geom_point(shape = '*', color = 'red', size = 5) + ylim(0,8000)\n\n\ntibble(Yrs, Data) %&gt;% \n  ggplot(aes(Yrs, Data)) + geom_point(shape = '*', color = 'red', size = 5) + ylim(0,8000) + geom_line(aes(Years, Pred), data = Predictions1) + geom_line(aes(Years, Pred), data = Predictions2, linetype = 2)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n# Now try to figure out the best time lag as shown in the on-line example. \n# Hint: plot SS as it relates to the time lag value. SS is your sums of \n# squares, so the value with the lowest sum of squares is the one you want. \n# Try lag values of 0 - 5. To include a lag, use the next function instead. \n# Note that there is now an additional parameter for lag t.\n\nLogisticGrowthModelLag&lt;-function(pars, Years, Numbers, t=0, ReturnResults=F){\n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]\n  TT&lt;-Years[1]:Years[length(Years)]\n  PP&lt;-vector(mode = \"numeric\", length = length(TT));\n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); \n  II&lt;-rr*BB*(1-BB/KK)\n  PP[1]&lt;-BB+II\n  \n  #implementing Logistic Growth for Years with no lagged Numbers data\n  if(t&gt;0){\n    for(i in 2:(t+1)){ \n      DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1]/KK);\n      PP[i]&lt;-PP[i-1] + DD[i-1]\n    }\n  }\n  \n  #implementing Lagged growth\n  for(i in (t+2):length(PP)){ \n    DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1-t]/KK); \n    PP[i]&lt;-PP[i-1] + DD[i-1]\n  }\n  \n  #print(PP) \n  Predictions&lt;-data.frame(Years = TT, Pred = PP); Observations&lt;-data.frame(Years, Numbers)\n  Results&lt;-merge(Predictions, Observations, by = \"Years\")\n  SS&lt;-sum((Results$Numbers - Results$Pred)^2)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(ifelse(is.infinite(SS), 10000000000, SS))}\n}\n\nLogisticGrowthModelLag(OptPars1$par, t=3,Yrs, Data, ReturnResults=T)\n\n[[1]]\n   Years         Pred\n1   1903    0.9264883\n2   1904    1.0729735\n3   1905    1.2426160\n4   1906    1.4390758\n5   1907    1.6666051\n6   1908    1.9301038\n7   1909    2.2352565\n8   1910    2.5886458\n9   1911    2.9978936\n10  1912    3.4718252\n11  1913    4.0206589\n12  1914    4.6562256\n13  1915    5.3922222\n14  1916    6.2445054\n15  1917    7.2314313\n16  1918    8.3742476\n17  1919    9.6975471\n18  1920   11.2297921\n19  1921   13.0039190\n20  1922   15.0580372\n21  1923   17.4362351\n22  1924   20.1895094\n23  1925   23.3768358\n24  1926   27.0664026\n25  1927   31.3370279\n26  1928   36.2797896\n27  1929   41.9998947\n28  1930   48.6188210\n29  1931   56.2767662\n30  1932   65.1354433\n31  1933   75.3812615\n32  1934   87.2289368\n33  1935  100.9255750\n34  1936  116.7552690\n35  1937  135.0442496\n36  1938  156.1666198\n37  1939  180.5506912\n38  1940  208.6859191\n39  1941  241.1304043\n40  1942  278.5188841\n41  1943  321.5710768\n42  1944  371.1001575\n43  1945  428.0210365\n44  1946  493.3579631\n45  1947  568.2507935\n46  1948  653.9590291\n47  1949  751.8624446\n48  1950  863.4567927\n49  1951  990.3426863\n50  1952 1134.2053549\n51  1953 1296.7825730\n52  1954 1479.8177352\n53  1955 1684.9949017\n54  1956 1913.8528027\n55  1957 2167.6754551\n56  1958 2447.3584494\n57  1959 2753.2523534\n58  1960 3084.9882786\n59  1961 3441.2955571\n60  1962 3819.8275401\n61  1963 4217.0181645\n62  1964 4627.9980060\n63  1965 5046.6022290\n64  1966 5465.5018160\n65  1967 5876.4813051\n66  1968 6270.8693448\n67  1969 6640.1030097\n68  1970 6976.3763500\n69  1971 7273.2948096\n70  1972 7526.4389775\n71  1973 7733.7422313\n72  1974 7895.6114652\n73  1975 8014.7651322\n74  1976 8095.8172321\n75  1977 8144.6841282\n76  1978 8167.9191361\n77  1979 8172.0809512\n78  1980 8163.2189742\n79  1981 8146.5217069\n80  1982 8126.1361650\n81  1983 8105.1365485\n82  1984 8085.6037441\n83  1985 8068.7730403\n84  1986 8055.2121226\n85  1987 8045.0005886\n86  1988 8037.8923087\n87  1989 8033.4507631\n88  1990 8031.1540810\n89  1991 8030.4708544\n90  1992 8030.9102644\n91  1993 8032.0511701\n92  1994 8033.5550140\n93  1995 8035.1670798\n94  1996 8036.7100344\n\n[[2]]\n[1] 41903227\n\nLogisticGrowthModelLag(OptPars2$par, t=3,Yrs, Data, ReturnResults=T)\n\n[[1]]\n   Years           Pred\n1   1903   1.457053e-25\n2   1904   4.131800e-25\n3   1905   1.171665e-24\n4   1906   3.322518e-24\n5   1907   9.421746e-24\n6   1908   2.671748e-23\n7   1909   7.576340e-23\n8   1910   2.148441e-22\n9   1911   6.092388e-22\n10  1912   1.727633e-21\n11  1913   4.899092e-21\n12  1914   1.389248e-20\n13  1915   3.939524e-20\n14  1916   1.117140e-19\n15  1917   3.167902e-19\n16  1918   8.983298e-19\n17  1919   2.547416e-18\n18  1920   7.223770e-18\n19  1921   2.048462e-17\n20  1922   5.808874e-17\n21  1923   1.647237e-16\n22  1924   4.671110e-16\n23  1925   1.324598e-15\n24  1926   3.756195e-15\n25  1927   1.065153e-14\n26  1928   3.020482e-14\n27  1929   8.565254e-14\n28  1930   2.428870e-13\n29  1931   6.887606e-13\n30  1932   1.953135e-12\n31  1933   5.538554e-12\n32  1934   1.570581e-11\n33  1935   4.453736e-11\n34  1936   1.262957e-10\n35  1937   3.581398e-10\n36  1938   1.015586e-09\n37  1939   2.879921e-09\n38  1940   8.166664e-09\n39  1941   2.315841e-08\n40  1942   6.567087e-08\n41  1943   1.862245e-07\n42  1944   5.280813e-07\n43  1945   1.497493e-06\n44  1946   4.246478e-06\n45  1947   1.204184e-05\n46  1948   3.414735e-05\n47  1949   9.683247e-05\n48  1950   2.745902e-04\n49  1951   7.786622e-04\n50  1952   2.208071e-03\n51  1953   6.261482e-03\n52  1954   1.775584e-02\n53  1955   5.035066e-02\n54  1956   1.427806e-01\n55  1957   4.048862e-01\n56  1958   1.148144e+00\n57  1959   3.255806e+00\n58  1960   9.232455e+00\n59  1961   2.617979e+01\n60  1962   7.423134e+01\n61  1963   2.104407e+02\n62  1964   5.962782e+02\n63  1965   1.687075e+03\n64  1966   4.753548e+03\n65  1967   1.323587e+04\n66  1968   3.560916e+04\n67  1969   8.633149e+04\n68  1970   1.447620e+05\n69  1971  -5.662549e+04\n70  1972   3.310179e+05\n71  1973  -6.028423e+06\n72  1974   1.956649e+08\n73  1975   3.256048e+09\n74  1976  -2.535354e+11\n75  1977  -3.733454e+14\n76  1978   1.780857e+19\n77  1979  -1.413675e+25\n78  1980  -8.738152e+32\n79  1981  -7.953561e+43\n80  1982   3.453203e+59\n81  1983   1.190154e+81\n82  1984  2.535443e+110\n83  1985  4.916395e+150\n84  1986 -4.139045e+206\n85  1987  1.200976e+284\n86  1988           -Inf\n87  1989            NaN\n88  1990            NaN\n89  1991            NaN\n90  1992            NaN\n91  1993            NaN\n92  1994            NaN\n93  1995            NaN\n94  1996            NaN\n\n[[2]]\n[1] NaN\n\nSS1&lt;-SS2&lt;-vector(mode = \"numeric\", length = 7)\nOptPars1_list&lt;-OptPars2_list&lt;-NULL\n\nfor(i in 0:6){\n  OptPars1_list[[i+1]]&lt;-try(optim(OptPars1$par, fn=LogisticGrowthModelLag, t=i, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000)), silent=T)\n  OptPars2_list[[i+1]]&lt;-try(optim(OptPars2$par, fn=LogisticGrowthModelLag, t=i, Years = Yrs, Numbers = Data, control = list(maxit = 50000)),silent=T)\n  SS1[i+1]&lt;-try(OptPars1_list[[i+1]]$value/1000000,silent=T )\n  SS2[i+1]&lt;-try(OptPars2_list[[i+1]]$value/1000000,silent=T )\n}\n\nSS1 \n\n[1] 42.51906 40.51695 38.37475 36.33096 34.87183 34.88024 37.08259\n\nSS2 \n\n[1] \"3.42846793085808\"                                                                      \n[2] \"7.36469949321783\"                                                                      \n[3] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[4] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[5] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[6] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[7] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n\n\n\n#Q - The time lag corresponding with the minimum SS is the best. What is time lag is that?\n#That is, which would you choose if the best model by least squares (SS)?\n\n\n\ntibble(lags=c(0:6), SS=SS1) %&gt;% \n  mutate(type = 'SS1') %&gt;% \n  bind_rows(tibble(lags=c(0:6), SS=ifelse(grepl('Error', SS2), NA_real_, as.numeric(SS2))) %&gt;% \n  mutate(type = 'SS2')) %&gt;% \n  group_by(type) %&gt;% \n  ggplot(aes(lags, SS)) + geom_point() + facet_wrap(~type)\n\nWarning in ifelse(grepl(\"Error\", SS2), NA_real_, as.numeric(SS2)): NAs\nintroduced by coercion\n\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n#Q - BUT, this is what happens with lag = 5. Why? (Compare plots with lag = 0 - 4)\n\nOptPars&lt;-NULL\n\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=5, Years = Yrs, Numbers = Data)\n\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=5,Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModelLag(c(OptPars1_list[[6]]$par), t=6-1,Yrs, Data, ReturnResults=T)[[1]]\n\nL5 &lt;- tibble(Yrs, Data) %&gt;% ggplot(aes(Yrs, Data)) + geom_point(shape = \"*\", color = \"red\", size = 5) + ylim(-1,9000) + geom_line(aes(Years, Pred), data = Predictions, lty = 2) + ggtitle('Lag 5') + geom_line(aes(Years, Pred), data = Predictions2, lty = 1)\n\nLogisticGrowthModelLag(OptPars2$par, t=5,Yrs, Data, ReturnResults=T)\n\n[[1]]\n   Years           Pred\n1   1903   1.457053e-25\n2   1904   4.131800e-25\n3   1905   1.171665e-24\n4   1906   3.322518e-24\n5   1907   9.421746e-24\n6   1908   2.671748e-23\n7   1909   7.576340e-23\n8   1910   2.148441e-22\n9   1911   6.092388e-22\n10  1912   1.727633e-21\n11  1913   4.899092e-21\n12  1914   1.389248e-20\n13  1915   3.939524e-20\n14  1916   1.117140e-19\n15  1917   3.167902e-19\n16  1918   8.983298e-19\n17  1919   2.547416e-18\n18  1920   7.223770e-18\n19  1921   2.048462e-17\n20  1922   5.808874e-17\n21  1923   1.647237e-16\n22  1924   4.671110e-16\n23  1925   1.324598e-15\n24  1926   3.756195e-15\n25  1927   1.065153e-14\n26  1928   3.020482e-14\n27  1929   8.565254e-14\n28  1930   2.428870e-13\n29  1931   6.887606e-13\n30  1932   1.953135e-12\n31  1933   5.538554e-12\n32  1934   1.570581e-11\n33  1935   4.453736e-11\n34  1936   1.262957e-10\n35  1937   3.581398e-10\n36  1938   1.015586e-09\n37  1939   2.879921e-09\n38  1940   8.166664e-09\n39  1941   2.315841e-08\n40  1942   6.567087e-08\n41  1943   1.862245e-07\n42  1944   5.280813e-07\n43  1945   1.497493e-06\n44  1946   4.246478e-06\n45  1947   1.204184e-05\n46  1948   3.414735e-05\n47  1949   9.683247e-05\n48  1950   2.745902e-04\n49  1951   7.786622e-04\n50  1952   2.208071e-03\n51  1953   6.261482e-03\n52  1954   1.775584e-02\n53  1955   5.035067e-02\n54  1956   1.427806e-01\n55  1957   4.048865e-01\n56  1958   1.148146e+00\n57  1959   3.255825e+00\n58  1960   9.232608e+00\n59  1961   2.618102e+01\n60  1962   7.424125e+01\n61  1963   2.105204e+02\n62  1964   5.969189e+02\n63  1965   1.692224e+03\n64  1966   4.794872e+03\n65  1967   1.356633e+04\n66  1968   3.822483e+04\n67  1969   1.064332e+05\n68  1970   2.863262e+05\n69  1971   6.938150e+05\n70  1972   1.156410e+06\n71  1973  -5.455113e+05\n72  1974   3.536788e+06\n73  1975  -8.174416e+07\n74  1976   5.474425e+09\n75  1977  -9.104813e+11\n76  1978   2.541112e+14\n77  1979   3.451609e+16\n78  1980  -2.966411e+19\n79  1981  -5.912635e+23\n80  1982   7.891325e+29\n81  1983   1.751670e+38\n82  1984  -1.085194e+49\n83  1985   9.131881e+61\n84  1986   6.604239e+77\n85  1987   9.519960e+97\n86  1988 -1.831540e+124\n87  1989  7.821676e+158\n88  1990  2.069371e+204\n89  1991 -4.607124e+262\n90  1992            Inf\n91  1993            NaN\n92  1994            NaN\n93  1995            NaN\n94  1996            NaN\n\n[[2]]\n[1] NaN\n\n#...compared with lag = 3\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=3, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=3,Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModelLag(c(OptPars1_list[[4]]$par), t=4-1,Yrs, Data, ReturnResults=T)[[1]]\nL3 &lt;- tibble(Yrs,Data) %&gt;% ggplot(aes(Yrs, Data)) + geom_point(shape = \"*\", color = \"red\", size = 5) + ylim(-1,9000) + geom_line(aes(Years, Pred), data = Predictions, lty = 2) + ggtitle('Lag 3') + geom_line(aes(Years, Pred), data = Predictions2, lty = 1)\n\n#...and no lag = 0\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=0, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=0,Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModelLag(c(OptPars1_list[[1]]$par), t=1-1,Yrs, Data, ReturnResults=T)[[1]]\nL0 &lt;- tibble(Yrs, Data) %&gt;% ggplot(aes(Yrs, Data)) + geom_point(shape = \"*\", color = \"red\", size = 5) + ylim(-1,9000) + geom_line(aes(Years, Pred), data = Predictions, lty = 2) + ggtitle('Lag 0') + geom_line(aes(Years, Pred), data = Predictions2, lty = 1)\n\n\n#Q - Which one fits the data best?\ngridExtra::grid.arrange(L5, L3, L0, ncol = 3)\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe optimizer matters\nThe starting values matter\nThe biology matters"
  },
  {
    "objectID": "Fitting_logistic_curve.html#logistic-growth---review",
    "href": "Fitting_logistic_curve.html#logistic-growth---review",
    "title": "Fitting a logistic curve",
    "section": "",
    "text": "Remember this equation from the Simple Population Models section?\n\\(N_t = N_{t-1} + rN_{t-1}(1-N_{t-1}/K)\\)\nLet’s review.\n\nWhat is the functional form?\nWhat are the parameters?\nWhat does each parameter control?\n\n\n\nIn this section, building on the basics we have learned on how statistical models are fit, we are going to fit a non-linear model. The model describes logistic growth of an elephant population. Note: here we use sum of squares as our objective function, which is not the best way to fit a non-linear model, but is used for demonstration.\nTake a look at this website below to get familiar with the system. Then continue with the exercise.\n\n\n\n\nYrs&lt;-c(1903, 1905, 1907, 1926, 1928, 1932, 1935, 1939, 1946, 1948, 1961, 1964, 1967, 1970, 1972, 1974, 1976, 1979, 1982, 1985, 1988, 1990, 1993, 1996)\nData&lt;-c(12, 15, 17, 25, 27, 30, 37, 42, 52, 108, 156, 219, 7563, 8642, 7378, 7245, 6983, 7367, 7689, 8503, 7654, 6803, 7204, 7343)\nlength(Yrs); length(Data)\n\n[1] 24\n\n\n[1] 24\n\n#pars, Years, and Numbers are vector arguments\nLogisticGrowthModel&lt;-function(pars, Years, Numbers, ReturnResults=F){\n  \n  #print(Years)\n  \n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]#assign parameters\n  \n  TT&lt;-Years[1]:Years[length(Years)] #stretch out years within model so no missing years\n\n  #print(TT)\n  \n  PP&lt;-vector(mode = \"numeric\", length = length(TT));  #empty population size vector\n  \n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); #empty population growth vector\n  \n  II&lt;-rr*BB*(1-BB/KK) #initial growth to reach year 1 from year 0\n  \n  PP[1]&lt;-BB+II # population assignment at year 1, based on parameter BB (initial population)\n  \n  for(i in 2:length(PP)){ \n    \n    DD[i-1] &lt;- rr*PP[i-1]*(1-PP[i-1]/KK); #this is dN/dt - amount the population grows based\n    #on conditions in the previous time step\n    \n    PP[i] &lt;- PP[i-1] + DD[i-1] #update population size\n    \n    } \n\n  #print(DD)\n  #print(PP)\n  \n   Predictions&lt;-data.frame(Years = TT, Pred = PP); #create predictions\n   \n   Observations&lt;-data.frame(Years, Numbers) #store observations in data frame\n   \n   Results&lt;-merge(Predictions, Observations, by = \"Years\") #match Predictions and Observations by Year\n  \n   SS&lt;-sum((Results$Numbers - Results$Pred)^2) #calculate sum of squares: sum((obs - pred)^2)\n  \n   #print('Results')\n   #print(Results)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(SS)}\n   #controls options for what is returned from function\n\n   }\n\n#LogisticGrowthModel(c(7500,0.5), Yrs, Data, t=0, ReturnResults=T) ERROR\nLogisticGrowthModel(c(7500,0.5,1), Years=Yrs, Numbers=Data, ReturnResults=T)\n\n[[1]]\n   Years        Pred\n1   1903    1.499933\n2   1904    2.249750\n3   1905    3.374288\n4   1906    5.060672\n5   1907    7.589301\n6   1908   11.380112\n7   1909   17.061534\n8   1910   25.572895\n9   1911   38.315744\n10  1912   57.375743\n11  1913   85.844149\n12  1914  128.274942\n13  1915  191.315449\n14  1916  284.533067\n15  1917  421.402329\n16  1918  620.264832\n17  1919  904.748684\n18  1920 1302.551681\n19  1921 1840.718129\n20  1922 2535.194312\n21  1923 3374.310788\n22  1924 4302.401296\n23  1925 5219.558149\n24  1926 6013.084739\n25  1927 6609.147903\n26  1928 7001.666121\n27  1929 7234.277284\n28  1930 7362.431404\n29  1931 7429.954028\n30  1932 7464.649918\n31  1933 7482.241650\n32  1934 7491.099801\n33  1935 7495.544620\n34  1936 7497.770987\n35  1937 7498.885162\n36  1938 7499.442498\n37  1939 7499.721228\n38  1940 7499.860609\n39  1941 7499.930303\n40  1942 7499.965151\n41  1943 7499.982576\n42  1944 7499.991288\n43  1945 7499.995644\n44  1946 7499.997822\n45  1947 7499.998911\n46  1948 7499.999455\n47  1949 7499.999728\n48  1950 7499.999864\n49  1951 7499.999932\n50  1952 7499.999966\n51  1953 7499.999983\n52  1954 7499.999991\n53  1955 7499.999996\n54  1956 7499.999998\n55  1957 7499.999999\n56  1958 7499.999999\n57  1959 7500.000000\n58  1960 7500.000000\n59  1961 7500.000000\n60  1962 7500.000000\n61  1963 7500.000000\n62  1964 7500.000000\n63  1965 7500.000000\n64  1966 7500.000000\n65  1967 7500.000000\n66  1968 7500.000000\n67  1969 7500.000000\n68  1970 7500.000000\n69  1971 7500.000000\n70  1972 7500.000000\n71  1973 7500.000000\n72  1974 7500.000000\n73  1975 7500.000000\n74  1976 7500.000000\n75  1977 7500.000000\n76  1978 7500.000000\n77  1979 7500.000000\n78  1980 7500.000000\n79  1981 7500.000000\n80  1982 7500.000000\n81  1983 7500.000000\n82  1984 7500.000000\n83  1985 7500.000000\n84  1986 7500.000000\n85  1987 7500.000000\n86  1988 7500.000000\n87  1989 7500.000000\n88  1990 7500.000000\n89  1991 7500.000000\n90  1992 7500.000000\n91  1993 7500.000000\n92  1994 7500.000000\n93  1995 7500.000000\n94  1996 7500.000000\n\n[[2]]\n[1] 471423138\n\n\nNow we would like to fit this model by minimizing the objective function using algorithms implemented in the ‘optim’ function.\n\n?optim\n# this function tries different parameter values to minimize the output of \n#LogisticGrowthModel. The output is the sum of squares.\n\n\nOptPars1&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000))\nOptPars1\n\n$par\n[1] 8040.6746970    0.1581262    0.8000000\n\n$value\n[1] 42519063\n\n$counts\nfunction gradient \n      60       60 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\"\n\nOptPars2&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, Years = Yrs, Numbers = Data, control = list(maxit = 50000))\nOptPars2\n\n$par\n[1] 7.529675e+03 1.835725e+00 5.138202e-26\n\n$value\n[1] 3428472\n\n$counts\nfunction gradient \n    2531       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\nPredictions1&lt;-LogisticGrowthModel(OptPars1$par, Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModel(OptPars2$par, Yrs, Data, ReturnResults=T)[[1]]\n\n#tibble(Yrs, Data) %&gt;% \n#  ggplot(aes(Yrs, Data)) + geom_point(shape = '*', color = 'red', size = 5) + ylim(0,8000)\n\n\ntibble(Yrs, Data) %&gt;% \n  ggplot(aes(Yrs, Data)) + geom_point(shape = '*', color = 'red', size = 5) + ylim(0,8000) + geom_line(aes(Years, Pred), data = Predictions1) + geom_line(aes(Years, Pred), data = Predictions2, linetype = 2)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n# Now try to figure out the best time lag as shown in the on-line example. \n# Hint: plot SS as it relates to the time lag value. SS is your sums of \n# squares, so the value with the lowest sum of squares is the one you want. \n# Try lag values of 0 - 5. To include a lag, use the next function instead. \n# Note that there is now an additional parameter for lag t.\n\nLogisticGrowthModelLag&lt;-function(pars, Years, Numbers, t=0, ReturnResults=F){\n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]\n  TT&lt;-Years[1]:Years[length(Years)]\n  PP&lt;-vector(mode = \"numeric\", length = length(TT));\n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); \n  II&lt;-rr*BB*(1-BB/KK)\n  PP[1]&lt;-BB+II\n  \n  #implementing Logistic Growth for Years with no lagged Numbers data\n  if(t&gt;0){\n    for(i in 2:(t+1)){ \n      DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1]/KK);\n      PP[i]&lt;-PP[i-1] + DD[i-1]\n    }\n  }\n  \n  #implementing Lagged growth\n  for(i in (t+2):length(PP)){ \n    DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1-t]/KK); \n    PP[i]&lt;-PP[i-1] + DD[i-1]\n  }\n  \n  #print(PP) \n  Predictions&lt;-data.frame(Years = TT, Pred = PP); Observations&lt;-data.frame(Years, Numbers)\n  Results&lt;-merge(Predictions, Observations, by = \"Years\")\n  SS&lt;-sum((Results$Numbers - Results$Pred)^2)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(ifelse(is.infinite(SS), 10000000000, SS))}\n}\n\nLogisticGrowthModelLag(OptPars1$par, t=3,Yrs, Data, ReturnResults=T)\n\n[[1]]\n   Years         Pred\n1   1903    0.9264883\n2   1904    1.0729735\n3   1905    1.2426160\n4   1906    1.4390758\n5   1907    1.6666051\n6   1908    1.9301038\n7   1909    2.2352565\n8   1910    2.5886458\n9   1911    2.9978936\n10  1912    3.4718252\n11  1913    4.0206589\n12  1914    4.6562256\n13  1915    5.3922222\n14  1916    6.2445054\n15  1917    7.2314313\n16  1918    8.3742476\n17  1919    9.6975471\n18  1920   11.2297921\n19  1921   13.0039190\n20  1922   15.0580372\n21  1923   17.4362351\n22  1924   20.1895094\n23  1925   23.3768358\n24  1926   27.0664026\n25  1927   31.3370279\n26  1928   36.2797896\n27  1929   41.9998947\n28  1930   48.6188210\n29  1931   56.2767662\n30  1932   65.1354433\n31  1933   75.3812615\n32  1934   87.2289368\n33  1935  100.9255750\n34  1936  116.7552690\n35  1937  135.0442496\n36  1938  156.1666198\n37  1939  180.5506912\n38  1940  208.6859191\n39  1941  241.1304043\n40  1942  278.5188841\n41  1943  321.5710768\n42  1944  371.1001575\n43  1945  428.0210365\n44  1946  493.3579631\n45  1947  568.2507935\n46  1948  653.9590291\n47  1949  751.8624446\n48  1950  863.4567927\n49  1951  990.3426863\n50  1952 1134.2053549\n51  1953 1296.7825730\n52  1954 1479.8177352\n53  1955 1684.9949017\n54  1956 1913.8528027\n55  1957 2167.6754551\n56  1958 2447.3584494\n57  1959 2753.2523534\n58  1960 3084.9882786\n59  1961 3441.2955571\n60  1962 3819.8275401\n61  1963 4217.0181645\n62  1964 4627.9980060\n63  1965 5046.6022290\n64  1966 5465.5018160\n65  1967 5876.4813051\n66  1968 6270.8693448\n67  1969 6640.1030097\n68  1970 6976.3763500\n69  1971 7273.2948096\n70  1972 7526.4389775\n71  1973 7733.7422313\n72  1974 7895.6114652\n73  1975 8014.7651322\n74  1976 8095.8172321\n75  1977 8144.6841282\n76  1978 8167.9191361\n77  1979 8172.0809512\n78  1980 8163.2189742\n79  1981 8146.5217069\n80  1982 8126.1361650\n81  1983 8105.1365485\n82  1984 8085.6037441\n83  1985 8068.7730403\n84  1986 8055.2121226\n85  1987 8045.0005886\n86  1988 8037.8923087\n87  1989 8033.4507631\n88  1990 8031.1540810\n89  1991 8030.4708544\n90  1992 8030.9102644\n91  1993 8032.0511701\n92  1994 8033.5550140\n93  1995 8035.1670798\n94  1996 8036.7100344\n\n[[2]]\n[1] 41903227\n\nLogisticGrowthModelLag(OptPars2$par, t=3,Yrs, Data, ReturnResults=T)\n\n[[1]]\n   Years           Pred\n1   1903   1.457053e-25\n2   1904   4.131800e-25\n3   1905   1.171665e-24\n4   1906   3.322518e-24\n5   1907   9.421746e-24\n6   1908   2.671748e-23\n7   1909   7.576340e-23\n8   1910   2.148441e-22\n9   1911   6.092388e-22\n10  1912   1.727633e-21\n11  1913   4.899092e-21\n12  1914   1.389248e-20\n13  1915   3.939524e-20\n14  1916   1.117140e-19\n15  1917   3.167902e-19\n16  1918   8.983298e-19\n17  1919   2.547416e-18\n18  1920   7.223770e-18\n19  1921   2.048462e-17\n20  1922   5.808874e-17\n21  1923   1.647237e-16\n22  1924   4.671110e-16\n23  1925   1.324598e-15\n24  1926   3.756195e-15\n25  1927   1.065153e-14\n26  1928   3.020482e-14\n27  1929   8.565254e-14\n28  1930   2.428870e-13\n29  1931   6.887606e-13\n30  1932   1.953135e-12\n31  1933   5.538554e-12\n32  1934   1.570581e-11\n33  1935   4.453736e-11\n34  1936   1.262957e-10\n35  1937   3.581398e-10\n36  1938   1.015586e-09\n37  1939   2.879921e-09\n38  1940   8.166664e-09\n39  1941   2.315841e-08\n40  1942   6.567087e-08\n41  1943   1.862245e-07\n42  1944   5.280813e-07\n43  1945   1.497493e-06\n44  1946   4.246478e-06\n45  1947   1.204184e-05\n46  1948   3.414735e-05\n47  1949   9.683247e-05\n48  1950   2.745902e-04\n49  1951   7.786622e-04\n50  1952   2.208071e-03\n51  1953   6.261482e-03\n52  1954   1.775584e-02\n53  1955   5.035066e-02\n54  1956   1.427806e-01\n55  1957   4.048862e-01\n56  1958   1.148144e+00\n57  1959   3.255806e+00\n58  1960   9.232455e+00\n59  1961   2.617979e+01\n60  1962   7.423134e+01\n61  1963   2.104407e+02\n62  1964   5.962782e+02\n63  1965   1.687075e+03\n64  1966   4.753548e+03\n65  1967   1.323587e+04\n66  1968   3.560916e+04\n67  1969   8.633149e+04\n68  1970   1.447620e+05\n69  1971  -5.662549e+04\n70  1972   3.310179e+05\n71  1973  -6.028423e+06\n72  1974   1.956649e+08\n73  1975   3.256048e+09\n74  1976  -2.535354e+11\n75  1977  -3.733454e+14\n76  1978   1.780857e+19\n77  1979  -1.413675e+25\n78  1980  -8.738152e+32\n79  1981  -7.953561e+43\n80  1982   3.453203e+59\n81  1983   1.190154e+81\n82  1984  2.535443e+110\n83  1985  4.916395e+150\n84  1986 -4.139045e+206\n85  1987  1.200976e+284\n86  1988           -Inf\n87  1989            NaN\n88  1990            NaN\n89  1991            NaN\n90  1992            NaN\n91  1993            NaN\n92  1994            NaN\n93  1995            NaN\n94  1996            NaN\n\n[[2]]\n[1] NaN\n\nSS1&lt;-SS2&lt;-vector(mode = \"numeric\", length = 7)\nOptPars1_list&lt;-OptPars2_list&lt;-NULL\n\nfor(i in 0:6){\n  OptPars1_list[[i+1]]&lt;-try(optim(OptPars1$par, fn=LogisticGrowthModelLag, t=i, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000)), silent=T)\n  OptPars2_list[[i+1]]&lt;-try(optim(OptPars2$par, fn=LogisticGrowthModelLag, t=i, Years = Yrs, Numbers = Data, control = list(maxit = 50000)),silent=T)\n  SS1[i+1]&lt;-try(OptPars1_list[[i+1]]$value/1000000,silent=T )\n  SS2[i+1]&lt;-try(OptPars2_list[[i+1]]$value/1000000,silent=T )\n}\n\nSS1 \n\n[1] 42.51906 40.51695 38.37475 36.33096 34.87183 34.88024 37.08259\n\nSS2 \n\n[1] \"3.42846793085808\"                                                                      \n[2] \"7.36469949321783\"                                                                      \n[3] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[4] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[5] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[6] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n[7] \"Error in OptPars2_list[[i + 1]]$value : \\n  $ operator is invalid for atomic vectors\\n\"\n\n\n\n#Q - The time lag corresponding with the minimum SS is the best. What is time lag is that?\n#That is, which would you choose if the best model by least squares (SS)?\n\n\n\ntibble(lags=c(0:6), SS=SS1) %&gt;% \n  mutate(type = 'SS1') %&gt;% \n  bind_rows(tibble(lags=c(0:6), SS=ifelse(grepl('Error', SS2), NA_real_, as.numeric(SS2))) %&gt;% \n  mutate(type = 'SS2')) %&gt;% \n  group_by(type) %&gt;% \n  ggplot(aes(lags, SS)) + geom_point() + facet_wrap(~type)\n\nWarning in ifelse(grepl(\"Error\", SS2), NA_real_, as.numeric(SS2)): NAs\nintroduced by coercion\n\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n#Q - BUT, this is what happens with lag = 5. Why? (Compare plots with lag = 0 - 4)\n\nOptPars&lt;-NULL\n\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=5, Years = Yrs, Numbers = Data)\n\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=5,Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModelLag(c(OptPars1_list[[6]]$par), t=6-1,Yrs, Data, ReturnResults=T)[[1]]\n\nL5 &lt;- tibble(Yrs, Data) %&gt;% ggplot(aes(Yrs, Data)) + geom_point(shape = \"*\", color = \"red\", size = 5) + ylim(-1,9000) + geom_line(aes(Years, Pred), data = Predictions, lty = 2) + ggtitle('Lag 5') + geom_line(aes(Years, Pred), data = Predictions2, lty = 1)\n\nLogisticGrowthModelLag(OptPars2$par, t=5,Yrs, Data, ReturnResults=T)\n\n[[1]]\n   Years           Pred\n1   1903   1.457053e-25\n2   1904   4.131800e-25\n3   1905   1.171665e-24\n4   1906   3.322518e-24\n5   1907   9.421746e-24\n6   1908   2.671748e-23\n7   1909   7.576340e-23\n8   1910   2.148441e-22\n9   1911   6.092388e-22\n10  1912   1.727633e-21\n11  1913   4.899092e-21\n12  1914   1.389248e-20\n13  1915   3.939524e-20\n14  1916   1.117140e-19\n15  1917   3.167902e-19\n16  1918   8.983298e-19\n17  1919   2.547416e-18\n18  1920   7.223770e-18\n19  1921   2.048462e-17\n20  1922   5.808874e-17\n21  1923   1.647237e-16\n22  1924   4.671110e-16\n23  1925   1.324598e-15\n24  1926   3.756195e-15\n25  1927   1.065153e-14\n26  1928   3.020482e-14\n27  1929   8.565254e-14\n28  1930   2.428870e-13\n29  1931   6.887606e-13\n30  1932   1.953135e-12\n31  1933   5.538554e-12\n32  1934   1.570581e-11\n33  1935   4.453736e-11\n34  1936   1.262957e-10\n35  1937   3.581398e-10\n36  1938   1.015586e-09\n37  1939   2.879921e-09\n38  1940   8.166664e-09\n39  1941   2.315841e-08\n40  1942   6.567087e-08\n41  1943   1.862245e-07\n42  1944   5.280813e-07\n43  1945   1.497493e-06\n44  1946   4.246478e-06\n45  1947   1.204184e-05\n46  1948   3.414735e-05\n47  1949   9.683247e-05\n48  1950   2.745902e-04\n49  1951   7.786622e-04\n50  1952   2.208071e-03\n51  1953   6.261482e-03\n52  1954   1.775584e-02\n53  1955   5.035067e-02\n54  1956   1.427806e-01\n55  1957   4.048865e-01\n56  1958   1.148146e+00\n57  1959   3.255825e+00\n58  1960   9.232608e+00\n59  1961   2.618102e+01\n60  1962   7.424125e+01\n61  1963   2.105204e+02\n62  1964   5.969189e+02\n63  1965   1.692224e+03\n64  1966   4.794872e+03\n65  1967   1.356633e+04\n66  1968   3.822483e+04\n67  1969   1.064332e+05\n68  1970   2.863262e+05\n69  1971   6.938150e+05\n70  1972   1.156410e+06\n71  1973  -5.455113e+05\n72  1974   3.536788e+06\n73  1975  -8.174416e+07\n74  1976   5.474425e+09\n75  1977  -9.104813e+11\n76  1978   2.541112e+14\n77  1979   3.451609e+16\n78  1980  -2.966411e+19\n79  1981  -5.912635e+23\n80  1982   7.891325e+29\n81  1983   1.751670e+38\n82  1984  -1.085194e+49\n83  1985   9.131881e+61\n84  1986   6.604239e+77\n85  1987   9.519960e+97\n86  1988 -1.831540e+124\n87  1989  7.821676e+158\n88  1990  2.069371e+204\n89  1991 -4.607124e+262\n90  1992            Inf\n91  1993            NaN\n92  1994            NaN\n93  1995            NaN\n94  1996            NaN\n\n[[2]]\n[1] NaN\n\n#...compared with lag = 3\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=3, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=3,Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModelLag(c(OptPars1_list[[4]]$par), t=4-1,Yrs, Data, ReturnResults=T)[[1]]\nL3 &lt;- tibble(Yrs,Data) %&gt;% ggplot(aes(Yrs, Data)) + geom_point(shape = \"*\", color = \"red\", size = 5) + ylim(-1,9000) + geom_line(aes(Years, Pred), data = Predictions, lty = 2) + ggtitle('Lag 3') + geom_line(aes(Years, Pred), data = Predictions2, lty = 1)\n\n#...and no lag = 0\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=0, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=0,Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModelLag(c(OptPars1_list[[1]]$par), t=1-1,Yrs, Data, ReturnResults=T)[[1]]\nL0 &lt;- tibble(Yrs, Data) %&gt;% ggplot(aes(Yrs, Data)) + geom_point(shape = \"*\", color = \"red\", size = 5) + ylim(-1,9000) + geom_line(aes(Years, Pred), data = Predictions, lty = 2) + ggtitle('Lag 0') + geom_line(aes(Years, Pred), data = Predictions2, lty = 1)\n\n\n#Q - Which one fits the data best?\ngridExtra::grid.arrange(L5, L3, L0, ncol = 3)\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe optimizer matters\nThe starting values matter\nThe biology matters"
  },
  {
    "objectID": "Fitting_logistic_curve.html#further-considerations",
    "href": "Fitting_logistic_curve.html#further-considerations",
    "title": "Gró-FTP - Stock assessment line - Simple pop. models",
    "section": "",
    "text": "What biological and ecological factors contribute to r and K?\n\n\n\nWhat population characteristic is the most important for natural resource management?\nHow to detect this without biological/ecological data?\nDraw a graph of how population growth rate (\\(dN/dt\\)) changes over time for the case studies of 1) exponential and 2) logistic growth. How does the density dependent term \\((1-N/K)\\) change over time?\nWhat species would be difficult to represent with this?"
  },
  {
    "objectID": "Review-what are models.html",
    "href": "Review-what are models.html",
    "title": "Review - what are models?",
    "section": "",
    "text": "Models are only a representation and simplification of reality. Population dynamics models represent how populations grow or decline over time. They can be parameterized by fitting models to available data.\nIn natural resource management of renewable resources, predictions from population dynamics models often used to provide advice for near-term removals from a population. For example, in fisheries management, usually the advice provides guidance on maximal amounts that can be removed from a fish population (i.e., catch) without depressing its ability to regenerate (i.e., overfishing).\nA wide variety of models and types of information can be included, but here we start with some of the most basic models. These are also often used when little detail is known about the population.\n\n\nParameterization is the process of choosing values for parameters in an equation. In a line equation:\n\\(y = a + bx\\) ,\nthe intercept parameter \\(a\\) and the slope parameter \\(b\\) can be chosen to be certain values. For example, what value of \\(y\\) results from \\(a=0\\) and \\(b=1\\)? Any two values can be used to create a single line that yields an infinite set of \\(x\\) and \\(y\\) values along it. Plotting these yields a line:\n\na &lt;- 0; b&lt;-1; x&lt;-1:10; y&lt;- a + b*x\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nThis line is an example of a mathematical model:\n\n\n\n\n\n\n\n\n\nWith a linear functional form:\n\n\n\n\n\n\n\n\n\nOther functional forms are possible. For example, can you find an equation that roughly represents this process?\n\n\n\n\n\n\n\n\n\nThis is one option, but others exist. Polynomial models for example could replicate this shape, even though they are a class of ‘linear’ models.\n\nx &lt;- seq(0,3*pi,pi/10); y &lt;- sin(x)\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nAnother kind of model could describe circular processes:\n\n\n\n\n\n\n\n\n\nMathematical models therefore can have a variety of forms, with specific parameterizations defining a certain model’s exact shape.\n\n\n\nInstead of describing operational linkages, statistical models describe variation surrounding a central process described by the functional form. The generation of this variation is often itself described of as a process, but it is instead an error process describing how chance results in certain numbers more frequently than other numbers. The result of are error distributions around a central process. These are modeled in statistics using equations for various error distributions, such as a binomial, Poisson, Gaussian (normal), or lognormal distributions.\n\n\n\n\n\n\n\n\n\nDistributions are well-used in the study of biological processes, because biological processes are notoriously imperfect and therefore partially controlled by chance. Growth for example, can be partially determined by age, but is also simply the result of variation:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(age, size)) + geom_point() + geom_line(aes(age, pred_size))\n\n\n\n\n\n\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(size - pred_size)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nIf we do not know the process behind a pattern observed in nature, how do we best describe the process using parameters, or parameterize our model?\nThis is done using statistical model fitting. For example:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntest.lm &lt;- lm(size~age)\nsummary(test.lm)\n\n\nCall:\nlm(formula = size ~ age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.707  -8.035  -0.802   7.076  30.687 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6069     2.2411   0.271    0.787    \nage           0.9625     0.0328  29.344   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.92 on 108 degrees of freedom\nMultiple R-squared:  0.8886,    Adjusted R-squared:  0.8875 \nF-statistic: 861.1 on 1 and 108 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that the fitting process accurately achieved our simulated parameters? How did it do that? It used a fitting algorithm to find the line that best fit all data points simultaneously. The fit is summarised as a single number in an objective function. Finding the best fit is done iteratively by flipping the sign of an equation that reflects fit to the model (i.e., taking the negative), and then following a routine to minimize the objective function. Flipping the sign is done simply because minimizing is computationally easier to handle than maximizing.\nA general form for such an algorithm goes something like:\n1 - Pick or jump to new values for parameters (e.g., \\(a\\) and \\(b\\) in a linear model).\n2- Calculate predictions\n3 - Evaluate the objection function, which is usually a statistic representing model fit (e.g., sum of squares or likelihood function)\n4 - Repeat 1-3, then compare the fit to the previous value.\n5 - If the fit is better, keep new parameter values. If worse, keep old values. Some algorithms add an element of chance here to avoid local minima (e.g., keep the better values only 90% of the time)\n6 - Repeat until no better answer can be found (i.e., essentially the same answer is found again and again, indicating convergence)\n\n\nBy comparison, let’s try to do something similar by hand:\n\n\n####Exercise 3 - fit linear model----###\nline_f&lt;-function(b, a = 0, x = xx){\n  y&lt;-a + b*x\n  return(y)\n}\n\n\nset.seed(100)\n?rnorm\nrnorm(100)\n\n  [1] -0.50219235  0.13153117 -0.07891709  0.88678481  0.11697127  0.31863009\n  [7] -0.58179068  0.71453271 -0.82525943 -0.35986213  0.08988614  0.09627446\n [13] -0.20163395  0.73984050  0.12337950 -0.02931671 -0.38885425  0.51085626\n [19] -0.91381419  2.31029682 -0.43808998  0.76406062  0.26196129  0.77340460\n [25] -0.81437912 -0.43845057 -0.72022155  0.23094453 -1.15772946  0.24707599\n [31] -0.09111356  1.75737562 -0.13792961 -0.11119350 -0.69001432 -0.22179423\n [37]  0.18290768  0.41732329  1.06540233  0.97020202 -0.10162924  1.40320349\n [43] -1.77677563  0.62286739 -0.52228335  1.32223096 -0.36344033  1.31906574\n [49]  0.04377907 -1.87865588 -0.44706218 -1.73859795  0.17886485  1.89746570\n [55] -2.27192549  0.98046414 -1.39882562  1.82487242  1.38129873 -0.83885188\n [61] -0.26199577 -0.06884403 -0.37888356  2.58195893  0.12983414 -0.71302498\n [67]  0.63799424  0.20169159 -0.06991695 -0.09248988  0.44890327 -1.06435567\n [73] -1.16241932  1.64852175 -2.06209602  0.01274972 -1.08752835  0.27053949\n [79]  1.00845187 -2.07440475  0.89682227 -0.04999577 -1.34534931 -1.93121153\n [85]  0.70958158 -0.15790503  0.21636787  0.81736208  1.72717575 -0.10377029\n [91] -0.55712229  1.42830143 -0.89295740 -1.15757124 -0.53029645  2.44568276\n [97] -0.83249580  0.41351985 -1.17868314 -1.17403476\n\nhist(rnorm(100))\n\n\n\n\n\n\n\nxx &lt;- seq(0.5,3.5,0.5)\nline_f(xx)\n\n[1]  0.25  1.00  2.25  4.00  6.25  9.00 12.25\n\nobs &lt;- 0 + 2.3*xx + rnorm(length(xx))\n\n\npred&lt;-line_f(1)\n\ntibble(xx, obs, pred) %&gt;% ggplot(aes(xx, obs)) + geom_point() + geom_line(aes(xx,pred))\n\n\n\n\n\n\n\nSS1&lt;- sum((obs-pred)^2)\n\npred&lt;-line_f(1.5)\nSS1.5&lt;- sum((obs-pred)^2)\npred1.5&lt;-line_f(1.5)\n\npred&lt;-line_f(2)\nSS2&lt;- sum((obs-pred)^2)\npred2&lt;-line_f(2)\n\npred&lt;-line_f(2.5)\nSS2.5&lt;- sum((obs-pred)^2)\npred2.5&lt;-line_f(2.5)\n\ntibble(xx, obs, pred = pred1.5) %&gt;%\n  mutate(type = '1.5') %&gt;% \n  bind_rows(tibble(xx, obs, pred = pred2) %&gt;% \n              mutate(type='2')) %&gt;% \n  bind_rows(tibble(xx, obs, pred = pred2.5) %&gt;% \n              mutate(type='2.5')) %&gt;%\n  group_by(type) %&gt;% \n  ggplot(aes(xx, obs)) + geom_point() + geom_line(aes(xx, pred, color = type))\n\n\n\n\n\n\n\npred&lt;-line_f( 3)\nSS3&lt;- sum((obs-pred)^2)\n\nSS1; SS1.5; SS2; SS2.5; SS3\n\n[1] 61.30771\n\n\n[1] 25.07943\n\n\n[1] 6.351145\n\n\n[1] 5.122861\n\n\n[1] 21.39458\n\ntibble(SS = c(SS1, SS1.5, SS2, SS2.5, SS3),\n       slope=c(1, 1.5, 2, 2.5, 3)) %&gt;% \n  ggplot(aes(slope, SS)) + geom_line()\n\n\n\n\n\n\n\nSS_f&lt;-function(b, a = 0, obs1 = obs){\n  pred&lt;-line_f(b)\n  SS&lt;- sum((obs1-pred)^2)\n  return(SS)\n}\n\nSS_f(2.5)\n\n[1] 5.122861\n\n\nFinding the right answer by hand is clearly a very tedious process. There must be a better way to do this, and in fact there are many many different better ways to do this. At [this web page](https://www.r-bloggers.com/2013/04/the-golden-section-search-method-modifying-the-bisection-method-with-the-golden-ratio-for-numerical-optimization/), one rather easy-to-understand algorithm is described as an example of an efficient search method for finding the minimum. It is called the ‘Golden Section search’ and is further described in a video [here](https://www.youtube.com/watch?v=hLm8xfwWYPw).\nIn order to run this script, the ‘golden.section.search’ function must be saved and sourced from [here](https://chemicalstatistician.wordpress.com/2013/04/22/using-r-to-implement-the-golden-bisection-method/).\n\nsource('goldensectionsearch.r')\n\ngolden.section.search(SS_f, 1, 3, 0.00001)\n\n \nIteration # 1 \nf1 = 13.01273 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 3 \nNew Lower Bound = 1.763932 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.527864 \n \nIteration # 2 \nf1 = 3.590522 \nf2 = 5.569206 \nf2 &gt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 1.763932 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.055728 \n \nIteration # 3 \nf1 = 5.3477 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 2.055728 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.347524 \n \nIteration # 4 \nf1 = 3.590522 \nf2 = 3.642812 \nf2 &gt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.055728 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.167184 \n \nIteration # 5 \nf1 = 3.992991 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.167184 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.27864 \n \nIteration # 6 \nf1 = 3.590522 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.236068 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.304952 \n \nIteration # 7 \nf1 = 3.507856 \nf2 = 3.5202 \nf2 &gt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.236068 \nNew Upper Test Point =  2.27864 \nNew Lower Test Point =  2.262379 \n \nIteration # 8 \nf1 = 3.524456 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.262379 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.28869 \n \nIteration # 9 \nf1 = 3.507856 \nf2 = 3.506851 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.27864 \nNew Lower Test Point =  2.28869 \nNew Upper Test Point =  2.294902 \n \nIteration # 10 \nf1 = 3.506851 \nf2 = 3.509765 \nf2 &gt; f1 \nNew Upper Bound = 2.294902 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.28869 \nNew Lower Test Point =  2.284852 \n \nIteration # 11 \nf1 = 3.5064 \nf2 = 3.506851 \nf2 &gt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.282479 \n \nIteration # 12 \nf1 = 3.506637 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.282479 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.286318 \n \nIteration # 13 \nf1 = 3.5064 \nf2 = 3.50645 \nf2 &gt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.282479 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.283945 \n \nIteration # 14 \nf1 = 3.506444 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.283945 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285412 \n \nIteration # 15 \nf1 = 3.5064 \nf2 = 3.506401 \nf2 &gt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.283945 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.284506 \n \nIteration # 16 \nf1 = 3.50641 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284506 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285066 \n \nIteration # 17 \nf1 = 3.5064 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284852 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285198 \n \nIteration # 18 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284852 \nNew Upper Test Point =  2.285066 \nNew Lower Test Point =  2.284984 \n \nIteration # 19 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284984 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285116 \n \nIteration # 20 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.285066 \nNew Lower Test Point =  2.285116 \nNew Upper Test Point =  2.285147 \n \nIteration # 21 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285147 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285116 \nNew Lower Test Point =  2.285097 \n \nIteration # 22 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285085 \n \nIteration # 23 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285085 \nNew Lower Test Point =  2.285097 \nNew Upper Test Point =  2.285104 \n \nIteration # 24 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285104 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285092 \n \nIteration # 25 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285092 \nNew Lower Test Point =  2.285089 \n \nIteration # 26 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285089 \nNew Lower Test Point =  2.285092 \nNew Upper Test Point =  2.285094 \n \nFinal Lower Bound = 2.285089 \nFinal Upper Bound = 2.285097 \nEstimated Minimizer = 2.285093 \n\n#Check out optim function\n?optim\n\nAnd for more complicated problems with many parameters, there are more complex algorithms, e.g. simulated annealing.\n\n\n\n\nUnderstanding the basics of programming, mathematical modeling, and statistical model fitting are not only the basis to stock assessment, but also yields other transferable skills.\n\n\n\n\n\n\n\n\n\nThis process can be frustrating as you learn debugging, or quickly learning how to find your own mistakes. Some things to remember as you get better at this:\n\n\n\n\n\n\n\n\n\n\n\nThis text, pictures, and figures came with the Quarto package. Can you fit linear models to each of the three groups of penguins and add the lines to the plots?\n\n\n\n\n\nThe penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\nThe plot below shows the relationship between flipper and bill lengths of these penguins."
  },
  {
    "objectID": "Review-what are models.html#parameterization",
    "href": "Review-what are models.html#parameterization",
    "title": "Review - what are models?",
    "section": "",
    "text": "Parameterization is the process of choosing values for parameters in an equation. In a line equation:\n\\(y = a + bx\\) ,\nthe intercept parameter \\(a\\) and the slope parameter \\(b\\) can be chosen to be certain values. For example, what value of \\(y\\) results from \\(a=0\\) and \\(b=1\\)? Any two values can be used to create a single line that yields an infinite set of \\(x\\) and \\(y\\) values along it. Plotting these yields a line:\n\na &lt;- 0; b&lt;-1; x&lt;-1:10; y&lt;- a + b*x\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nThis line is an example of a mathematical model:\n\n\n\n\n\n\n\n\n\nWith a linear functional form:\n\n\n\n\n\n\n\n\n\nOther functional forms are possible. For example, can you find an equation that roughly represents this process?\n\n\n\n\n\n\n\n\n\nThis is one option, but others exist. Polynomial models for example could replicate this shape, even though they are a class of ‘linear’ models.\n\nx &lt;- seq(0,3*pi,pi/10); y &lt;- sin(x)\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nAnother kind of model could describe circular processes:\n\n\n\n\n\n\n\n\n\nMathematical models therefore can have a variety of forms, with specific parameterizations defining a certain model’s exact shape."
  },
  {
    "objectID": "Review-what are models.html#statistical-models",
    "href": "Review-what are models.html#statistical-models",
    "title": "Review - what are models?",
    "section": "",
    "text": "Instead of describing operational linkages, statistical models describe variation surrounding a central process described by the functional form. The generation of this variation is often itself described of as a process, but it is instead an error process describing how chance results in certain numbers more frequently than other numbers. The result of are error distributions around a central process. These are modeled in statistics using equations for various error distributions, such as a binomial, Poisson, Gaussian (normal), or lognormal distributions.\n\n\n\n\n\n\n\n\n\nDistributions are well-used in the study of biological processes, because biological processes are notoriously imperfect and therefore partially controlled by chance. Growth for example, can be partially determined by age, but is also simply the result of variation:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(age, size)) + geom_point() + geom_line(aes(age, pred_size))\n\n\n\n\n\n\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(size - pred_size)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Review-what are models.html#fitting",
    "href": "Review-what are models.html#fitting",
    "title": "Review - what are models?",
    "section": "",
    "text": "If we do not know the process behind a pattern observed in nature, how do we best describe the process using parameters, or parameterize our model?\nThis is done using statistical model fitting. For example:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntest.lm &lt;- lm(size~age)\nsummary(test.lm)\n\n\nCall:\nlm(formula = size ~ age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.707  -8.035  -0.802   7.076  30.687 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6069     2.2411   0.271    0.787    \nage           0.9625     0.0328  29.344   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.92 on 108 degrees of freedom\nMultiple R-squared:  0.8886,    Adjusted R-squared:  0.8875 \nF-statistic: 861.1 on 1 and 108 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that the fitting process accurately achieved our simulated parameters? How did it do that? It used a fitting algorithm to find the line that best fit all data points simultaneously. The fit is summarised as a single number in an objective function. Finding the best fit is done iteratively by flipping the sign of an equation that reflects fit to the model (i.e., taking the negative), and then following a routine to minimize the objective function. Flipping the sign is done simply because minimizing is computationally easier to handle than maximizing.\nA general form for such an algorithm goes something like:\n1 - Pick or jump to new values for parameters (e.g., \\(a\\) and \\(b\\) in a linear model).\n2- Calculate predictions\n3 - Evaluate the objection function, which is usually a statistic representing model fit (e.g., sum of squares or likelihood function)\n4 - Repeat 1-3, then compare the fit to the previous value.\n5 - If the fit is better, keep new parameter values. If worse, keep old values. Some algorithms add an element of chance here to avoid local minima (e.g., keep the better values only 90% of the time)\n6 - Repeat until no better answer can be found (i.e., essentially the same answer is found again and again, indicating convergence)\n\n\nBy comparison, let’s try to do something similar by hand:\n\n\n####Exercise 3 - fit linear model----###\nline_f&lt;-function(b, a = 0, x = xx){\n  y&lt;-a + b*x\n  return(y)\n}\n\n\nset.seed(100)\n?rnorm\nrnorm(100)\n\n  [1] -0.50219235  0.13153117 -0.07891709  0.88678481  0.11697127  0.31863009\n  [7] -0.58179068  0.71453271 -0.82525943 -0.35986213  0.08988614  0.09627446\n [13] -0.20163395  0.73984050  0.12337950 -0.02931671 -0.38885425  0.51085626\n [19] -0.91381419  2.31029682 -0.43808998  0.76406062  0.26196129  0.77340460\n [25] -0.81437912 -0.43845057 -0.72022155  0.23094453 -1.15772946  0.24707599\n [31] -0.09111356  1.75737562 -0.13792961 -0.11119350 -0.69001432 -0.22179423\n [37]  0.18290768  0.41732329  1.06540233  0.97020202 -0.10162924  1.40320349\n [43] -1.77677563  0.62286739 -0.52228335  1.32223096 -0.36344033  1.31906574\n [49]  0.04377907 -1.87865588 -0.44706218 -1.73859795  0.17886485  1.89746570\n [55] -2.27192549  0.98046414 -1.39882562  1.82487242  1.38129873 -0.83885188\n [61] -0.26199577 -0.06884403 -0.37888356  2.58195893  0.12983414 -0.71302498\n [67]  0.63799424  0.20169159 -0.06991695 -0.09248988  0.44890327 -1.06435567\n [73] -1.16241932  1.64852175 -2.06209602  0.01274972 -1.08752835  0.27053949\n [79]  1.00845187 -2.07440475  0.89682227 -0.04999577 -1.34534931 -1.93121153\n [85]  0.70958158 -0.15790503  0.21636787  0.81736208  1.72717575 -0.10377029\n [91] -0.55712229  1.42830143 -0.89295740 -1.15757124 -0.53029645  2.44568276\n [97] -0.83249580  0.41351985 -1.17868314 -1.17403476\n\nhist(rnorm(100))\n\n\n\n\n\n\n\nxx &lt;- seq(0.5,3.5,0.5)\nline_f(xx)\n\n[1]  0.25  1.00  2.25  4.00  6.25  9.00 12.25\n\nobs &lt;- 0 + 2.3*xx + rnorm(length(xx))\n\n\npred&lt;-line_f(1)\n\ntibble(xx, obs, pred) %&gt;% ggplot(aes(xx, obs)) + geom_point() + geom_line(aes(xx,pred))\n\n\n\n\n\n\n\nSS1&lt;- sum((obs-pred)^2)\n\npred&lt;-line_f(1.5)\nSS1.5&lt;- sum((obs-pred)^2)\npred1.5&lt;-line_f(1.5)\n\npred&lt;-line_f(2)\nSS2&lt;- sum((obs-pred)^2)\npred2&lt;-line_f(2)\n\npred&lt;-line_f(2.5)\nSS2.5&lt;- sum((obs-pred)^2)\npred2.5&lt;-line_f(2.5)\n\ntibble(xx, obs, pred = pred1.5) %&gt;%\n  mutate(type = '1.5') %&gt;% \n  bind_rows(tibble(xx, obs, pred = pred2) %&gt;% \n              mutate(type='2')) %&gt;% \n  bind_rows(tibble(xx, obs, pred = pred2.5) %&gt;% \n              mutate(type='2.5')) %&gt;%\n  group_by(type) %&gt;% \n  ggplot(aes(xx, obs)) + geom_point() + geom_line(aes(xx, pred, color = type))\n\n\n\n\n\n\n\npred&lt;-line_f( 3)\nSS3&lt;- sum((obs-pred)^2)\n\nSS1; SS1.5; SS2; SS2.5; SS3\n\n[1] 61.30771\n\n\n[1] 25.07943\n\n\n[1] 6.351145\n\n\n[1] 5.122861\n\n\n[1] 21.39458\n\ntibble(SS = c(SS1, SS1.5, SS2, SS2.5, SS3),\n       slope=c(1, 1.5, 2, 2.5, 3)) %&gt;% \n  ggplot(aes(slope, SS)) + geom_line()\n\n\n\n\n\n\n\nSS_f&lt;-function(b, a = 0, obs1 = obs){\n  pred&lt;-line_f(b)\n  SS&lt;- sum((obs1-pred)^2)\n  return(SS)\n}\n\nSS_f(2.5)\n\n[1] 5.122861\n\n\nFinding the right answer by hand is clearly a very tedious process. There must be a better way to do this, and in fact there are many many different better ways to do this. At [this web page](https://www.r-bloggers.com/2013/04/the-golden-section-search-method-modifying-the-bisection-method-with-the-golden-ratio-for-numerical-optimization/), one rather easy-to-understand algorithm is described as an example of an efficient search method for finding the minimum. It is called the ‘Golden Section search’ and is further described in a video [here](https://www.youtube.com/watch?v=hLm8xfwWYPw).\nIn order to run this script, the ‘golden.section.search’ function must be saved and sourced from [here](https://chemicalstatistician.wordpress.com/2013/04/22/using-r-to-implement-the-golden-bisection-method/).\n\nsource('goldensectionsearch.r')\n\ngolden.section.search(SS_f, 1, 3, 0.00001)\n\n \nIteration # 1 \nf1 = 13.01273 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 3 \nNew Lower Bound = 1.763932 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.527864 \n \nIteration # 2 \nf1 = 3.590522 \nf2 = 5.569206 \nf2 &gt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 1.763932 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.055728 \n \nIteration # 3 \nf1 = 5.3477 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 2.055728 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.347524 \n \nIteration # 4 \nf1 = 3.590522 \nf2 = 3.642812 \nf2 &gt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.055728 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.167184 \n \nIteration # 5 \nf1 = 3.992991 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.167184 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.27864 \n \nIteration # 6 \nf1 = 3.590522 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.236068 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.304952 \n \nIteration # 7 \nf1 = 3.507856 \nf2 = 3.5202 \nf2 &gt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.236068 \nNew Upper Test Point =  2.27864 \nNew Lower Test Point =  2.262379 \n \nIteration # 8 \nf1 = 3.524456 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.262379 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.28869 \n \nIteration # 9 \nf1 = 3.507856 \nf2 = 3.506851 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.27864 \nNew Lower Test Point =  2.28869 \nNew Upper Test Point =  2.294902 \n \nIteration # 10 \nf1 = 3.506851 \nf2 = 3.509765 \nf2 &gt; f1 \nNew Upper Bound = 2.294902 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.28869 \nNew Lower Test Point =  2.284852 \n \nIteration # 11 \nf1 = 3.5064 \nf2 = 3.506851 \nf2 &gt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.282479 \n \nIteration # 12 \nf1 = 3.506637 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.282479 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.286318 \n \nIteration # 13 \nf1 = 3.5064 \nf2 = 3.50645 \nf2 &gt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.282479 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.283945 \n \nIteration # 14 \nf1 = 3.506444 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.283945 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285412 \n \nIteration # 15 \nf1 = 3.5064 \nf2 = 3.506401 \nf2 &gt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.283945 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.284506 \n \nIteration # 16 \nf1 = 3.50641 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284506 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285066 \n \nIteration # 17 \nf1 = 3.5064 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284852 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285198 \n \nIteration # 18 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284852 \nNew Upper Test Point =  2.285066 \nNew Lower Test Point =  2.284984 \n \nIteration # 19 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284984 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285116 \n \nIteration # 20 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.285066 \nNew Lower Test Point =  2.285116 \nNew Upper Test Point =  2.285147 \n \nIteration # 21 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285147 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285116 \nNew Lower Test Point =  2.285097 \n \nIteration # 22 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285085 \n \nIteration # 23 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285085 \nNew Lower Test Point =  2.285097 \nNew Upper Test Point =  2.285104 \n \nIteration # 24 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285104 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285092 \n \nIteration # 25 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285092 \nNew Lower Test Point =  2.285089 \n \nIteration # 26 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285089 \nNew Lower Test Point =  2.285092 \nNew Upper Test Point =  2.285094 \n \nFinal Lower Bound = 2.285089 \nFinal Upper Bound = 2.285097 \nEstimated Minimizer = 2.285093 \n\n#Check out optim function\n?optim\n\nAnd for more complicated problems with many parameters, there are more complex algorithms, e.g. simulated annealing."
  },
  {
    "objectID": "Review-what are models.html#conclusion",
    "href": "Review-what are models.html#conclusion",
    "title": "Review - what are models?",
    "section": "",
    "text": "Understanding the basics of programming, mathematical modeling, and statistical model fitting are not only the basis to stock assessment, but also yields other transferable skills.\n\n\n\n\n\n\n\n\n\nThis process can be frustrating as you learn debugging, or quickly learning how to find your own mistakes. Some things to remember as you get better at this:\n\n\n\n\n\n\n\n\n\n\n\nThis text, pictures, and figures came with the Quarto package. Can you fit linear models to each of the three groups of penguins and add the lines to the plots?"
  },
  {
    "objectID": "Review-what are models.html#meet-the-penguins",
    "href": "Review-what are models.html#meet-the-penguins",
    "title": "Review - what are models?",
    "section": "",
    "text": "The penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\nThe plot below shows the relationship between flipper and bill lengths of these penguins."
  },
  {
    "objectID": "Surplus_production_models.html",
    "href": "Surplus_production_models.html",
    "title": "Surplus production models",
    "section": "",
    "text": "Surplus production is an economic term given to the excess yield (i.e., excess after replacement) resulting from a general production process.\nIn fisheries research, yield takes on a specific meaning, which is the surplus resulting from a population under a constant replacement scenario. This constant-replacement-scenario ensures that we are talking about a long-term concept, as catch fluctuates with error/uncertainty/diversity in the short-term. Therefore, usually when we are talking about yield we are talking about a theoretical long-term property of the population, given a constant fishing pressure over time. When we are talking about catch it is usually related to the result of fishing activities in a given year.\nThe amount of yield that a fish population produces changes with it’s population size. This change is the result of density dependence as we saw in logistic growth. That is, when population sizes are close to carrying capacity, they cannot grow as fast. It is also a result of exponential growth: if populations are very small, they amount of fish reproduced the following year is smaller simply because the parental population is smaller. These properties you will demonstrate just using the logistic growth equation:\n\n\n\n### How does amount produced by a population relate to population size? \n# This is done by: \n#1) Changing model to have max time = 100, K = 2000, and r = 0.2.\n#2) Saving Population size as a vector, then Amount Produced by that population size as a vector, and plot them.\n#3) Remember that \"Amount Produced\" by a given population size contributes to the next years population size. \n#It is not created by next years population size. The \"Amount Produced\" beyond replacement is also called \"Yield.\"\n\n#Exponential\n\nPP&lt;-vector(mode = \"numeric\", length = 100); PP0&lt;-1; rr&lt;- 0.2\nDD&lt;-vector(mode = \"numeric\", length = 100); \nTT&lt;-1:100 # not actually used within the loop but corresponds with i\n\nDD0&lt;-rr*PP0 # initial population growth - can be small so doesn't really affect model\nPP[1]&lt;-PP0+DD0 # initial population size + growth in first year of model\n\nfor(i in 2:100){ \n  \n  DD[i-1]&lt;-rr*PP[i-1]; \n\n  PP[i]&lt;-PP[i-1] + DD[i-1] \n  \n  } #DD is the production added to the previous years population size\n\nDD[100]&lt;-rr*PP[100] # this is just to finish series\n\npar(mfrow = c(1,3))\np1 &lt;- tibble(PP,DD) %&gt;% ggplot(aes(PP,DD)) + geom_point() + xlab(\"Population B\") + ylab(\"Increase in B  from t to t+1 (dB / dt)\")\n\n#Now do the same for the logistic equation\n\n#Logistic\nPP&lt;-vector(mode = \"numeric\", length = 100); PP0&lt;-1; rr&lt;- 0.2\nDD&lt;-vector(mode = \"numeric\", length = 100); \n# DD is a new storage compoenent. It will store the growth component of the logistic equation.\nTT&lt;-1:100# not actually used within the loop but corresponds with i\n\nKK&lt;-2000 # This is a new parameter - carrying capacity.\n\nTry filling in the bits of code missing by replacing the text beginning with ‘#&lt;’ and ending with ‘&gt;’.\n\n#FILL IN CODE BELOW. HINT: LOOK AT HOW DD0 IS DEFINED IN THE FIRST STEP IN COMPARISON\n#WITH HOW DD IS DEFINED INSIDE THE LOOP. THE FORMULA STRUCTURE STAYS THE \n#SAME BUT THE T SUBSCRIPTS MUST MATCH.\n\nDD0&lt;- rr*PP0*(1-PP0/KK)    # This keeps the equation structure the same. It is the initial DD component for the first year.\nPP[1]&lt;-PP0+DD0\n\nfor(i in 2:100){ \n  \n  DD[i-1]&lt;- #&lt;FILL IN CODE HERE &gt;\n  \n  PP[i]&lt;-PP[i-1] + DD[i-1] \n  \n  \n  } \n\nDD[100]&lt;- #&lt;FINISH OFF THE LAST STEP HERE&gt;\n\nAre you ready for the answer? See below.\n\n#FILL IN CODE BELOW. HINT: LOOK AT HOW DD0 IS DEFINED IN THE FIRST STEP IN COMPARISON\n#WITH HOW DD IS DEFINED INSIDE THE LOOP. THE FORMULA STRUCTURE STAYS THE \n#SAME BUT THE T SUBSCRIPTS MUST MATCH.\n\nDD0&lt;- rr*PP0*(1-PP0/KK)    # This keeps the equation structure the same. It is the initial DD component for the first year.\nPP[1]&lt;-PP0+DD0\n\nfor(i in 2:100){ \n  \n  DD[i-1]&lt;-rr*PP[i-1]*(1-PP[i-1]/KK); \n  \n  PP[i]&lt;-PP[i-1] + DD[i-1] \n  \n  \n  } \n\nDD[100]&lt;- rr*PP[100]*(1-PP[100]/KK)#FINISH OFF THE LAST STEP HERE\n  \np2 &lt;- tibble(PP,DD) %&gt;% ggplot(aes(PP,DD)) + geom_point() + xlab(\"Population B\") + ylab(\"Increase in N  from t to t+1 (dN / dt)\")\n\n#Now graph the population over time alongside the above:\np3 &lt;- tibble(PP,TT) %&gt;% ggplot(aes(TT,PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population B\")\n\ngridExtra::grid.arrange(p1,p2,p3, ncol = 3)\n\n\n\n\n\n\n\n\nYield is maximized at a certain value for population size, usually towards a middle value when assuming a logistic growth process. Population size can change for a variety of reasons, but one major reason in fisheries is the fishing pressure itself. Therefore, if we want higher yield, we want to fish an unfished population down to a size that maximizes (roughly) its yield production.\nTo understand how much we should fish in order to achieve the ‘right’ population size, we need three pieces of information:\n\nthe relationship between fishing activity and population size (effort x biomass).\nthe relationship between population size and growth rate (growth rate x biomass).\nhow growth rate translates into concrete fish removals in terms of tonnes or kg (growth rate and effort x catch).\n\nThese pieces of information are controlled by parameters. Choosing the parameters is done through fitting the model to data in order to have a well-supported parameterization.\n\n\n\nProduction models are useful in that they put these three pieces of information together into the same model, that can often be parameterized using data. The Schaeffer model is the simplest form of production models and uses the assumption that catch per unit effort is linearly related to population biomass.\nRemember the logistic growth equation:\n\\(N_t=N_{t−1}+r∗N_{t−1}∗(1−N_{t−1}/K)\\)\nAbove is enough to understand how growth rate and biomass relate to each other, but understading relationships with catch and effort require us to change this into an economic model including human activities. The first step is adding catch removals on the kg or tonnes scale, as this is the same scale on which population changes occur. To do this, we change from numbers to biomass (by multiplying by an average weight), and insert catch removals (\\(C_{t-1}\\)). As removals, catch will decrease the availability of biomass to fish in the next time step.\n\\(B_t=B_{t−1}+r∗B_{t−1}∗(1−B_{t−1}/K)- \\bf{C_{t-1}}\\)\nCatch removals can be further related to two properties: the amount of human activity (effort, \\(E\\)) and the effectiveness of that activity (catchability, \\(q\\)). Here, they can be included by further defining \\(C_{t-1}\\):\n\\(C_t=qEB_t\\)\nHere we see that within a each time step (\\(t\\) or \\(t-1\\)), catch is assumed to be linearly related to effort, linearly later to catchability, and linearly related to biomass of the stock. It indicates that as effort increases, catch will increase, also causing a reduction in total biomass ($B_t$) in the next time step. It also indicates that if any of these three components of the fishing activity increase (amount of activity, effectiveness of activity, or biomass available to remove), then so will the catch. We may be able to observe effort levels, but as \\(q\\) and \\(B\\) cannot be observed directly, they are confounded:\n\n\n\n\n\n\n\n\n\nIn addition, this assumption provides for a convenient relationship that is exploited when trying to fit such a model to data. Under this assumption defining catch, we can switch the equation around and also write:\n\\(C_t/E = qN_t\\)\nIn this form, it becomes clear that if we have data as a set of observations of catch per unit effort ($C_t/E$) and catch ($C_t$) over time, and there is enough contrast in the data, then we may be able to detect catchability and biomass.\nWe need contrast in the data to be able to detect informative parameter values (i.e., how growth changes at different population sizes and how easy the fish are to catch). Because we inherently don’t know and can’t observe the total population size of fish, we don’t know how catchable they are. If we assume high catchability, then we have to assume low population size, and vice versa. Getting the wrong answer could mean assuming that a large mass of removals can be taken from a small population, causing overfishing. Therefore, understanding where the best fishing level is not always easy or possible if there are not enough contrast in the data.\n\n\n\n\n\n\n\n\n\nWhat is contrast in the data? Having contrast in the data means that the population has been observed at various states, so that it’s responses to various conditions have been observed. If we have a CPUE and catch data series, to fit a surplus production model, there must be information in just those two data sources concerning:\n\ngrowth ($r$)\ncarrying capacity ($K$)\ncatchability ($q$)\ninitial population size ($N_{t=0}$)\n\nAs you go through the following exercise, think about the different ‘experiments’ needed to get information about these parameters. What states of population size (high/low) and fishing (heavy/light) should be observed to get this information at a minimum?\nAfter getting confident estimates of the above parameters, the system can then be analyzed (or simulated) to determine:\n\nAt what population biomass level is population growth the greatest? (Remember to distinguish between growth rate, which is in units per time step, and growth amount, catch, which is a single unit of removal in kg or tonnes in a time step)\nAt what effort level do you also get the highest yield?\n\nFurther considerations include:\n\nWhat is maximum sustainable yield (MSY)? How is this value related to r and K?\nIf cost is linearly related to effort, what is maximum economic yield (MEY)?\nFitting a surplus production model is similar to fitting a logistic growth model, as both are non-linear. Unlike in the previous exercise, biomass is unknown in this example, as is normal in fisheries, so CPUE is used as an index (as supported in the above theory). Keep in mind: where is the error (i.e., the statistical model)? What are the parameters? What objective function are we using?\n\n\n\n\n\n###------------------------------Exercise 5: Production functions-----------------------###\n\n#Now add population removals - catch, which is a linear function of fishing effort, \n#catchability, and population size\n\n#Here you will run a population for a long time at a certain effort level. \n#Notice that the value at which the population stabilizes is no longer carrying \n#capacity, because catch has been removed. Also note how catch is related to \n#population size. \n\nPP&lt;-vector(mode = \"numeric\", length = 500); PP0&lt;-1; rr&lt;- 0.2\nDD&lt;-vector(mode = \"numeric\", length = 500);\nCC&lt;-vector(mode = \"numeric\", length = 500); \nEffort&lt;-0.8; Catchability&lt;-0.1\nTT&lt;-1:500\nKK&lt;-2000\n\nTry filling in the bits of code missing by replacing the text beginning with ‘#&lt;’ and ending with ‘&gt;’.\n\n#FILL IN CODE BELOW. HINT: LOOK AT HOW DD0 IS DEFINED ABOVE IN COMPARISON\n#WITH HOW DD IS DEFINED INSIDE THE LOOP. THE FORMULA STRUCTURE STAYS THE \n#SAME BUT THE T SUBSCRIPTS MUST MATCH.\n\nDD0&lt;-rr*PP0*(1-PP0/KK)\n\nCC0&lt;-PP0*Effort*Catchability; \n\nPP[1]&lt;-PP0 + DD0 - CC0\n\nfor(i in 2:500){ \n\n  DD[i-1]&lt;-  #&lt;FILL IN CODE HERE &gt;\n\n  CC[i-1]&lt;- PP[i-1]*Effort*Catchability\n\n  PP[i]&lt;- PP[i-1] + DD[i-1] - CC[i-1]\n  \n  } \n\nDD[500]&lt;-  #&lt;FINISH OFF THE LAST STEP HERE &gt;\nCC[500]&lt;- PP[500]*Effort*Catchability\n\nAre you ready for the answer? See below.\n\nDD0&lt;-rr*PP0*(1-PP0/KK)\n\nCC0&lt;-PP0*Effort*Catchability; \n\nPP[1]&lt;-PP0 + DD0 - CC0\n\nfor(i in 2:500){ \n\n  DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1]/KK)\n\n  CC[i-1]&lt;- PP[i-1]*Effort*Catchability\n\n  PP[i]&lt;- PP[i-1] + DD[i-1] - CC[i-1]\n  \n  } \n\nDD[500]&lt;- rr*PP[500]*(1-PP[500]/KK)\nCC[500]&lt;- PP[500]*Effort*Catchability\n\n\np1 &lt;- tibble(CC,TT) %&gt;% ggplot(aes(TT, CC)) + geom_point() + xlab(\"Time\") + ylab(\"Catch\")\np2 &lt;- tibble(PP,TT) %&gt;% ggplot(aes(TT, PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population Biomass\")\np3 &lt;- tibble(PP,CC) %&gt;% ggplot(aes(PP, CC)) + geom_point() + ylab(\"Catch\") + xlab(\"Population Biomass\")\n\ngridExtra::grid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n#This graph just shows you that at a single value of effort, your catch is directly related to your population size. As your population grows, if your effort stays the same,  effort*catchability defines the proportion of the population taken out. Since effort is not at a high value (0.8), it does not affect the population dynamics much, so catch follows the same pattern as population (directly related).\n\nThe above results show a single version of how the population could grow, alongside catch, given that it starts from a low value and is subject to a single effort value. That means this is a single snapshot of the population’s expected dynamics. It shows simulated contrast (not contrast in data) in how the population grows at high and low population levels, and the catch results, but it does not show contrast in high/low fishing effort.\nIn this next exercise, we want to determine the level of fishing of effort that leads to the highest annual catch removals in the long-term. Therefore, we:\n\nonly consider values at the end of a simulated time series to reflect the equilibrium results (i.e., values when \\(B_{t-1}=B_t\\)). This reflects long-term results of fishing.\nExperiment with effort levels by trying a variety of them, and looking at equilibrium states as a function of different effort values.\n\n\n\n#Now try to find the Effort value that lead to CC = MSY = (rr*KK)/4 in the long term. \n#[Hint: try looping over values for effort and plotting the results below.]\n\neff&lt;-seq(0,2,0.1) # values of effort to loop over\nYY&lt;-vector(mode = \"numeric\", length = length(eff))\nXX&lt;-vector(mode = \"numeric\", length = length(eff))\n\nCatchability&lt;-0.1\nTT&lt;-1:500\nKK&lt;-2000\nPP0&lt;-1; rr&lt;- 0.2\n\nfor(j in 1:length(eff)){\n  \n  PP&lt;-vector(mode = \"numeric\", length = 500); \n  DD&lt;-vector(mode = \"numeric\", length = 500);\n  CC&lt;-vector(mode = \"numeric\", length = 500); \n  \n  Effort&lt;- eff[j] \n  \n  DD0&lt;-rr*PP0*(1-PP0/KK)\n  CC0&lt;-PP0*Effort*Catchability; \n  PP[1]&lt;-PP0 + DD0 - CC0\n  \n  for(i in 2:500){ \n    \n    DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1]/KK)\n    CC[i-1]&lt;- PP[i-1]*Effort*Catchability\n    PP[i]&lt;- PP[i-1] + DD[i-1] - CC[i-1]\n    \n  } \n  \n  DD[500]&lt;- rr*PP[500]*(1-PP[500]/KK)\n  CC[500]&lt;- PP[500]*Effort*Catchability\n  \n  #SAVE RESULTS: LAST (EQUILIBRIUM) VALUE OF CATCH AND LAST VALUE OF POPULATION \n  YY[j] &lt;- CC[500] # Equilibrium Yield\n  XX[j] &lt;-  PP[500] #or PP[500] # Equilibrium Population B\n\n  }\n\n\np1 &lt;- tibble(XX,YY,eff) %&gt;% ggplot(aes(eff, YY)) + geom_point() + ylab(\"Yield\") + xlab( \"Effort\")\np2 &lt;- tibble(XX,YY,eff) %&gt;% ggplot(aes(eff, XX)) + geom_point() + ylab(\"Population B\") + xlab( \"Effort\")\np3 &lt;- tibble(XX,YY,eff) %&gt;% mutate(CPUE = YY/eff) %&gt;% ggplot(aes(eff, CPUE)) + geom_point() + xlab(\"Effort\") \n\ngridExtra::grid.arrange(p1, p2, p3, ncol = 3)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n#Q - Find on each graph where MSY is located. How are the last 2 graphs related? To answer the following questions, it helps to change the r and K values. How is MSY related to K and r? At what population size does this occur (Nmsy)? How does this relate to K and r? What does r affect? \n\n\n#Q - Note that these graphs demonstrate the long-term equilibrium results \n#under different fishing strategies. It is dome-shaped. We saw a graph at the \n#beginning of this exercise that was also dome-shaped (dn/dt of the population \n#over population size); however this graph is not based on equilibrium results. \n#How are these two graphs related?\n\nHere is a demonstration of the difference between maximum sustainable yield (MSY) and maximum economic yield (MEY) concepts.\n\n#If cost linearly increases with effort...\nCost&lt;-eff*40\n#Q - then what is maximum economic yield (MEY)? [Hint: MEY is the yield with the greatest profit; that is, the greatest difference between Yield and cost]\ntibble(YY, eff, Cost) %&gt;% ggplot(aes(eff, YY)) + geom_point() + ylab(\"Yield\") + xlab(\"Effort\") + geom_point(aes(eff, Cost), shape = 2)\n\n\n\n\n\n\n\n# MSY = K*r/4\n# Nmsy = K/2\n\neff[which.max(YY)] #MSY\n\n[1] 1\n\neff[which.max(YY - Cost)] #MEY\n\n[1] 0.8\n\n\n\n\n\n\n#Extra - Now look up the equation for a Pella-Tomlinson production function and \n#repeat the above exercise to plot Pella-Tomlinson surplus production function alongside\n#Schaeffer production function on the same figure (hint: Schaeffer is a special case of the\n#Pella-Tomlinson where the extra parameter (exponent) is set to 1).\n#How is the shape of the MSY relationship changed by the extra parmaeter?\n\n\n\n\nBelow is a demonstration of a quick-and-dirty shortcut to finding some of the equilibrium properties of production models based on simple linear models fitted to CPUE and effort data. This method was useful when computational speed was an issue and demonstrates further how some of the equilibrium values can be obtained, although some assumptions are not easy to uphold, so it is not used much today. This example, found here, and the false assumption explained also below.\nIn addition, another assumption of the Schaeffer model brought up in this example is the assumption that CPUE is directly related to biomass (i.e., CPUE reflects a constant proportion of biomass at all population sizes, determined by catchability \\(q\\)). This is also not always the case, so this example also tests an alternate model of a log-linear relationship, yielding greater CPUE proportion at lower biomass levels.\n\n######-------FAO shortcut from http://www.fao.org/3/w5449e/w5449e10.pdf-------########\n\nYrs &lt;- 1969:1977\nYield &lt;- c(50, 49, 47.5, 45, 51, 56, 66, 58, 52)\nEffort &lt;- c(623, 628, 520, 513, 661, 919, 1158, 1970, 1317)\n\nlength(Yrs); length(Yield); length(Effort)\n\n[1] 9\n\n\n[1] 9\n\n\n[1] 9\n\nSchaefer.lm &lt;- lm(Yield/Effort ~ Effort)\nsummary(Schaefer.lm)\n\n\nCall:\nlm(formula = Yield/Effort ~ Effort)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0104590 -0.0014437  0.0002383  0.0033219  0.0074826 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.064e-01  4.655e-03  22.855 7.78e-08 ***\nEffort      -4.285e-05  4.518e-06  -9.486 3.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.006199 on 7 degrees of freedom\nMultiple R-squared:  0.9278,    Adjusted R-squared:  0.9175 \nF-statistic: 89.98 on 1 and 7 DF,  p-value: 3.026e-05\n\nSchaefer.glm &lt;- glm(Yield/Effort ~ Effort)\nsummary(Schaefer.glm)\n\n\nCall:\nglm(formula = Yield/Effort ~ Effort)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.064e-01  4.655e-03  22.855 7.78e-08 ***\nEffort      -4.285e-05  4.518e-06  -9.486 3.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3.84281e-05)\n\n    Null deviance: 0.0037268  on 8  degrees of freedom\nResidual deviance: 0.0002690  on 7  degrees of freedom\nAIC: -62.221\n\nNumber of Fisher Scoring iterations: 2\n\na &lt;-coef(Schaefer.lm)[1]\nb &lt;-coef(Schaefer.lm)[2]\nMSY &lt;- -0.25*a^2/b\nnames(MSY) &lt;- \"MSY\"\nMSY\n\n     MSY \n66.02075 \n\nFox.lm &lt;- lm(log(Yield/Effort) ~ Effort)\nsummary(Fox.lm)\n\n\nCall:\nlm(formula = log(Yield/Effort) ~ Effort)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.158062 -0.017854  0.006417  0.054975  0.084179 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.040e+00  5.720e-02  -35.66 3.54e-09 ***\nEffort      -7.851e-04  5.551e-05  -14.14 2.10e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07618 on 7 degrees of freedom\nMultiple R-squared:  0.9662,    Adjusted R-squared:  0.9614 \nF-statistic:   200 on 1 and 7 DF,  p-value: 2.097e-06\n\nc &lt;-coef(Fox.lm)[1]\nd &lt;-coef(Fox.lm)[2]\nMSY &lt;- -(1/d)*exp(c-1)\nnames(MSY) &lt;- \"MSY\"\nMSY\n\n     MSY \n60.93882 \n\n\nAs mentioned before, this method is not recommended as it requires the assumption that the values you are observing are long-term results, or equilibrium values, as gathered in the above exercises. This is obviously not a correct assumption, as they are observations taken in consecutive years, although in many of the cases you could argue that they are close to equilibrium values when observed. also shows how this assumption is not correct, especially at the initiation of a new fishery when the population is transitioning from an unfished population to a fished one, creating a mismatch between short- and long-term expectations of CPUE. In the earliest years of a fishery, CPUE tends to more greatly exceed long-term expectations for CPUE given a constant effort level, as biomass has not had a chance to decrease over time. See this figure taken directly from the FAO report linked above:"
  },
  {
    "objectID": "Surplus_production_models.html#lets-talk-about-a-shortcut",
    "href": "Surplus_production_models.html#lets-talk-about-a-shortcut",
    "title": "Surplus production models",
    "section": "",
    "text": "Below is a demonstration of a quick-and-dirty shortcut to finding some of the equilibrium properties of production models based on simple linear models fitted to CPUE and effort data. This method was useful when computational speed was an issue and demonstrates further how some of the equilibrium values can be obtained, although some assumptions are not easy to uphold, so it is not used much today. This example, found here, and the false assumption explained also below.\nIn addition, another assumption of the Schaeffer model brought up in this example is the assumption that CPUE is directly related to biomass (i.e., CPUE reflects a constant proportion of biomass at all population sizes, determined by catchability \\(q\\)). This is also not always the case, so this example also tests an alternate model of a log-linear relationship, yielding greater CPUE proportion at lower biomass levels.\n\n######-------FAO shortcut from http://www.fao.org/3/w5449e/w5449e10.pdf-------########\n\nYrs &lt;- 1969:1977\nYield &lt;- c(50, 49, 47.5, 45, 51, 56, 66, 58, 52)\nEffort &lt;- c(623, 628, 520, 513, 661, 919, 1158, 1970, 1317)\n\nlength(Yrs); length(Yield); length(Effort)\n\n[1] 9\n\n\n[1] 9\n\n\n[1] 9\n\nSchaefer.lm &lt;- lm(Yield/Effort ~ Effort)\nsummary(Schaefer.lm)\n\n\nCall:\nlm(formula = Yield/Effort ~ Effort)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0104590 -0.0014437  0.0002383  0.0033219  0.0074826 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.064e-01  4.655e-03  22.855 7.78e-08 ***\nEffort      -4.285e-05  4.518e-06  -9.486 3.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.006199 on 7 degrees of freedom\nMultiple R-squared:  0.9278,    Adjusted R-squared:  0.9175 \nF-statistic: 89.98 on 1 and 7 DF,  p-value: 3.026e-05\n\nSchaefer.glm &lt;- glm(Yield/Effort ~ Effort)\nsummary(Schaefer.glm)\n\n\nCall:\nglm(formula = Yield/Effort ~ Effort)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.064e-01  4.655e-03  22.855 7.78e-08 ***\nEffort      -4.285e-05  4.518e-06  -9.486 3.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3.84281e-05)\n\n    Null deviance: 0.0037268  on 8  degrees of freedom\nResidual deviance: 0.0002690  on 7  degrees of freedom\nAIC: -62.221\n\nNumber of Fisher Scoring iterations: 2\n\na &lt;-coef(Schaefer.lm)[1]\nb &lt;-coef(Schaefer.lm)[2]\nMSY &lt;- -0.25*a^2/b\nnames(MSY) &lt;- \"MSY\"\nMSY\n\n     MSY \n66.02075 \n\nFox.lm &lt;- lm(log(Yield/Effort) ~ Effort)\nsummary(Fox.lm)\n\n\nCall:\nlm(formula = log(Yield/Effort) ~ Effort)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.158062 -0.017854  0.006417  0.054975  0.084179 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.040e+00  5.720e-02  -35.66 3.54e-09 ***\nEffort      -7.851e-04  5.551e-05  -14.14 2.10e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07618 on 7 degrees of freedom\nMultiple R-squared:  0.9662,    Adjusted R-squared:  0.9614 \nF-statistic:   200 on 1 and 7 DF,  p-value: 2.097e-06\n\nc &lt;-coef(Fox.lm)[1]\nd &lt;-coef(Fox.lm)[2]\nMSY &lt;- -(1/d)*exp(c-1)\nnames(MSY) &lt;- \"MSY\"\nMSY\n\n     MSY \n60.93882 \n\n\nAs mentioned before, this method is not recommended as it requires the assumption that the values you are observing are long-term results, or equilibrium values, as gathered in the above exercises. This is obviously not a correct assumption, as they are observations taken in consecutive years, although in many of the cases you could argue that they are close to equilibrium values when observed. also shows how this assumption is not correct, especially at the initiation of a new fishery when the population is transitioning from an unfished population to a fished one, creating a mismatch between short- and long-term expectations of CPUE. In the earliest years of a fishery, CPUE tends to more greatly exceed long-term expectations for CPUE given a constant effort level, as biomass has not had a chance to decrease over time. See this figure taken directly from the FAO report linked above:"
  },
  {
    "objectID": "Simple_population_models.html#exponential-models",
    "href": "Simple_population_models.html#exponential-models",
    "title": "Simple population models",
    "section": "Exponential models",
    "text": "Exponential models\nThe simplest of all population models is that of exponential growth, representing some rate of reproduction.\nWikipedia example: if a species of bacteria doubles every 10 minutes, starting with 1 bacteria, how many bacteria would there be after 1 hour?\nThere are 3 pieces of information here, which we can represent with mathematical notation:\n\n\\(N_0\\) = initial population = 1\n\n\n\nR = rate of increase = 2 per 10 minutes (or 2/(10 mins/60 mins) in per-hour units)\nt = total number of time steps = 60 mins/10 mins (or 1 hour/ (10/60 hour)) = 6\n\nHere is an equation that describes the process of increasing over time:\n\\(N_t = N_0R^t\\)\n\\(N_6 = 1*2^6 = 64\\)\nAfter one hour, or 6 ten-minute intervals, there would be sixty-four bacteria.\nExponential growth can be written a variety of ways, all of which are equivalent but allow us to think of the problem a different way.\n\\(N_t = N_0R^t = RN_{t-1} = N_{t-1} + rN_{t-1}\\)\nThe Wikipedia example shows that if the initial population, rate of replication per time step, and the number of time steps are known, then it is possible to calculate \\(N_t\\) as \\(N_0R^t\\).\nThe equation \\(N_t=RN_{t-1}\\) demonstrates how the number at the next time step can be calculated from the value at time step \\(t\\). The replication rate \\(R\\) is only applied one time because we are only transitioning one time step.\nIn the form \\(N_t = N_{t-1} + rN_{t-1}\\), we can see that, instead of replication being expressed as a multiplier \\(R\\), it can be expressed as an additive increase when a proportion of last year’s population size is added to it.\nIn this additive form \\(N_t = N_{t-1} + rN_{t-1}\\), we can see that the added term can be separated out as that which goes beyond the same value as last year, or a rate indicating how many are added per time step. That is, the change in \\(N\\) over the change in \\(t\\) can be expressed as \\(dN/dt = rN\\).\n\nExercise\nIn this exercise, you will demonstrate that the above relationships among the different forms hold true.\nBut first, notice in the last two forms that \\(R\\) and \\(r\\) are different and cannot represent the same value. How are they related? Can you do some algebra with the equation below to show what \\(R\\) equals? (hint: isolate \\(R\\) on the left-hand side)\n\\(RN_{t-1} = N_{t-1} + rN_{t-1}\\)\n\n#Below, TT and PP are storage vectors for your time step (TT) \n#and your population abundance measure at that time step. So TT[1] shows year 1 \n#as your first time step; PP[1] shows your population abundance at TT[1]. \nPP&lt;-vector(mode = \"numeric\", length = 5); RR&lt;-exp(1); PP0&lt;-1\nTT&lt;-1:5\nPP[1]; TT[1]\n\n[1] 0\n\n\n[1] 1\n\n#RR and PP0 above are the 2 parameters included in this model (in contrast to storage). \n#The 2 parameters affect the values resulting in PP, based on TT). \n#Therefore, they affect the shape of your population growth function. \n#TT is input, PP is output, and the PP output values depend on parameters RR and PP0.\n# See how equations in slides are translated into the model below. Also try to keep \n# track of your parameters versus storage components as the models presented become \n# more complex.\n\n#This is the model - it updates your expected population abundance at each of the \n#5 time step, assuming the population grows exponentially.\nfor(i in 1:5){ PP[i]&lt;-PP0*exp(TT[i])}\n\n#Note that the model can also be written as below with the same result.\nfor(i in 1:5){ PP[i]&lt;-PP0*RR^TT[i]}\nPP\n\n[1]   2.718282   7.389056  20.085537  54.598150 148.413159\n\ntibble(PP,TT) %&gt;% ggplot(aes(TT,PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population N\")\n\n\n\n\n\n\n\n\n\n#### Still Other ways to write the exponential model ... \nPP&lt;-vector(mode = \"numeric\", length = 5); PP0&lt;-1\nTT&lt;-1:5\nPP[1]&lt;-PP0*RR\nfor(i in 2:5){ PP[i]&lt;-RR*PP[i-1]} \nPP\n\n[1]   2.718282   7.389056  20.085537  54.598150 148.413159\n\ntibble(PP,TT) %&gt;% ggplot(aes(TT,PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population N\")\n\n\n\n\n\n\n\n\n\n# If RR = 1 + rr, then this is the same as PP[i]&lt;-(1 + rr)*PP[i-1] = PP[i-1] + rr*PP[i-1].\n#  Change the code above to the last form to show this: \n\nPP&lt;-vector(mode = \"numeric\", length = 5); PP0&lt;-1\nTT&lt;-1:5\nrr &lt;- RR - 1\nPP[1]&lt;-PP0 + rr*PP0\nPP[1]&lt;-PP0*(1 + rr)\nfor(i in 2:5){ PP[i]&lt;-PP[i-1] + rr*PP[i-1]} \nfor(i in 2:5){ PP[i]&lt;-PP[i-1]*(1 + rr)} \nPP\n\n[1]   2.718282   7.389056  20.085537  54.598150 148.413159\n\ntibble(PP,TT) %&gt;% ggplot(aes(TT,PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population N\")"
  },
  {
    "objectID": "Simple_population_models.html#logistic-growth",
    "href": "Simple_population_models.html#logistic-growth",
    "title": "Simple population models",
    "section": "Logistic growth",
    "text": "Logistic growth\nExponential models are really only useful for very small organisms like bacteria. Logistic growth models are a step closer to representing population dynamics of most other organisms big enough to be visible to the naked eye, such as fish.\nThe logistic growth equation contains the exponential growth equation last analyzed in the above section \\(N_t=N_{t-1} + rN_{t-1}\\). But, it includes another parameter:\n\\(N_t = N_{t-1} + r*N_{t-1}*(1-N_{t-1}/K)\\)\n\nWhat differs from the exponential model?\nWhat is the new parameter? What is it’s relationship with the idea that it is needed to represent population dynamics of organisms big enough to be visible to the naked eye?\nConsidering the equation for \\(dN/dt\\) in the exponential model, what is the equation for \\(dN/dt\\) in the logistic model?\nDo a little experiment in your mind. First pick a value for \\(r\\) (which remember is a percentage increase in \\(N\\) over a time step). What happens to \\(dN/dt\\) when\n\n\\(N_{t-1} \\sim 0\\)\n\\(N_{t-1} \\sim K/2\\)\n\\(N_{t-1} \\sim K\\)\nWith these values, draw a plot of \\(dN/dt\\) as a function of \\(N_{t-1}\\)\n\n\n\n# Adding density dependence into the code below to reflect logistic growth. \n# What section fo the code was added?\n\nPP&lt;-vector(mode = \"numeric\", length = 10); PP0&lt;-2; rr&lt;-1 ;\nTT&lt;-1:10\nKK&lt;-10\nPP[1]&lt;-PP0+rr*PP0*(1-PP0/KK)\nfor(i in 2:10){ PP[i]&lt;-PP[i-1] + rr*PP[i-1]*(1-PP[i-1]/KK)} \nPP\n\n [1]  3.600000  5.904000  8.322278  9.718525  9.992077  9.999994 10.000000\n [8] 10.000000 10.000000 10.000000\n\ntibble(PP,TT) %&gt;% ggplot(aes(TT,PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population N\")\n\n\n\n\n\n\n\n\n\n# Now Change rr and K values above to see how they affect the model \n#Q2.0: Try KK between values of 10 and 70. What does it control?\n\nPP&lt;-vector(mode = \"numeric\", length = 10); PP0&lt;-2; rr&lt;-1 ;\nTT&lt;-1:10\nKK&lt;-70\nPP[1]&lt;-PP0+rr*PP0*(1-PP0/KK)\nfor(i in 2:10){ PP[i]&lt;-PP[i-1] + rr*PP[i-1]*(1-PP[i-1]/KK)} \nPP\n\n [1]  3.942857  7.663627 14.488237 25.977774 42.314908 59.050510 68.287267\n [8] 69.958093 69.999975 70.000000\n\ntibble(PP,TT) %&gt;% ggplot(aes(TT,PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population N\")\n\n\n\n\n\n\n\n\n\n#Q2.1: Set KK at 50 and rr at 2. Why do you get strange shapes at higher rr values?\n\nPP&lt;-vector(mode = \"numeric\", length = 1500); PP0&lt;-2; rr&lt;-2 ;\nTT&lt;-1:1500\nKK&lt;-50\nPP[1]&lt;-PP0+rr*PP0*(1-PP0/KK)\nfor(i in 2:1500){ PP[i]&lt;-PP[i-1] + rr*PP[i-1]*(1-PP[i-1]/KK)} \nPP\n\n   [1]  5.84000 16.15578 38.02696 56.23889 42.20416 55.36484 43.48390 54.81772\n   [9] 44.25387 54.42541 44.79122 54.12352 45.19634 53.88066 45.51696 53.67913\n  [17] 45.77943 53.50804 45.99970 53.36020 46.18816 53.23064 46.35188 53.11577\n  [25] 46.49591 53.01294 46.62395 52.92014 46.73877 52.83581 46.84252 52.75869\n  [33] 46.93689 52.68780 47.02323 52.62233 47.10261 52.56160 47.17593 52.50505\n  [41] 47.24394 52.45223 47.30723 52.40273 47.36635 52.35621 47.42173 52.31237\n  [49] 47.47374 52.27098 47.52273 52.23180 47.56897 52.19464 47.61271 52.15933\n  [57] 47.65417 52.12572 47.69354 52.09367 47.73099 52.06307 47.76667 52.03382\n  [65] 47.80073 52.00580 47.83327 51.97894 47.86441 51.95316 47.89425 51.92839\n  [73] 47.92287 51.90455 47.95035 51.88160 47.97678 51.85948 48.00221 51.83814\n  [81] 48.02670 51.81754 48.05032 51.79763 48.07311 51.77837 48.09512 51.75973\n  [89] 48.11640 51.74168 48.13698 51.72419 48.15690 51.70722 48.17620 51.69075\n  [97] 48.19490 51.67476 48.21304 51.65923 48.23065 51.64412 48.24775 51.62944\n [105] 48.26436 51.61514 48.28051 51.60122 48.29622 51.58766 48.31151 51.57445\n [113] 48.32639 51.56157 48.34089 51.54900 48.35502 51.53674 48.36880 51.52477\n [121] 48.38223 51.51308 48.39534 51.50166 48.40814 51.49050 48.42064 51.47959\n [129] 48.43285 51.46892 48.44478 51.45847 48.45644 51.44826 48.46784 51.43826\n [137] 48.47900 51.42846 48.48992 51.41887 48.50061 51.40947 48.51107 51.40025\n [145] 48.52132 51.39122 48.53136 51.38237 48.54120 51.37368 48.55084 51.36516\n [153] 48.56030 51.35679 48.56957 51.34858 48.57867 51.34052 48.58760 51.33261\n [161] 48.59636 51.32483 48.60496 51.31720 48.61340 51.30969 48.62170 51.30231\n [169] 48.62985 51.29506 48.63785 51.28793 48.64572 51.28092 48.65345 51.27402\n [177] 48.66105 51.26723 48.66853 51.26056 48.67588 51.25399 48.68311 51.24752\n [185] 48.69023 51.24115 48.69723 51.23488 48.70412 51.22870 48.71091 51.22262\n [193] 48.71758 51.21663 48.72416 51.21073 48.73064 51.20491 48.73702 51.19918\n [201] 48.74330 51.19353 48.74949 51.18796 48.75559 51.18247 48.76160 51.17705\n [209] 48.76753 51.17171 48.77337 51.16644 48.77914 51.16124 48.78482 51.15612\n [217] 48.79042 51.15106 48.79595 51.14606 48.80140 51.14114 48.80678 51.13627\n [225] 48.81208 51.13147 48.81732 51.12673 48.82249 51.12205 48.82759 51.11743\n [233] 48.83262 51.11286 48.83760 51.10836 48.84251 51.10390 48.84735 51.09950\n [241] 48.85214 51.09516 48.85687 51.09086 48.86154 51.08662 48.86615 51.08242\n [249] 48.87071 51.07828 48.87522 51.07418 48.87967 51.07013 48.88407 51.06612\n [257] 48.88842 51.06216 48.89271 51.05824 48.89696 51.05437 48.90116 51.05054\n [265] 48.90531 51.04675 48.90942 51.04301 48.91348 51.03930 48.91749 51.03563\n [273] 48.92147 51.03200 48.92539 51.02841 48.92928 51.02486 48.93312 51.02135\n [281] 48.93693 51.01787 48.94069 51.01442 48.94441 51.01102 48.94810 51.00764\n [289] 48.95174 51.00430 48.95535 51.00100 48.95892 50.99772 48.96246 50.99448\n [297] 48.96596 50.99127 48.96942 50.98809 48.97285 50.98494 48.97625 50.98183\n [305] 48.97961 50.97874 48.98294 50.97568 48.98624 50.97265 48.98951 50.96965\n [313] 48.99274 50.96667 48.99595 50.96373 48.99912 50.96081 49.00227 50.95791\n [321] 49.00538 50.95505 49.00847 50.95221 49.01152 50.94939 49.01455 50.94660\n [329] 49.01756 50.94384 49.02053 50.94109 49.02348 50.93838 49.02640 50.93568\n [337] 49.02930 50.93301 49.03217 50.93037 49.03501 50.92774 49.03783 50.92514\n [345] 49.04063 50.92256 49.04340 50.92000 49.04615 50.91746 49.04887 50.91494\n [353] 49.05157 50.91245 49.05425 50.90997 49.05691 50.90752 49.05954 50.90508\n [361] 49.06215 50.90267 49.06474 50.90027 49.06731 50.89789 49.06986 50.89554\n [369] 49.07238 50.89320 49.07489 50.89088 49.07738 50.88857 49.07984 50.88629\n [377] 49.08229 50.88402 49.08472 50.88177 49.08713 50.87954 49.08952 50.87732\n [385] 49.09189 50.87512 49.09424 50.87294 49.09658 50.87078 49.09889 50.86863\n [393] 49.10119 50.86649 49.10348 50.86437 49.10574 50.86227 49.10799 50.86019\n [401] 49.11022 50.85811 49.11243 50.85606 49.11463 50.85401 49.11681 50.85199\n [409] 49.11898 50.84997 49.12113 50.84798 49.12326 50.84599 49.12538 50.84402\n [417] 49.12748 50.84207 49.12957 50.84012 49.13165 50.83819 49.13370 50.83628\n [425] 49.13575 50.83437 49.13778 50.83248 49.13979 50.83061 49.14180 50.82874\n [433] 49.14378 50.82689 49.14576 50.82505 49.14772 50.82323 49.14967 50.82141\n [441] 49.15160 50.81961 49.15352 50.81782 49.15543 50.81604 49.15732 50.81427\n [449] 49.15921 50.81252 49.16108 50.81077 49.16293 50.80904 49.16478 50.80732\n [457] 49.16661 50.80561 49.16843 50.80391 49.17024 50.80222 49.17204 50.80054\n [465] 49.17383 50.79887 49.17560 50.79721 49.17737 50.79556 49.17912 50.79393\n [473] 49.18086 50.79230 49.18259 50.79068 49.18431 50.78908 49.18602 50.78748\n [481] 49.18771 50.78589 49.18940 50.78432 49.19108 50.78275 49.19275 50.78119\n [489] 49.19440 50.77964 49.19605 50.77810 49.19768 50.77657 49.19931 50.77505\n [497] 49.20093 50.77353 49.20253 50.77203 49.20413 50.77053 49.20572 50.76905\n [505] 49.20730 50.76757 49.20886 50.76610 49.21042 50.76464 49.21197 50.76319\n [513] 49.21351 50.76174 49.21505 50.76031 49.21657 50.75888 49.21809 50.75746\n [521] 49.21959 50.75605 49.22109 50.75464 49.22258 50.75325 49.22406 50.75186\n [529] 49.22553 50.75048 49.22699 50.74911 49.22845 50.74774 49.22989 50.74638\n [537] 49.23133 50.74503 49.23276 50.74369 49.23419 50.74235 49.23560 50.74102\n [545] 49.23701 50.73970 49.23841 50.73839 49.23980 50.73708 49.24119 50.73578\n [553] 49.24256 50.73449 49.24393 50.73320 49.24529 50.73192 49.24665 50.73065\n [561] 49.24800 50.72938 49.24934 50.72812 49.25067 50.72687 49.25200 50.72562\n [569] 49.25332 50.72438 49.25463 50.72315 49.25593 50.72192 49.25723 50.72070\n [577] 49.25852 50.71948 49.25981 50.71828 49.26109 50.71707 49.26236 50.71588\n [585] 49.26363 50.71468 49.26488 50.71350 49.26614 50.71232 49.26738 50.71115\n [593] 49.26862 50.70998 49.26986 50.70882 49.27108 50.70766 49.27231 50.70651\n [601] 49.27352 50.70537 49.27473 50.70423 49.27593 50.70310 49.27713 50.70197\n [609] 49.27832 50.70085 49.27951 50.69973 49.28069 50.69862 49.28186 50.69751\n [617] 49.28303 50.69641 49.28419 50.69531 49.28535 50.69422 49.28650 50.69314\n [625] 49.28764 50.69206 49.28878 50.69098 49.28992 50.68991 49.29105 50.68885\n [633] 49.29217 50.68779 49.29329 50.68673 49.29441 50.68568 49.29551 50.68463\n [641] 49.29662 50.68359 49.29771 50.68256 49.29881 50.68153 49.29990 50.68050\n [649] 49.30098 50.67948 49.30206 50.67846 49.30313 50.67745 49.30420 50.67644\n [657] 49.30526 50.67543 49.30632 50.67443 49.30737 50.67344 49.30842 50.67245\n [665] 49.30946 50.67146 49.31050 50.67048 49.31154 50.66950 49.31257 50.66853\n [673] 49.31359 50.66756 49.31461 50.66660 49.31563 50.66564 49.31664 50.66468\n [681] 49.31765 50.66373 49.31865 50.66278 49.31965 50.66184 49.32064 50.66090\n [689] 49.32163 50.65996 49.32262 50.65903 49.32360 50.65810 49.32458 50.65718\n [697] 49.32555 50.65626 49.32652 50.65534 49.32748 50.65443 49.32844 50.65352\n [705] 49.32940 50.65261 49.33035 50.65171 49.33130 50.65082 49.33224 50.64992\n [713] 49.33318 50.64903 49.33412 50.64815 49.33505 50.64726 49.33598 50.64638\n [721] 49.33690 50.64551 49.33782 50.64464 49.33874 50.64377 49.33965 50.64290\n [729] 49.34056 50.64204 49.34147 50.64118 49.34237 50.64033 49.34327 50.63948\n [737] 49.34416 50.63863 49.34505 50.63779 49.34594 50.63695 49.34682 50.63611\n [745] 49.34770 50.63528 49.34858 50.63444 49.34945 50.63362 49.35032 50.63279\n [753] 49.35119 50.63197 49.35205 50.63115 49.35291 50.63034 49.35377 50.62953\n [761] 49.35462 50.62872 49.35547 50.62791 49.35631 50.62711 49.35716 50.62631\n [769] 49.35800 50.62552 49.35883 50.62472 49.35967 50.62393 49.36049 50.62315\n [777] 49.36132 50.62236 49.36214 50.62158 49.36296 50.62080 49.36378 50.62003\n [785] 49.36459 50.61926 49.36540 50.61849 49.36621 50.61772 49.36702 50.61696\n [793] 49.36782 50.61620 49.36862 50.61544 49.36941 50.61468 49.37020 50.61393\n [801] 49.37099 50.61318 49.37178 50.61243 49.37256 50.61169 49.37334 50.61095\n [809] 49.37412 50.61021 49.37490 50.60947 49.37567 50.60874 49.37644 50.60801\n [817] 49.37720 50.60728 49.37797 50.60656 49.37873 50.60583 49.37948 50.60511\n [825] 49.38024 50.60440 49.38099 50.60368 49.38174 50.60297 49.38249 50.60226\n [833] 49.38323 50.60155 49.38397 50.60085 49.38471 50.60014 49.38545 50.59944\n [841] 49.38618 50.59875 49.38691 50.59805 49.38764 50.59736 49.38837 50.59667\n [849] 49.38909 50.59598 49.38981 50.59530 49.39053 50.59461 49.39125 50.59393\n [857] 49.39196 50.59325 49.39267 50.59258 49.39338 50.59190 49.39408 50.59123\n [865] 49.39479 50.59056 49.39549 50.58990 49.39619 50.58923 49.39688 50.58857\n [873] 49.39757 50.58791 49.39827 50.58725 49.39895 50.58660 49.39964 50.58594\n [881] 49.40033 50.58529 49.40101 50.58464 49.40169 50.58399 49.40236 50.58335\n [889] 49.40304 50.58271 49.40371 50.58207 49.40438 50.58143 49.40505 50.58079\n [897] 49.40572 50.58016 49.40638 50.57953 49.40704 50.57890 49.40770 50.57827\n [905] 49.40836 50.57764 49.40901 50.57702 49.40966 50.57640 49.41032 50.57578\n [913] 49.41096 50.57516 49.41161 50.57454 49.41225 50.57393 49.41290 50.57332\n [921] 49.41354 50.57271 49.41417 50.57210 49.41481 50.57149 49.41544 50.57089\n [929] 49.41608 50.57029 49.41670 50.56969 49.41733 50.56909 49.41796 50.56849\n [937] 49.41858 50.56790 49.41920 50.56730 49.41982 50.56671 49.42044 50.56612\n [945] 49.42106 50.56554 49.42167 50.56495 49.42228 50.56437 49.42289 50.56379\n [953] 49.42350 50.56321 49.42411 50.56263 49.42471 50.56205 49.42531 50.56148\n [961] 49.42591 50.56090 49.42651 50.56033 49.42711 50.55976 49.42770 50.55920\n [969] 49.42830 50.55863 49.42889 50.55807 49.42948 50.55750 49.43006 50.55694\n [977] 49.43065 50.55638 49.43123 50.55583 49.43182 50.55527 49.43240 50.55472\n [985] 49.43297 50.55417 49.43355 50.55361 49.43413 50.55307 49.43470 50.55252\n [993] 49.43527 50.55197 49.43584 50.55143 49.43641 50.55089 49.43698 50.55035\n[1001] 49.43754 50.54981 49.43810 50.54927 49.43866 50.54873 49.43922 50.54820\n[1009] 49.43978 50.54766 49.44034 50.54713 49.44089 50.54660 49.44145 50.54607\n[1017] 49.44200 50.54555 49.44255 50.54502 49.44310 50.54450 49.44364 50.54398\n[1025] 49.44419 50.54346 49.44473 50.54294 49.44527 50.54242 49.44581 50.54190\n[1033] 49.44635 50.54139 49.44689 50.54088 49.44742 50.54036 49.44796 50.53985\n[1041] 49.44849 50.53934 49.44902 50.53884 49.44955 50.53833 49.45008 50.53783\n[1049] 49.45060 50.53732 49.45113 50.53682 49.45165 50.53632 49.45217 50.53582\n[1057] 49.45269 50.53532 49.45321 50.53483 49.45373 50.53433 49.45425 50.53384\n[1065] 49.45476 50.53335 49.45527 50.53286 49.45579 50.53237 49.45630 50.53188\n[1073] 49.45680 50.53139 49.45731 50.53091 49.45782 50.53042 49.45832 50.52994\n[1081] 49.45883 50.52946 49.45933 50.52898 49.45983 50.52850 49.46033 50.52802\n[1089] 49.46082 50.52755 49.46132 50.52707 49.46181 50.52660 49.46231 50.52613\n[1097] 49.46280 50.52566 49.46329 50.52519 49.46378 50.52472 49.46427 50.52425\n[1105] 49.46476 50.52379 49.46524 50.52332 49.46572 50.52286 49.46621 50.52240\n[1113] 49.46669 50.52193 49.46717 50.52147 49.46765 50.52102 49.46813 50.52056\n[1121] 49.46860 50.52010 49.46908 50.51965 49.46955 50.51919 49.47002 50.51874\n[1129] 49.47049 50.51829 49.47096 50.51784 49.47143 50.51739 49.47190 50.51694\n[1137] 49.47237 50.51650 49.47283 50.51605 49.47330 50.51561 49.47376 50.51516\n[1145] 49.47422 50.51472 49.47468 50.51428 49.47514 50.51384 49.47560 50.51340\n[1153] 49.47605 50.51296 49.47651 50.51253 49.47696 50.51209 49.47742 50.51166\n[1161] 49.47787 50.51123 49.47832 50.51079 49.47877 50.51036 49.47922 50.50993\n[1169] 49.47967 50.50951 49.48011 50.50908 49.48056 50.50865 49.48100 50.50823\n[1177] 49.48144 50.50780 49.48188 50.50738 49.48232 50.50696 49.48276 50.50653\n[1185] 49.48320 50.50611 49.48364 50.50570 49.48408 50.50528 49.48451 50.50486\n[1193] 49.48494 50.50444 49.48538 50.50403 49.48581 50.50362 49.48624 50.50320\n[1201] 49.48667 50.50279 49.48710 50.50238 49.48752 50.50197 49.48795 50.50156\n[1209] 49.48838 50.50115 49.48880 50.50075 49.48922 50.50034 49.48965 50.49994\n[1217] 49.49007 50.49953 49.49049 50.49913 49.49091 50.49873 49.49132 50.49833\n[1225] 49.49174 50.49793 49.49216 50.49753 49.49257 50.49713 49.49298 50.49673\n[1233] 49.49340 50.49634 49.49381 50.49594 49.49422 50.49555 49.49463 50.49515\n[1241] 49.49504 50.49476 49.49545 50.49437 49.49585 50.49398 49.49626 50.49359\n[1249] 49.49666 50.49320 49.49707 50.49281 49.49747 50.49243 49.49787 50.49204\n[1257] 49.49827 50.49166 49.49867 50.49127 49.49907 50.49089 49.49947 50.49051\n[1265] 49.49987 50.49013 49.50027 50.48975 49.50066 50.48937 49.50106 50.48899\n[1273] 49.50145 50.48861 49.50184 50.48823 49.50223 50.48786 49.50262 50.48748\n[1281] 49.50301 50.48711 49.50340 50.48673 49.50379 50.48636 49.50418 50.48599\n[1289] 49.50456 50.48562 49.50495 50.48525 49.50533 50.48488 49.50572 50.48451\n[1297] 49.50610 50.48414 49.50648 50.48378 49.50686 50.48341 49.50724 50.48305\n[1305] 49.50762 50.48268 49.50800 50.48232 49.50838 50.48196 49.50875 50.48159\n[1313] 49.50913 50.48123 49.50950 50.48087 49.50988 50.48051 49.51025 50.48016\n[1321] 49.51062 50.47980 49.51099 50.47944 49.51136 50.47908 49.51173 50.47873\n[1329] 49.51210 50.47837 49.51247 50.47802 49.51284 50.47767 49.51320 50.47732\n[1337] 49.51357 50.47697 49.51394 50.47661 49.51430 50.47626 49.51466 50.47592\n[1345] 49.51502 50.47557 49.51539 50.47522 49.51575 50.47487 49.51611 50.47453\n[1353] 49.51647 50.47418 49.51682 50.47384 49.51718 50.47349 49.51754 50.47315\n[1361] 49.51789 50.47281 49.51825 50.47247 49.51860 50.47213 49.51896 50.47179\n[1369] 49.51931 50.47145 49.51966 50.47111 49.52001 50.47077 49.52036 50.47043\n[1377] 49.52071 50.47010 49.52106 50.46976 49.52141 50.46943 49.52176 50.46909\n[1385] 49.52210 50.46876 49.52245 50.46843 49.52280 50.46810 49.52314 50.46776\n[1393] 49.52348 50.46743 49.52383 50.46710 49.52417 50.46677 49.52451 50.46645\n[1401] 49.52485 50.46612 49.52519 50.46579 49.52553 50.46546 49.52587 50.46514\n[1409] 49.52621 50.46481 49.52654 50.46449 49.52688 50.46417 49.52722 50.46384\n[1417] 49.52755 50.46352 49.52788 50.46320 49.52822 50.46288 49.52855 50.46256\n[1425] 49.52888 50.46224 49.52922 50.46192 49.52955 50.46160 49.52988 50.46128\n[1433] 49.53021 50.46097 49.53053 50.46065 49.53086 50.46033 49.53119 50.46002\n[1441] 49.53152 50.45971 49.53184 50.45939 49.53217 50.45908 49.53249 50.45877\n[1449] 49.53282 50.45845 49.53314 50.45814 49.53346 50.45783 49.53378 50.45752\n[1457] 49.53410 50.45721 49.53443 50.45690 49.53475 50.45660 49.53506 50.45629\n[1465] 49.53538 50.45598 49.53570 50.45568 49.53602 50.45537 49.53633 50.45507\n[1473] 49.53665 50.45476 49.53697 50.45446 49.53728 50.45415 49.53760 50.45385\n[1481] 49.53791 50.45355 49.53822 50.45325 49.53853 50.45295 49.53885 50.45265\n[1489] 49.53916 50.45235 49.53947 50.45205 49.53978 50.45175 49.54009 50.45145\n[1497] 49.54039 50.45116 49.54070 50.45086\n\ntibble(PP,TT) %&gt;% ggplot(aes(TT,PP)) + geom_point() + xlab(\"Time\") + ylab(\"Population N\")"
  },
  {
    "objectID": "Simple_population_models.html#further-considerations",
    "href": "Simple_population_models.html#further-considerations",
    "title": "Simple population models",
    "section": "Further considerations",
    "text": "Further considerations\n\nWhat biological and ecological factors contribute to r and K?\nWhat population characteristic is the most important for natural resource management?\nWhat species would be difficult to represent with this?"
  },
  {
    "objectID": "SPICT_ex.html",
    "href": "SPICT_ex.html",
    "title": "Using SPICT for surplus production model fitting",
    "section": "",
    "text": "SPICT\nSPiCT is a model of ‘Surplus Production in Continuous Time’, implemented in the ‘spict’ R package, found here. It is relatively easy to implement and comes with a lot of powerful diagnostic tools, but should be used with caution because even if a model is created, it may be unusable due to high levels of uncertainty. In this session, we are mainly going to work from the above web page, and follow instructions there on how to install the package, and follow the vignette to learn its functionality. The website also has a link to technical guidelines on how to judge whether the model created is trustworthy and informative. Here some points to keep in mind from the technical guidelines:\n\nMain assumptions and input data\n\nCatch should be representative of total removals from the population and aligned correctly in timing\nStock size indices should be in terms of biomass (not numbers) and represent exploitable stock biomass. Timing should be correctly aligned.\n\nChecklist for the acceptance of a SPiCT assessment\n\nassessment converged\nAll variance parameters are finite\nNo violation of model assumptions based on one-step-ahead residuals (e.g., no bias or auto-correlation should be present to violate normality assumptions).\nThere should be no tendency of consistent uner- or overestimation of fishing mortality or biomass in successive assessments\nRealistic production curve (not too skewed)\nAssessment uncertainty is not too high (not span more than 1 order of magnitude\ninitial values do not influence parameter estimates\n\nWith this in mind, let’s install and try a vignette."
  },
  {
    "objectID": "Fitting_SPM.html",
    "href": "Fitting_SPM.html",
    "title": "Surplus production models",
    "section": "",
    "text": "In this exercise, we take the concepts we learned yesterday regarding equilibrium properties of the Schaeffer surplus production model, and we attempt to apply them using whelk data.\nWhelk data you can retrieve by downloading and importing them from the institute advice webpage.\n\nWHE_landings &lt;- read_csv('WHE_landings.csv')\n\nRows: 30 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): species_name, latin_name, country, area, gear\ndbl (2): year, landings\nlgl (1): landings_type\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nWHE_assessment &lt;- read_csv('WHE_assessment.csv')\n\nRows: 19 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): species_name, latin_name\ndbl (4): year, high_cpue, low_cpue, cpue\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwhelk.data &lt;- \n  WHE_landings %&gt;% \n  group_by(year) %&gt;% \n  summarise(landings = sum(landings)) %&gt;% \n  left_join(WHE_assessment) %&gt;% \n  rename(YY = year, II = cpue, CC = landings) %&gt;% \n  filter(YY &gt; 2003) %&gt;% \n  bind_rows(tibble(YY=2020, II=NA))\n\nJoining with `by = join_by(year)`\n\nProductionFunction&lt;-function(rr, KK, qq, PP0=KK, CC){\n  # II = CPUE data; rr, KK, qq, and PP0 are parameters\n  PP&lt;-vector(\"numeric\", length = length(CC)); DD&lt;-PP; Pred&lt;-PP # this just defines the storage vectors beforehand\n  PP[1]&lt;-PP0 #assigns the first year's abundance \n  for(ii in 2:length(CC)){ #then for every other year,\n    DD[ii-1]&lt;- rr*PP[ii-1]*(1-PP[ii-1]/KK) # calculate the growth of the population based on rr and KK\n    PP[ii]&lt;-PP[ii-1] + DD[ii-1] - CC[ii-1] # and add that growth to that year's population, remove the catch, and update population levels for next year\n  }\n  Pred &lt;- qq*PP #calculate the Predictions for the index of abundance by translating the model population by catchability\n  return(Pred) # this function returns predictions\n}\n\n#rr is intrinsic growth rate\n#KK is carrying capacity\n#qq is catchability\n#PP0 is initial population size\n#note that KK and PP0 reflect the absolute abundance level of the population, which can be \n#difficult to detect if the data don't have much contrast \n#(i.e., they don't go up AND down a lot over time)\n\n#Note the scale of q, KK, and PP0 should be roughly right for nls to work. If not, try multiplying\n#or dividing by 10 or 100 or 1000\n#c(rr = 0.25, KK = 9000, qq = 0.01)\nWhelk.nls &lt;- nls(II ~ ProductionFunction(rr=rr, KK=KK, qq=qq, CC=CC), \n               data = list(CC = whelk.data$CC, II = whelk.data$II), \n               start = c(rr = 0.15, KK = 10000, qq = 0.00008), \n               control = list(maxiter = 10000, minFactor=0.000001))\n\nsummary(Whelk.nls)\n\n\nFormula: II ~ ProductionFunction(rr = rr, KK = KK, qq = qq, CC = CC)\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)  \nrr 1.091e-01  8.831e-02   1.236   0.2356  \nKK 9.827e+03  5.235e+03   1.877   0.0801 .\nqq 7.328e-05  4.703e-05   1.558   0.1400  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1214 on 15 degrees of freedom\n\nNumber of iterations to convergence: 9 \nAchieved convergence tolerance: 5.153e-06\n  (1 observation deleted due to missingness)\n\nbest_pars &lt;- coef(Whelk.nls) #extract best-fit parameters\nbest_pred &lt;- ProductionFunction(rr=best_pars[1], \n                                KK=best_pars[2], \n                                qq=best_pars[3], \n                                CC=whelk.data$CC) # create predictions\n\n\ntibble(YY=whelk.data$YY, II = whelk.data$II, best_pred) %&gt;% ggplot(aes(YY, best_pred)) + geom_line() + geom_point(aes(YY, II)) + xlab(\"Time\") + ylab(\"Observed (points) and Predicted (line) CPUE\")\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nNow try changing the starting values for parameters (especially KK and PP0) to see whether they affect results. If they do, that means the data don’t carry much information, we need a better estimation method, or we need to rule out some local minima external to the modeling.\n\nWhelk.nls &lt;- nls(II ~ ProductionFunction(rr=rr, KK=KK, qq=qq, CC=CC), \n               data = list(CC = whelk.data$CC, II = whelk.data$II), \n               start = c(rr = 1, KK = 5000, qq = 0.08), \n               control = list(maxiter = 10000, minFactor=0.000001))\n\nsummary(Whelk.nls)\n\n\nFormula: II ~ ProductionFunction(rr = rr, KK = KK, qq = qq, CC = CC)\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)  \nrr  1.169e+00  4.838e-01   2.416   0.0289 *\nKK -2.258e+03  1.593e+03  -1.417   0.1769  \nqq -2.113e-04  1.312e-04  -1.610   0.1282  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.128 on 15 degrees of freedom\n\nNumber of iterations to convergence: 13 \nAchieved convergence tolerance: 5.261e-06\n  (1 observation deleted due to missingness)\n\nbest_pars2 &lt;- coef(Whelk.nls) #extract best-fit parameters\nbest_pred2 &lt;- ProductionFunction(rr=best_pars2[1], \n                                KK=best_pars2[2], \n                                qq=best_pars2[3], \n                                CC=whelk.data$CC) # create predictions\n\ntibble(YY=whelk.data$YY, II = whelk.data$II, best_pred, best_pred2) %&gt;% ggplot(aes(YY, best_pred)) + geom_line() + geom_point(aes(YY, II)) + xlab(\"Time\") + ylab(\"Observed (points) and Predicted (lines) CPUE\") + geom_line(aes(YY, best_pred2), color='red', linetype = 2)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nWhich do you think is closer to the real biological scenario?\n\nbest_pars\n\n          rr           KK           qq \n1.091162e-01 9.827211e+03 7.327566e-05 \n\n\n\nbest_pars2\n\n           rr            KK            qq \n 1.169129e+00 -2.258058e+03 -2.112510e-04 \n\n\nNow calculate MSY and Bmsy according to your model.\n\n#MSY = r*K / 4\n#Bmsy = K / 2\n\nMSY &lt;- best_pars[1]*best_pars[2]/4\nBmsy &lt;- best_pars[2]/2\n\nMSY; Bmsy\n\n     rr \n268.077 \n\n\n      KK \n4913.606 \n\n\nNow, we would like to loosen the assumption that the population is starting out at carrying capacity. This adds a parameter, so makes the estimation more difficult.\n\nProductionFunction2&lt;-function(rr, KK, qq, a = 1, CC = CC){\n  # II = CPUE data; rr, KK, qq, and PP0 are parameters\n  PP0 &lt;- a*KK\n  PP&lt;-vector(\"numeric\", length = length(CC)); DD&lt;-PP; Pred&lt;-PP # this just defines the storage vectors beforehand\n  PP[1]&lt;-PP0 #assigns the first year's abundance \n  for(ii in 2:length(CC)){ #then for every other year,\n    DD[ii-1]&lt;- rr*PP[ii-1]*(1-PP[ii-1]/KK) # calculate the growth of the population based on rr and KK\n    PP[ii]&lt;-PP[ii-1] + DD[ii-1] - CC[ii-1] # and add that growth to that year's population, remove the catch, and update population levels for next year\n  }\n  Pred &lt;- qq*PP #calculate the Predictions for the index of abundance by translating the model population by catchability\n  return(Pred) # this function returns predictions\n}\nProductionFunction2(rr = 0.27, KK = 7000, qq = 0.006, CC = whelk.data$CC)\n\n [1] 42.00000 36.82200 32.10170 29.11039 28.19853 28.31241 30.10767 31.55742\n [9] 30.60390 30.59596 32.30500 33.76041 34.44466 34.14364 34.90807 35.32956\n[17] 34.73854 35.32816 35.09740\n\n\nBelow, the model fitting process is written twice so that you can play around with the parameter starting values and lower and upper bounds of the second version while remembering which parameters you tried last in the first version. Can you find a solution that converges? Use your best solution to check how MSY and \\(B_{MSY}\\) values have changed from the other version.\n\nWhelk.nls3.1&lt;-nls(II ~ ProductionFunction2(rr=rr, KK=KK, qq=qq, a = a, CC = CC), \n               algorithm = 'port',\n               data = list(CC = whelk.data$CC, II = whelk.data$II), \n               start = c(rr = 0.15, KK = 10000, qq = 0.00008, a = 0.8),\n               lower = c(rr = 0.01, KK = 1000, qq = 0.000001, a = 0.1),\n               upper = c(rr = 1, KK = 30000, qq = 0.1, a = 1),\n               control = list(maxiter = 100000, minFactor=0.000001), \n               trace = TRUE)\n\n  0:    0.13018275: 0.150000  10000.0 8.00000e-05 0.800000\n  1:    0.11789722: 0.131131  9953.59 8.08475e-05 0.823517\n  2:    0.11095491: 0.112741  9950.17 8.26395e-05 0.846349\n  3:    0.10931601: 0.0969817  10088.5 8.61790e-05 0.827417\n  4:    0.10899659: 0.0968142  10244.3 8.84597e-05 0.792043\n  5:    0.10882289: 0.0960943  10374.5 9.01341e-05 0.766726\n  6:    0.10868635: 0.0951602  10542.4 9.15364e-05 0.741283\n  7:    0.10857393: 0.0953840  10546.1 9.25985e-05 0.734648\n  8:    0.10832050: 0.0917580  10870.3 9.68881e-05 0.685841\n  9:    0.10792061: 0.0912533  11249.4 0.000100948 0.637642\n 10:    0.10764554: 0.0908543  11570.1 0.000101820 0.613598\n 11:    0.10749593: 0.0905338  11692.2 0.000103705 0.599633\n 12:    0.10734488: 0.0907260  11881.3 0.000106264 0.577229\n 13:    0.10727095: 0.0910519  11958.9 0.000107303 0.568927\n 14:    0.10721481: 0.0912283  12034.5 0.000108265 0.560843\n 15:    0.10707645: 0.0908617  12413.1 0.000111036 0.530377\n 16:    0.10688654: 0.0905583  12860.6 0.000113277 0.502368\n 17:    0.10675009: 0.0907081  13060.8 0.000114402 0.491013\n 18:    0.10667455: 0.0905862  13270.1 0.000115506 0.479212\n 19:    0.10661472: 0.0905528  13446.0 0.000116237 0.470122\n 20:    0.10652238: 0.0900468  13825.8 0.000117256 0.452883\n 21:    0.10633946: 0.0874745  14934.5 0.000115889 0.420587\n 22:    0.10623247: 0.0878625  15340.8 0.000119328 0.399999\n 23:    0.10610173: 0.0875922  15743.7 0.000120173 0.387723\n 24:    0.10603215: 0.0874024  16077.7 0.000120913 0.377690\n 25:    0.10597843: 0.0872476  16362.1 0.000121489 0.369596\n 26:    0.10593152: 0.0870656  16641.3 0.000121974 0.362084\n 27:    0.10588604: 0.0863275  17295.1 0.000122591 0.346198\n 28:    0.10578375: 0.0862696  17692.0 0.000123302 0.336998\n 29:    0.10573377: 0.0861530  18035.3 0.000123877 0.329262\n 30:    0.10569404: 0.0860411  18328.6 0.000124304 0.323020\n 31:    0.10566019: 0.0859194  18601.4 0.000124657 0.317471\n 32:    0.10561563: 0.0855296  19106.0 0.000125050 0.307975\n 33:    0.10556153: 0.0854327  19503.0 0.000125748 0.300483\n 34:    0.10554211: 0.0848199  20309.9 0.000126012 0.287301\n 35:    0.10544676: 0.0847914  20738.0 0.000126520 0.280698\n 36:    0.10542576: 0.0844136  21571.4 0.000127537 0.267911\n 37:    0.10534982: 0.0843753  21832.9 0.000127412 0.265018\n 38:    0.10533886: 0.0840219  22702.1 0.000128315 0.253111\n 39:    0.10525927: 0.0837474  23534.0 0.000128780 0.243410\n 40:    0.10521957: 0.0837716  23656.4 0.000128650 0.242455\n 41:    0.10521153: 0.0838043  23773.3 0.000128834 0.240972\n 42:    0.10519404: 0.0836752  24068.5 0.000128952 0.237774\n 43:    0.10517141: 0.0834977  24465.2 0.000129108 0.233635\n 44:    0.10514687: 0.0833343  24901.1 0.000129292 0.229231\n 45:    0.10512439: 0.0832286  25288.0 0.000129486 0.225427\n 46:    0.10510492: 0.0831494  25640.9 0.000129671 0.222036\n 47:    0.10508768: 0.0830831  25965.6 0.000129840 0.218999\n 48:    0.10506977: 0.0829957  26327.6 0.000130000 0.215722\n 49:    0.10505861: 0.0830389  26511.1 0.000130352 0.213884\n 50:    0.10504111: 0.0830056  26888.6 0.000130600 0.210484\n 51:    0.10502056: 0.0826460  27659.1 0.000130473 0.204467\n 52:    0.10498773: 0.0825484  28179.3 0.000130657 0.200496\n 53:    0.10496772: 0.0824862  28623.4 0.000130865 0.197120\n 54:    0.10495140: 0.0824268  29008.1 0.000131024 0.194301\n 55:    0.10494605: 0.0824738  30000.0 0.000131917 0.186680\n 56:    0.10491037: 0.0827561  30000.0 0.000132272 0.186506\n 57:    0.10491027: 0.0828966  30000.0 0.000132470 0.186269\n 58:    0.10491026: 0.0829393  30000.0 0.000132527 0.186200\n 59:    0.10491026: 0.0829528  30000.0 0.000132545 0.186178\n 60:    0.10491026: 0.0829570  30000.0 0.000132551 0.186171\n 61:    0.10491026: 0.0829584  30000.0 0.000132552 0.186169\n\nsummary(Whelk.nls3.1)\n\n\nFormula: II ~ ProductionFunction2(rr = rr, KK = KK, qq = qq, a = a, CC = CC)\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)\nrr 8.296e-02  3.329e-01   0.249    0.807\nKK 3.000e+04  9.626e+05   0.031    0.976\nqq 1.326e-04  1.216e-04   1.090    0.294\na  1.862e-01  6.135e+00   0.030    0.976\n\nResidual standard error: 0.1224 on 14 degrees of freedom\n\nAlgorithm \"port\", convergence message: relative convergence (4)\n  (1 observation deleted due to missingness)\n\nWhelk.nls3.2&lt;-nls(II ~ ProductionFunction2(rr=rr, KK=KK, qq=qq, a = a, CC = CC), \n               algorithm = 'port',\n               data = list(CC = whelk.data$CC, II = whelk.data$II), \n               start = c(rr = 1, KK = 5000, qq = 0.08, a = 0.8),\n               lower = c(rr = 0.01, KK = 1000, qq = 0.000001, a = 0.1),\n               upper = c(rr = 1, KK = 30000, qq = 0.1, a = 1),\n               control = list(maxiter = 100000, minFactor=0.000001), \n               trace = TRUE)\n\n  0:     1179600.1:  1.00000  5000.00 0.0800000 0.800000\n  1:     150833.12: 0.694538  3389.37 0.0506772 0.770942\n  2:     744.77650: 0.751045  3124.63 0.00406267 0.790542\n  3:     2.6515836: 0.709914  3271.14 1.00000e-06 0.811514\n  4:    0.36759644: 0.819729  3669.24 0.000204489 0.852355\n  5:    0.22958799: 0.920483  4010.72 0.000136196 0.931015\n  6:    0.20127242:  1.00000  4414.89 0.000137295 0.988022\n  7:    0.19489382:  1.00000  4964.49 0.000108243  1.00000\n  8:    0.17959837:  1.00000  5518.37 0.000101845  1.00000\n  9:    0.17475801:  1.00000  6125.00 9.05372e-05  1.00000\n 10:    0.17283314:  1.00000  7395.23 7.16580e-05  1.00000\n 11:    0.16483495:  1.00000  8039.81 6.84380e-05  1.00000\n 12:    0.16300073:  1.00000  9310.04 5.75298e-05  1.00000\n 13:    0.15933608:  1.00000  10580.3 5.09345e-05  1.00000\n 14:    0.15727725:  1.00000  11850.5 4.54360e-05  1.00000\n 15:    0.15568814:  1.00000  13120.7 4.10296e-05  1.00000\n 16:    0.15446627:  1.00000  14391.0 3.73903e-05  1.00000\n 17:    0.15349120:  1.00000  15661.2 3.43395e-05  1.00000\n 18:    0.15269463:  1.00000  16931.4 3.17458e-05  1.00000\n 19:    0.15203103:  1.00000  18201.6 2.95142e-05  1.00000\n 20:    0.15146930:  1.00000  19471.9 2.75743e-05  1.00000\n 21:    0.15143138:  1.00000  22166.7 2.38160e-05  1.00000\n 22:    0.15013237:  1.00000  23514.0 2.28131e-05  1.00000\n 23:    0.14981298:  1.00000  24861.4 2.15452e-05  1.00000\n 24:    0.14963470:  1.00000  25546.1 2.10113e-05  1.00000\n 25:    0.14949845:  1.00000  26191.4 2.04882e-05  1.00000\n 26:    0.14933902:  1.00000  27003.7 1.98573e-05  1.00000\n 27:    0.14922540:  1.00000  30000.0 1.76459e-05  1.00000\n 28:    0.14882335:  1.00000  30000.0 1.78702e-05  1.00000\n 29:    0.14882334:  1.00000  30000.0 1.78714e-05  1.00000\n\nsummary(Whelk.nls3.2)\n\n\nFormula: II ~ ProductionFunction2(rr = rr, KK = KK, qq = qq, a = a, CC = CC)\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)   \nrr 1.000e+00  7.861e+00   0.127  0.90058   \nKK 3.000e+04  2.406e+05   0.125  0.90254   \nqq 1.787e-05  1.442e-04   0.124  0.90314   \na  1.000e+00  2.961e-01   3.377  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1458 on 14 degrees of freedom\n\nAlgorithm \"port\", convergence message: X-convergence (3)\n  (1 observation deleted due to missingness)\n\n# insert either Whelk.nls3.1 or Whelk.nls3.2 into the coef() function below, \n# whichever one you found as the best model you could come up with.\nbest_pars3 &lt;- coef(Whelk.nls3.1) #extract best-fit parameters\n\nbest_pred3 &lt;- ProductionFunction2(rr=best_pars3[1], \n                                KK=best_pars3[2], \n                                qq=best_pars3[3], \n                                a=best_pars3[4], \n                                CC=whelk.data$CC) # create predictions\n\n\ntibble(YY=whelk.data$YY, II = whelk.data$II, best_pred3) %&gt;% ggplot(aes(YY, best_pred)) + geom_line() + geom_point(aes(YY, II)) + xlab(\"Time\") + ylab(\"Observed (points) and Predicted (lines) CPUE\") + geom_line(aes(YY, best_pred2), color='red', linetype = 2)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nMSY &lt;- best_pars3[1]*best_pars3[2]/4\nBmsy &lt;- best_pars3[2]/2\n\nMSY; Bmsy\n\n      rr \n622.1877 \n\n\n   KK \n15000"
  }
]