[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PopModels",
    "section": "",
    "text": "For this part of the GRO-FTP ARAM line, you are expected to already have some fluency in using Rstudio and tidyverse. At this point you should also start thinking of how you plan to organize your files and work into a project, or even start using Rmarkdown files to take notes and do exploratory studies and plots."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "PopModels",
    "section": "",
    "text": "For this part of the GRO-FTP ARAM line, you are expected to already have some fluency in using Rstudio and tidyverse. At this point you should also start thinking of how you plan to organize your files and work into a project, or even start using Rmarkdown files to take notes and do exploratory studies and plots."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "PopModels",
    "section": "Schedule",
    "text": "Schedule\nThe population dynamics section of teaching is split into roughly 8 half-day sessions:\nSimple population models\nReview-what are models?\nFitting a logistic growth curve\n[Surplus production models]\n[SPICT exercise]\n[Components of an age-based model 1](\n[Components of an age-based model 2]\n[Age-based models 1]\n[Age-based models 2]"
  },
  {
    "objectID": "Fitting_logistic_curve.html",
    "href": "Fitting_logistic_curve.html",
    "title": "Gr√≥-FTP - Stock assessment line - Simple pop. models",
    "section": "",
    "text": "Remember this from the Simple Population Models section?\n\\(N_t = N_{t-1} + r*N_{t-1}*(1-N_{t-1}/K)\\)\n\nWhat is the functional form?\nWhat are the parameters?\nWhat does each parameter control?\n\n\n\nNote - here we use sum of squares as our objective function, which is not the best way to fit a non-linear model, but is used for demonstration.\nTake a look at the website below to get familiar with the system. Then continue with the exercise.\nhttps://ecosystem-project.com/epopulationgrowth\n\n\n\n\nYrs&lt;-c(1903, 1905, 1907, 1926, 1928, 1932, 1935, 1939, 1946, 1948, 1961, 1964, 1967, 1970, 1972, 1974, 1976, 1979, 1982, 1985, 1988, 1990, 1993, 1996)\nData&lt;-c(12, 15, 17, 25, 27, 30, 37, 42, 52, 108, 156, 219, 7563, 8642, 7378, 7245, 6983, 7367, 7689, 8503, 7654, 6803, 7204, 7343)\nlength(Yrs); length(Data)\n\n[1] 24\n\n\n[1] 24\n\n#pars, Years, and Numbers are vector arguments\nLogisticGrowthModel&lt;-function(pars, Years, Numbers, ReturnResults=F){\n  \n  print(Years)\n  \n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]#assign parameters\n  \n  TT&lt;-Years[1]:Years[length(Years)] #stretch out years within model so no missing years\n\n  print(TT)\n  \n  PP&lt;-vector(mode = \"numeric\", length = length(TT));  #empty population size vector\n  \n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); #empty population growth vector\n  \n  II&lt;-rr*BB*(1-BB/KK) #initial growth to reach year 1 from year 0\n  \n  PP[1]&lt;-BB+II # population assignment at year 1, based on parameter BB (initial population)\n  \n  for(i in 2:length(PP)){ \n    \n    DD[i-1] &lt;- rr*PP[i-1]*(1-PP[i-1]/KK); #this is dN/dt - amount the population grows based\n    #on conditions in the previous time step\n    \n    PP[i] &lt;- PP[i-1] + DD[i-1] #update population size\n    \n    } \n\n  print(DD)\n  print(PP)\n  \n   Predictions&lt;-data.frame(Years = TT, Pred = PP); #create predictions\n   \n   Observations&lt;-data.frame(Years, Numbers) #store observations in data frame\n   \n   Results&lt;-merge(Predictions, Observations, by = \"Years\") #match Predictions and Observations by Year\n  \n   SS&lt;-sum((Results$Numbers - Results$Pred)^2) #calculate sum of squares: sum((obs - pred)^2)\n  \n   print('Results')\n   print(Results)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(SS)}\n   #controls options for what is returned from function\n\n   }\n\n#LogisticGrowthModel(c(7500,0.5), Yrs, Data, t=0, ReturnResults=T) ERROR\nLogisticGrowthModel(c(7500,0.5,1), Years=Yrs, Numbers=Data, ReturnResults=T)\n\n [1] 1903 1905 1907 1926 1928 1932 1935 1939 1946 1948 1961 1964 1967 1970 1972\n[16] 1974 1976 1979 1982 1985 1988 1990 1993 1996\n [1] 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917\n[16] 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932\n[31] 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947\n[46] 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962\n[61] 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977\n[76] 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992\n[91] 1993 1994 1995 1996\n [1] 7.498167e-01 1.124538e+00 1.686385e+00 2.528629e+00 3.790811e+00\n [6] 5.681422e+00 8.511361e+00 1.274285e+01 1.906000e+01 2.846841e+01\n[11] 4.243079e+01 6.304051e+01 9.321762e+01 1.368693e+02 1.988625e+02\n[16] 2.844839e+02 3.978030e+02 5.381664e+02 6.944762e+02 8.391165e+02\n[21] 9.280905e+02 9.171569e+02 7.935266e+02 5.960632e+02 3.925182e+02\n[26] 2.326112e+02 1.281541e+02 6.752262e+01 3.469589e+01 1.759173e+01\n[31] 8.858151e+00 4.444818e+00 2.226367e+00 1.114176e+00 5.573361e-01\n[36] 2.787302e-01 1.393806e-01 6.969421e-02 3.484807e-02 1.742428e-02\n[41] 8.712201e-03 4.356116e-03 2.178062e-03 1.089032e-03 5.445161e-04\n[46] 2.722581e-04 1.361291e-04 6.806454e-05 3.403227e-05 1.701614e-05\n[51] 8.508068e-06 4.254034e-06 2.127016e-06 1.063508e-06 5.317542e-07\n[56] 2.658773e-07 1.329387e-07 6.646891e-08 3.323467e-08 1.661754e-08\n[61] 8.308770e-09 4.154177e-09 2.076672e-09 1.038752e-09 5.191680e-10\n[66] 2.597922e-10 1.294798e-10 6.494805e-11 3.289036e-11 1.623701e-11\n[71] 8.326673e-12 4.163336e-12 1.665335e-12 8.326673e-13 4.163336e-13\n[76] 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13\n[81] 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13\n[86] 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13\n[91] 4.163336e-13 4.163336e-13 4.163336e-13 0.000000e+00\n [1]    1.499933    2.249750    3.374288    5.060672    7.589301   11.380112\n [7]   17.061534   25.572895   38.315744   57.375743   85.844149  128.274942\n[13]  191.315449  284.533067  421.402329  620.264832  904.748684 1302.551681\n[19] 1840.718129 2535.194312 3374.310788 4302.401296 5219.558149 6013.084739\n[25] 6609.147903 7001.666121 7234.277284 7362.431404 7429.954028 7464.649918\n[31] 7482.241650 7491.099801 7495.544620 7497.770987 7498.885162 7499.442498\n[37] 7499.721228 7499.860609 7499.930303 7499.965151 7499.982576 7499.991288\n[43] 7499.995644 7499.997822 7499.998911 7499.999455 7499.999728 7499.999864\n[49] 7499.999932 7499.999966 7499.999983 7499.999991 7499.999996 7499.999998\n[55] 7499.999999 7499.999999 7500.000000 7500.000000 7500.000000 7500.000000\n[61] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[67] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[73] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[79] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[85] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[91] 7500.000000 7500.000000 7500.000000 7500.000000\n[1] \"Results\"\n   Years        Pred Numbers\n1   1903    1.499933      12\n2   1905    3.374288      15\n3   1907    7.589301      17\n4   1926 6013.084739      25\n5   1928 7001.666121      27\n6   1932 7464.649918      30\n7   1935 7495.544620      37\n8   1939 7499.721228      42\n9   1946 7499.997822      52\n10  1948 7499.999455     108\n11  1961 7500.000000     156\n12  1964 7500.000000     219\n13  1967 7500.000000    7563\n14  1970 7500.000000    8642\n15  1972 7500.000000    7378\n16  1974 7500.000000    7245\n17  1976 7500.000000    6983\n18  1979 7500.000000    7367\n19  1982 7500.000000    7689\n20  1985 7500.000000    8503\n21  1988 7500.000000    7654\n22  1990 7500.000000    6803\n23  1993 7500.000000    7204\n24  1996 7500.000000    7343\n\n\n[[1]]\n   Years        Pred\n1   1903    1.499933\n2   1904    2.249750\n3   1905    3.374288\n4   1906    5.060672\n5   1907    7.589301\n6   1908   11.380112\n7   1909   17.061534\n8   1910   25.572895\n9   1911   38.315744\n10  1912   57.375743\n11  1913   85.844149\n12  1914  128.274942\n13  1915  191.315449\n14  1916  284.533067\n15  1917  421.402329\n16  1918  620.264832\n17  1919  904.748684\n18  1920 1302.551681\n19  1921 1840.718129\n20  1922 2535.194312\n21  1923 3374.310788\n22  1924 4302.401296\n23  1925 5219.558149\n24  1926 6013.084739\n25  1927 6609.147903\n26  1928 7001.666121\n27  1929 7234.277284\n28  1930 7362.431404\n29  1931 7429.954028\n30  1932 7464.649918\n31  1933 7482.241650\n32  1934 7491.099801\n33  1935 7495.544620\n34  1936 7497.770987\n35  1937 7498.885162\n36  1938 7499.442498\n37  1939 7499.721228\n38  1940 7499.860609\n39  1941 7499.930303\n40  1942 7499.965151\n41  1943 7499.982576\n42  1944 7499.991288\n43  1945 7499.995644\n44  1946 7499.997822\n45  1947 7499.998911\n46  1948 7499.999455\n47  1949 7499.999728\n48  1950 7499.999864\n49  1951 7499.999932\n50  1952 7499.999966\n51  1953 7499.999983\n52  1954 7499.999991\n53  1955 7499.999996\n54  1956 7499.999998\n55  1957 7499.999999\n56  1958 7499.999999\n57  1959 7500.000000\n58  1960 7500.000000\n59  1961 7500.000000\n60  1962 7500.000000\n61  1963 7500.000000\n62  1964 7500.000000\n63  1965 7500.000000\n64  1966 7500.000000\n65  1967 7500.000000\n66  1968 7500.000000\n67  1969 7500.000000\n68  1970 7500.000000\n69  1971 7500.000000\n70  1972 7500.000000\n71  1973 7500.000000\n72  1974 7500.000000\n73  1975 7500.000000\n74  1976 7500.000000\n75  1977 7500.000000\n76  1978 7500.000000\n77  1979 7500.000000\n78  1980 7500.000000\n79  1981 7500.000000\n80  1982 7500.000000\n81  1983 7500.000000\n82  1984 7500.000000\n83  1985 7500.000000\n84  1986 7500.000000\n85  1987 7500.000000\n86  1988 7500.000000\n87  1989 7500.000000\n88  1990 7500.000000\n89  1991 7500.000000\n90  1992 7500.000000\n91  1993 7500.000000\n92  1994 7500.000000\n93  1995 7500.000000\n94  1996 7500.000000\n\n[[2]]\n[1] 471423138\n\n\nNow we would like to fit this model by minimizing the objective function using algorithms implemented in the ‚Äòoptim‚Äô function.\n\n?optim # this function tries different parameter values to minimize the output of \n#LogisticGrowthModel. The output is the sum of squares.\n\nOptPars1&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000))\nOptPars1\n\nOptPars2&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, Years = Yrs, Numbers = Data, control = list(maxit = 50000))\nOptPars2\n\nPredictions1&lt;-LogisticGrowthModel(OptPars1$par, Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModel(OptPars2$par, Yrs, Data, ReturnResults=T)[[1]]\n\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), ylab = '', xlab = '')\npar(new = T)\nplot(Pred~Years, data = Predictions1, type = \"l\", ylim = c(0,8000))\nlines(Pred~Years, data = Predictions2, lty = 2)\n\n# Now try to figure out the best time lag as shown in the on-line example. \n# Hint: plot SS as it relates to the time lag value. SS is your sums of \n# squares, so the value with the lowest sum of squares is the one you want. \n# Try lag values of 0 - 5. To include a lag, use the next function instead. \n# Note that there is now an additional parameter for lag t.\n\nLogisticGrowthModelLag&lt;-function(pars, Years, Numbers, t=0, ReturnResults=F){\n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]\n  TT&lt;-Years[1]:Years[length(Years)]\n  PP&lt;-vector(mode = \"numeric\", length = length(TT));\n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); \n  II&lt;-rr*BB*(1-BB/KK)\n  PP[1]&lt;-BB+II\n  \n  #implementing Logistic Growth for Years with no lagged Numbers data\n  if(t&gt;0){\n    for(i in 2:(t+1)){ \n      DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1]/KK);\n      PP[i]&lt;-PP[i-1] + DD[i-1]\n    }\n  }\n  \n  #implementing Lagged growth\n  for(i in (t+2):length(PP)){ \n    DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1-t]/KK); \n    PP[i]&lt;-PP[i-1] + DD[i-1]\n  }\n  \n  #print(PP) \n  Predictions&lt;-data.frame(Years = TT, Pred = PP); Observations&lt;-data.frame(Years, Numbers)\n  Results&lt;-merge(Predictions, Observations, by = \"Years\")\n  SS&lt;-sum((Results$Numbers - Results$Pred)^2)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(ifelse(is.infinite(SS), 10000000000, SS))}\n}\n\nLogisticGrowthModelLag(OptPars1$par, t=3,Yrs, Data, ReturnResults=T)\nLogisticGrowthModelLag(OptPars2$par, t=3,Yrs, Data, ReturnResults=T)\n\nSS1&lt;-SS2&lt;-vector(mode = \"numeric\", length = 6)\nOptPars1_list&lt;-OptPars2_list&lt;-NULL\n\nfor(i in 0:6){\n  OptPars1_list[[i+1]]&lt;-try(optim(OptPars1$par, fn=LogisticGrowthModelLag, t=i, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000)), silent=T)\n  OptPars2_list[[i+1]]&lt;-try(optim(OptPars2$par, fn=LogisticGrowthModelLag, t=i, Years = Yrs, Numbers = Data, control = list(maxit = 50000)),silent=T)\n  SS1[i+1]&lt;-try(OptPars1_list[[i+1]]$value/1000000,silent=T )\n  SS2[i+1]&lt;-try(OptPars2_list[[i+1]]$value/1000000,silent=T )\n}\n\nSS1 \nSS2\n\n\n#Q - The time lag corresponding with the minimum SS is the best. What is time lag is that?\n#That is, which would you choose if the best model by least squares (SS)?\n\npar(mfrow = c(1,2))\nplot(SS1~c(0:6))\nplot(SS2~c(0:6))\n\n#Q - BUT, this is what happens with lag = 5. Why? (Compare plots with lag = 0 - 4)\npar(mfrow = c(1,3))\n\nOptPars&lt;-NULL\n\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=5, Years = Yrs, Numbers = Data)\n\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=5,Yrs, Data, ReturnResults=T)[[1]]\n\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), xlab = '', ylab = '')\n\npar(new = T)\n\nplot(Pred~Years, data = Predictions, type = \"l\", ylim = c(0,8000))\n\nmtext('Lag 5')\n\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars1_list[[6]]$par), t=6-1,Yrs, Data, ReturnResults=T)[[1]]\n\nlines(Pred~Years, data = Predictions, lty = 2)\n\nLogisticGrowthModelLag(OptPars2$par, t=5,Yrs, Data, ReturnResults=T)\n\n\n#...compared with lag = 3\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=3, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=3,Yrs, Data, ReturnResults=T)[[1]]\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), xlab = '', ylab = '')\npar(new = T)\nplot(Pred~Years, data = Predictions, type = \"l\", ylim = c(0,8000))\nmtext('Lag 3')\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars1_list[[4]]$par), t=4-1,Yrs, Data, ReturnResults=T)[[1]]\nlines(Pred~Years, data = Predictions, lty = 2)\n\n\n#...and no lag = 0\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=0, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=0,Yrs, Data, ReturnResults=T)[[1]]\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), xlab = '', ylab = '')\npar(new = T)\nplot(Pred~Years, data = Predictions, type = \"l\", ylim = c(0,8000))\nmtext('Lag 0')\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars1_list[[1]]$par), t=1-1,Yrs, Data, ReturnResults=T)[[1]]\nlines(Pred~Years, data = Predictions, lty = 2)\n\n\n#Q - Which one fits the data best?\n\n\n#TAKE HOME MESSAGES\n#- the optimizer matters\n#- the starting values matter\n#- the biology matters\n\n\n\n\n\n\nWhat biological and ecological factors contribute to r and K?\n\n\n\nWhat population characteristic is the most important for natural resource management?\nHow to detect this without biological/ecological data?\nDraw a graph of how population growth rate (\\(dN/dt\\)) changes over time for the case studies of 1) exponential and 2) logistic growth. How does the density dependent term \\((1-N/K)\\) change over time?\nWhat species would be difficult to represent with this?"
  },
  {
    "objectID": "Fitting_logistic_curve.html#logistic-growth---review",
    "href": "Fitting_logistic_curve.html#logistic-growth---review",
    "title": "Gr√≥-FTP - Stock assessment line - Simple pop. models",
    "section": "",
    "text": "Remember this from the Simple Population Models section?\n\\(N_t = N_{t-1} + r*N_{t-1}*(1-N_{t-1}/K)\\)\n\nWhat is the functional form?\nWhat are the parameters?\nWhat does each parameter control?\n\n\n\nNote - here we use sum of squares as our objective function, which is not the best way to fit a non-linear model, but is used for demonstration.\nTake a look at the website below to get familiar with the system. Then continue with the exercise.\nhttps://ecosystem-project.com/epopulationgrowth\n\n\n\n\nYrs&lt;-c(1903, 1905, 1907, 1926, 1928, 1932, 1935, 1939, 1946, 1948, 1961, 1964, 1967, 1970, 1972, 1974, 1976, 1979, 1982, 1985, 1988, 1990, 1993, 1996)\nData&lt;-c(12, 15, 17, 25, 27, 30, 37, 42, 52, 108, 156, 219, 7563, 8642, 7378, 7245, 6983, 7367, 7689, 8503, 7654, 6803, 7204, 7343)\nlength(Yrs); length(Data)\n\n[1] 24\n\n\n[1] 24\n\n#pars, Years, and Numbers are vector arguments\nLogisticGrowthModel&lt;-function(pars, Years, Numbers, ReturnResults=F){\n  \n  print(Years)\n  \n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]#assign parameters\n  \n  TT&lt;-Years[1]:Years[length(Years)] #stretch out years within model so no missing years\n\n  print(TT)\n  \n  PP&lt;-vector(mode = \"numeric\", length = length(TT));  #empty population size vector\n  \n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); #empty population growth vector\n  \n  II&lt;-rr*BB*(1-BB/KK) #initial growth to reach year 1 from year 0\n  \n  PP[1]&lt;-BB+II # population assignment at year 1, based on parameter BB (initial population)\n  \n  for(i in 2:length(PP)){ \n    \n    DD[i-1] &lt;- rr*PP[i-1]*(1-PP[i-1]/KK); #this is dN/dt - amount the population grows based\n    #on conditions in the previous time step\n    \n    PP[i] &lt;- PP[i-1] + DD[i-1] #update population size\n    \n    } \n\n  print(DD)\n  print(PP)\n  \n   Predictions&lt;-data.frame(Years = TT, Pred = PP); #create predictions\n   \n   Observations&lt;-data.frame(Years, Numbers) #store observations in data frame\n   \n   Results&lt;-merge(Predictions, Observations, by = \"Years\") #match Predictions and Observations by Year\n  \n   SS&lt;-sum((Results$Numbers - Results$Pred)^2) #calculate sum of squares: sum((obs - pred)^2)\n  \n   print('Results')\n   print(Results)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(SS)}\n   #controls options for what is returned from function\n\n   }\n\n#LogisticGrowthModel(c(7500,0.5), Yrs, Data, t=0, ReturnResults=T) ERROR\nLogisticGrowthModel(c(7500,0.5,1), Years=Yrs, Numbers=Data, ReturnResults=T)\n\n [1] 1903 1905 1907 1926 1928 1932 1935 1939 1946 1948 1961 1964 1967 1970 1972\n[16] 1974 1976 1979 1982 1985 1988 1990 1993 1996\n [1] 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917\n[16] 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932\n[31] 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947\n[46] 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962\n[61] 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977\n[76] 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992\n[91] 1993 1994 1995 1996\n [1] 7.498167e-01 1.124538e+00 1.686385e+00 2.528629e+00 3.790811e+00\n [6] 5.681422e+00 8.511361e+00 1.274285e+01 1.906000e+01 2.846841e+01\n[11] 4.243079e+01 6.304051e+01 9.321762e+01 1.368693e+02 1.988625e+02\n[16] 2.844839e+02 3.978030e+02 5.381664e+02 6.944762e+02 8.391165e+02\n[21] 9.280905e+02 9.171569e+02 7.935266e+02 5.960632e+02 3.925182e+02\n[26] 2.326112e+02 1.281541e+02 6.752262e+01 3.469589e+01 1.759173e+01\n[31] 8.858151e+00 4.444818e+00 2.226367e+00 1.114176e+00 5.573361e-01\n[36] 2.787302e-01 1.393806e-01 6.969421e-02 3.484807e-02 1.742428e-02\n[41] 8.712201e-03 4.356116e-03 2.178062e-03 1.089032e-03 5.445161e-04\n[46] 2.722581e-04 1.361291e-04 6.806454e-05 3.403227e-05 1.701614e-05\n[51] 8.508068e-06 4.254034e-06 2.127016e-06 1.063508e-06 5.317542e-07\n[56] 2.658773e-07 1.329387e-07 6.646891e-08 3.323467e-08 1.661754e-08\n[61] 8.308770e-09 4.154177e-09 2.076672e-09 1.038752e-09 5.191680e-10\n[66] 2.597922e-10 1.294798e-10 6.494805e-11 3.289036e-11 1.623701e-11\n[71] 8.326673e-12 4.163336e-12 1.665335e-12 8.326673e-13 4.163336e-13\n[76] 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13\n[81] 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13\n[86] 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13 4.163336e-13\n[91] 4.163336e-13 4.163336e-13 4.163336e-13 0.000000e+00\n [1]    1.499933    2.249750    3.374288    5.060672    7.589301   11.380112\n [7]   17.061534   25.572895   38.315744   57.375743   85.844149  128.274942\n[13]  191.315449  284.533067  421.402329  620.264832  904.748684 1302.551681\n[19] 1840.718129 2535.194312 3374.310788 4302.401296 5219.558149 6013.084739\n[25] 6609.147903 7001.666121 7234.277284 7362.431404 7429.954028 7464.649918\n[31] 7482.241650 7491.099801 7495.544620 7497.770987 7498.885162 7499.442498\n[37] 7499.721228 7499.860609 7499.930303 7499.965151 7499.982576 7499.991288\n[43] 7499.995644 7499.997822 7499.998911 7499.999455 7499.999728 7499.999864\n[49] 7499.999932 7499.999966 7499.999983 7499.999991 7499.999996 7499.999998\n[55] 7499.999999 7499.999999 7500.000000 7500.000000 7500.000000 7500.000000\n[61] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[67] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[73] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[79] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[85] 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000 7500.000000\n[91] 7500.000000 7500.000000 7500.000000 7500.000000\n[1] \"Results\"\n   Years        Pred Numbers\n1   1903    1.499933      12\n2   1905    3.374288      15\n3   1907    7.589301      17\n4   1926 6013.084739      25\n5   1928 7001.666121      27\n6   1932 7464.649918      30\n7   1935 7495.544620      37\n8   1939 7499.721228      42\n9   1946 7499.997822      52\n10  1948 7499.999455     108\n11  1961 7500.000000     156\n12  1964 7500.000000     219\n13  1967 7500.000000    7563\n14  1970 7500.000000    8642\n15  1972 7500.000000    7378\n16  1974 7500.000000    7245\n17  1976 7500.000000    6983\n18  1979 7500.000000    7367\n19  1982 7500.000000    7689\n20  1985 7500.000000    8503\n21  1988 7500.000000    7654\n22  1990 7500.000000    6803\n23  1993 7500.000000    7204\n24  1996 7500.000000    7343\n\n\n[[1]]\n   Years        Pred\n1   1903    1.499933\n2   1904    2.249750\n3   1905    3.374288\n4   1906    5.060672\n5   1907    7.589301\n6   1908   11.380112\n7   1909   17.061534\n8   1910   25.572895\n9   1911   38.315744\n10  1912   57.375743\n11  1913   85.844149\n12  1914  128.274942\n13  1915  191.315449\n14  1916  284.533067\n15  1917  421.402329\n16  1918  620.264832\n17  1919  904.748684\n18  1920 1302.551681\n19  1921 1840.718129\n20  1922 2535.194312\n21  1923 3374.310788\n22  1924 4302.401296\n23  1925 5219.558149\n24  1926 6013.084739\n25  1927 6609.147903\n26  1928 7001.666121\n27  1929 7234.277284\n28  1930 7362.431404\n29  1931 7429.954028\n30  1932 7464.649918\n31  1933 7482.241650\n32  1934 7491.099801\n33  1935 7495.544620\n34  1936 7497.770987\n35  1937 7498.885162\n36  1938 7499.442498\n37  1939 7499.721228\n38  1940 7499.860609\n39  1941 7499.930303\n40  1942 7499.965151\n41  1943 7499.982576\n42  1944 7499.991288\n43  1945 7499.995644\n44  1946 7499.997822\n45  1947 7499.998911\n46  1948 7499.999455\n47  1949 7499.999728\n48  1950 7499.999864\n49  1951 7499.999932\n50  1952 7499.999966\n51  1953 7499.999983\n52  1954 7499.999991\n53  1955 7499.999996\n54  1956 7499.999998\n55  1957 7499.999999\n56  1958 7499.999999\n57  1959 7500.000000\n58  1960 7500.000000\n59  1961 7500.000000\n60  1962 7500.000000\n61  1963 7500.000000\n62  1964 7500.000000\n63  1965 7500.000000\n64  1966 7500.000000\n65  1967 7500.000000\n66  1968 7500.000000\n67  1969 7500.000000\n68  1970 7500.000000\n69  1971 7500.000000\n70  1972 7500.000000\n71  1973 7500.000000\n72  1974 7500.000000\n73  1975 7500.000000\n74  1976 7500.000000\n75  1977 7500.000000\n76  1978 7500.000000\n77  1979 7500.000000\n78  1980 7500.000000\n79  1981 7500.000000\n80  1982 7500.000000\n81  1983 7500.000000\n82  1984 7500.000000\n83  1985 7500.000000\n84  1986 7500.000000\n85  1987 7500.000000\n86  1988 7500.000000\n87  1989 7500.000000\n88  1990 7500.000000\n89  1991 7500.000000\n90  1992 7500.000000\n91  1993 7500.000000\n92  1994 7500.000000\n93  1995 7500.000000\n94  1996 7500.000000\n\n[[2]]\n[1] 471423138\n\n\nNow we would like to fit this model by minimizing the objective function using algorithms implemented in the ‚Äòoptim‚Äô function.\n\n?optim # this function tries different parameter values to minimize the output of \n#LogisticGrowthModel. The output is the sum of squares.\n\nOptPars1&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000))\nOptPars1\n\nOptPars2&lt;-optim(c(7500,0.5,1), LogisticGrowthModel, Years = Yrs, Numbers = Data, control = list(maxit = 50000))\nOptPars2\n\nPredictions1&lt;-LogisticGrowthModel(OptPars1$par, Yrs, Data, ReturnResults=T)[[1]]\nPredictions2&lt;-LogisticGrowthModel(OptPars2$par, Yrs, Data, ReturnResults=T)[[1]]\n\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), ylab = '', xlab = '')\npar(new = T)\nplot(Pred~Years, data = Predictions1, type = \"l\", ylim = c(0,8000))\nlines(Pred~Years, data = Predictions2, lty = 2)\n\n# Now try to figure out the best time lag as shown in the on-line example. \n# Hint: plot SS as it relates to the time lag value. SS is your sums of \n# squares, so the value with the lowest sum of squares is the one you want. \n# Try lag values of 0 - 5. To include a lag, use the next function instead. \n# Note that there is now an additional parameter for lag t.\n\nLogisticGrowthModelLag&lt;-function(pars, Years, Numbers, t=0, ReturnResults=F){\n  KK&lt;-pars[1]; rr&lt;-pars[2]; BB&lt;-pars[3]\n  TT&lt;-Years[1]:Years[length(Years)]\n  PP&lt;-vector(mode = \"numeric\", length = length(TT));\n  DD&lt;-vector(mode = \"numeric\", length = length(TT)); \n  II&lt;-rr*BB*(1-BB/KK)\n  PP[1]&lt;-BB+II\n  \n  #implementing Logistic Growth for Years with no lagged Numbers data\n  if(t&gt;0){\n    for(i in 2:(t+1)){ \n      DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1]/KK);\n      PP[i]&lt;-PP[i-1] + DD[i-1]\n    }\n  }\n  \n  #implementing Lagged growth\n  for(i in (t+2):length(PP)){ \n    DD[i-1]&lt;- rr*PP[i-1]*(1-PP[i-1-t]/KK); \n    PP[i]&lt;-PP[i-1] + DD[i-1]\n  }\n  \n  #print(PP) \n  Predictions&lt;-data.frame(Years = TT, Pred = PP); Observations&lt;-data.frame(Years, Numbers)\n  Results&lt;-merge(Predictions, Observations, by = \"Years\")\n  SS&lt;-sum((Results$Numbers - Results$Pred)^2)\n  if(ReturnResults==T){ return(list(Predictions, SS))} else {return(ifelse(is.infinite(SS), 10000000000, SS))}\n}\n\nLogisticGrowthModelLag(OptPars1$par, t=3,Yrs, Data, ReturnResults=T)\nLogisticGrowthModelLag(OptPars2$par, t=3,Yrs, Data, ReturnResults=T)\n\nSS1&lt;-SS2&lt;-vector(mode = \"numeric\", length = 6)\nOptPars1_list&lt;-OptPars2_list&lt;-NULL\n\nfor(i in 0:6){\n  OptPars1_list[[i+1]]&lt;-try(optim(OptPars1$par, fn=LogisticGrowthModelLag, t=i, method = 'L-BFGS-B', Years = Yrs, Numbers = Data, lower = c(2000, 0,0.8), upper = c(10000, 1,100), control = list(maxit = 50000)), silent=T)\n  OptPars2_list[[i+1]]&lt;-try(optim(OptPars2$par, fn=LogisticGrowthModelLag, t=i, Years = Yrs, Numbers = Data, control = list(maxit = 50000)),silent=T)\n  SS1[i+1]&lt;-try(OptPars1_list[[i+1]]$value/1000000,silent=T )\n  SS2[i+1]&lt;-try(OptPars2_list[[i+1]]$value/1000000,silent=T )\n}\n\nSS1 \nSS2\n\n\n#Q - The time lag corresponding with the minimum SS is the best. What is time lag is that?\n#That is, which would you choose if the best model by least squares (SS)?\n\npar(mfrow = c(1,2))\nplot(SS1~c(0:6))\nplot(SS2~c(0:6))\n\n#Q - BUT, this is what happens with lag = 5. Why? (Compare plots with lag = 0 - 4)\npar(mfrow = c(1,3))\n\nOptPars&lt;-NULL\n\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=5, Years = Yrs, Numbers = Data)\n\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=5,Yrs, Data, ReturnResults=T)[[1]]\n\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), xlab = '', ylab = '')\n\npar(new = T)\n\nplot(Pred~Years, data = Predictions, type = \"l\", ylim = c(0,8000))\n\nmtext('Lag 5')\n\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars1_list[[6]]$par), t=6-1,Yrs, Data, ReturnResults=T)[[1]]\n\nlines(Pred~Years, data = Predictions, lty = 2)\n\nLogisticGrowthModelLag(OptPars2$par, t=5,Yrs, Data, ReturnResults=T)\n\n\n#...compared with lag = 3\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=3, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=3,Yrs, Data, ReturnResults=T)[[1]]\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), xlab = '', ylab = '')\npar(new = T)\nplot(Pred~Years, data = Predictions, type = \"l\", ylim = c(0,8000))\nmtext('Lag 3')\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars1_list[[4]]$par), t=4-1,Yrs, Data, ReturnResults=T)[[1]]\nlines(Pred~Years, data = Predictions, lty = 2)\n\n\n#...and no lag = 0\nOptPars&lt;-NULL\nOptPars&lt;-optim(c(7500,0.5,1), LogisticGrowthModelLag, t=0, Years = Yrs, Numbers = Data)\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars$par), t=0,Yrs, Data, ReturnResults=T)[[1]]\nplot(Data~Yrs, pch = \"*\", col = \"red\", ylim = c(0,8000), xlab = '', ylab = '')\npar(new = T)\nplot(Pred~Years, data = Predictions, type = \"l\", ylim = c(0,8000))\nmtext('Lag 0')\nPredictions&lt;-LogisticGrowthModelLag(c(OptPars1_list[[1]]$par), t=1-1,Yrs, Data, ReturnResults=T)[[1]]\nlines(Pred~Years, data = Predictions, lty = 2)\n\n\n#Q - Which one fits the data best?\n\n\n#TAKE HOME MESSAGES\n#- the optimizer matters\n#- the starting values matter\n#- the biology matters"
  },
  {
    "objectID": "Fitting_logistic_curve.html#further-considerations",
    "href": "Fitting_logistic_curve.html#further-considerations",
    "title": "Gr√≥-FTP - Stock assessment line - Simple pop. models",
    "section": "",
    "text": "What biological and ecological factors contribute to r and K?\n\n\n\nWhat population characteristic is the most important for natural resource management?\nHow to detect this without biological/ecological data?\nDraw a graph of how population growth rate (\\(dN/dt\\)) changes over time for the case studies of 1) exponential and 2) logistic growth. How does the density dependent term \\((1-N/K)\\) change over time?\nWhat species would be difficult to represent with this?"
  },
  {
    "objectID": "Review-what are models.html",
    "href": "Review-what are models.html",
    "title": "Gr√≥-FTP - Stock assessment line - what are models?",
    "section": "",
    "text": "Models are only a representation and simplification of reality. Population dynamics models represent how populations grow or decline over time. They can be parameterized by fitting models to available data.\nIn natural resource management of renewable resources, predictions from population dynamics models often used to provide advice for near-term removals from a population.\nA wide variety of models and types of information can be included, but here we start with some of the most basic models. These are also often used when little detail is known about the population.\n\n\nParameterization is the process of choosing values for parameters in an equation. For example, in a line equation:\n\\(y = a + bx\\) ,\nthe intercept parameter \\(a\\) and the slope parameter \\(b\\) can be chosen to be certain values. For example, what value of \\(y\\) results from \\(a=0\\) and \\(b=1\\)? Any two values can be used to create a single line that yields a given set of \\(x\\) and \\(y\\) values. Plotting these yields a line:\n\na &lt;- 0; b&lt;-1; x&lt;-1:10; y&lt;- a + b*x\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nThis line is an example of a mathematical model:\n\nknitr::include_graphics('imgs/Mathematical_Models.png')\n\n\n\n\n\n\n\n\nWith a linear functional form:\n\nknitr::include_graphics('imgs/Linear_Models.png')\n\n\n\n\n\n\n\n\nOther functional forms are possible. For example, can you find an equation that roughly represents this process?\n\nknitr::include_graphics('imgs/Wave_model.png')\n\n\n\n\n\n\n\n\nThis is one option, but others exist:\n\nx &lt;- seq(0,3*pi,pi/10); y &lt;- sin(x)\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nAnother kind of model could describe circular processes:\n\nknitr::include_graphics('imgs/Circular_model.png')\n\n\n\n\n\n\n\n\nMathematical models therefore can have a variety of forms, with specific parameterizations defining a certain model‚Äôs exact shape.\n\n\n\nInstead of describing operational linkages, statistical models describe variation surrounding a central process. The generation of this variation is often itself described of as a process, but it is instead a process describing how chance results in certain numbers more frequently than other numbers, given the type of process. The result of these processes are distributions around a central process, such as a binomial, Poisson, Gaussian (normal), or lognormal distributions.\n\nknitr::include_graphics('imgs/Statistical models.png')\n\n\n\n\n\n\n\n\nDistributions are well-used in the study of biological processes, because they are notoriously imperfect and therefore partially controlled by chance. Growth for example, can be partially determined by age, but is also simply the result of variation:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(age, size)) + geom_point() + geom_line(aes(age, pred_size))\n\n\n\n\n\n\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(size - pred_size)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nIf we do not know the process behind a pattern observed in nature, how do we best describe the process using parameters, or parameterize our model?\nThis is done using statistical model fitting. For example:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntest.lm &lt;- lm(size~age)\nsummary(test.lm)\n\n\nCall:\nlm(formula = size ~ age)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.5942  -5.2334  -0.3966   6.3316  24.3620 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.73259    2.00460   2.361     0.02 *  \nage          0.92313    0.02934  31.465   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.771 on 108 degrees of freedom\nMultiple R-squared:  0.9016,    Adjusted R-squared:  0.9007 \nF-statistic:   990 on 1 and 108 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that the fitting process accurately achieved our simulated parameters? How did it do that? It used a fitting algorithm to find the line that best fit all data points simultaneously. This is done by minimizing the objective function.\nA general form for an algorithm goes something like:\n1 - Pick or jump to new values for parameters (e.g., \\(a\\) and \\(b\\) in a linear model).\n2- Calculate predictions\n3 - Evaluate the objection function, which is usually a statistic representing model fit (e.g., sum of squares or likelihood function)\n4 - Repeat 1-3, then compare the fit to the previous value.\n5 - If the fit is better, keep new parameter values. If worse, keep old values. Some algorithms add an element of chance here to avoid local minima (e.g., keep the better values only 90% of the time)\n6 - Repeat until no better answer can be found (i.e., essentially the same answer is found again and again, indicating convergence)\n\n\nBy comparison, let‚Äôs try to do something similar by hand:\n\n\n####Exercise 3 - fit linear model----###\nline_f&lt;-function(b, a = 0, x = xx){\n  y&lt;-a + b*x\n  return(y)\n}\n\n\nset.seed(100)\n?rnorm\nrnorm(100)\n\n  [1] -0.50219235  0.13153117 -0.07891709  0.88678481  0.11697127  0.31863009\n  [7] -0.58179068  0.71453271 -0.82525943 -0.35986213  0.08988614  0.09627446\n [13] -0.20163395  0.73984050  0.12337950 -0.02931671 -0.38885425  0.51085626\n [19] -0.91381419  2.31029682 -0.43808998  0.76406062  0.26196129  0.77340460\n [25] -0.81437912 -0.43845057 -0.72022155  0.23094453 -1.15772946  0.24707599\n [31] -0.09111356  1.75737562 -0.13792961 -0.11119350 -0.69001432 -0.22179423\n [37]  0.18290768  0.41732329  1.06540233  0.97020202 -0.10162924  1.40320349\n [43] -1.77677563  0.62286739 -0.52228335  1.32223096 -0.36344033  1.31906574\n [49]  0.04377907 -1.87865588 -0.44706218 -1.73859795  0.17886485  1.89746570\n [55] -2.27192549  0.98046414 -1.39882562  1.82487242  1.38129873 -0.83885188\n [61] -0.26199577 -0.06884403 -0.37888356  2.58195893  0.12983414 -0.71302498\n [67]  0.63799424  0.20169159 -0.06991695 -0.09248988  0.44890327 -1.06435567\n [73] -1.16241932  1.64852175 -2.06209602  0.01274972 -1.08752835  0.27053949\n [79]  1.00845187 -2.07440475  0.89682227 -0.04999577 -1.34534931 -1.93121153\n [85]  0.70958158 -0.15790503  0.21636787  0.81736208  1.72717575 -0.10377029\n [91] -0.55712229  1.42830143 -0.89295740 -1.15757124 -0.53029645  2.44568276\n [97] -0.83249580  0.41351985 -1.17868314 -1.17403476\n\nhist(rnorm(100))\n\n\n\n\n\n\n\nxx &lt;- seq(0.5,3.5,0.5)\nline_f(xx)\n\n[1]  0.25  1.00  2.25  4.00  6.25  9.00 12.25\n\nobs &lt;- 0 + 2.3*xx + rnorm(length(xx))\n\n\npred&lt;-line_f(1)\n\npar(mfrow = c(1,1))\nplot(obs~xx)\nlines(pred~xx)\n\n\n\n\n\n\n\nSS1&lt;- sum((obs-pred)^2)\n\npred&lt;-line_f(1.5)\nSS1.5&lt;- sum((obs-pred)^2)\npred1.5&lt;-line_f(1.5)\n\npred&lt;-line_f(2)\nSS2&lt;- sum((obs-pred)^2)\npred2&lt;-line_f(2)\n\npred&lt;-line_f(2.5)\nSS2.5&lt;- sum((obs-pred)^2)\npred2.5&lt;-line_f(2.5)\n\npar(mfrow = c(1,1))\nplot(obs~xx)\nlines(pred1.5~xx)\nlines(pred2~xx, col = 2)\nlines(pred2.5~xx, col = 3)\n\n\n\n\n\n\n\npred&lt;-line_f( 3)\nSS3&lt;- sum((obs-pred)^2)\n\nSS1; SS1.5; SS2; SS2.5; SS3\n\n[1] 61.30771\n\n\n[1] 25.07943\n\n\n[1] 6.351145\n\n\n[1] 5.122861\n\n\n[1] 21.39458\n\nplot(c(SS1, SS1.5, SS2, SS2.5, SS3)~c(1, 1.5, 2, 2.5, 3), type = 'l')\n\n\n\n\n\n\n\nSS_f&lt;-function(b, a = 0, obs1 = obs){\n  pred&lt;-line_f(b)\n  SS&lt;- sum((obs1-pred)^2)\n  return(SS)\n}\n\nSS_f(2.5)\n\n[1] 5.122861\n\n\nThere must be a better way to do this‚Ä¶ try https://www.r-bloggers.com/2013/04/the-golden-section-search-method-modifying-the-bisection-method-with-the-golden-ratio-for-numerical-optimization/\nhttps://www.youtube.com/watch?v=VBFuqglVW3c\n\nsource('goldensectionsearch.r')\n\ngolden.section.search(SS_f, 1, 3, 0.00001)\n\n \nIteration # 1 \nf1 = 13.01273 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 3 \nNew Lower Bound = 1.763932 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.527864 \n \nIteration # 2 \nf1 = 3.590522 \nf2 = 5.569206 \nf2 &gt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 1.763932 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.055728 \n \nIteration # 3 \nf1 = 5.3477 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 2.055728 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.347524 \n \nIteration # 4 \nf1 = 3.590522 \nf2 = 3.642812 \nf2 &gt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.055728 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.167184 \n \nIteration # 5 \nf1 = 3.992991 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.167184 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.27864 \n \nIteration # 6 \nf1 = 3.590522 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.236068 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.304952 \n \nIteration # 7 \nf1 = 3.507856 \nf2 = 3.5202 \nf2 &gt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.236068 \nNew Upper Test Point =  2.27864 \nNew Lower Test Point =  2.262379 \n \nIteration # 8 \nf1 = 3.524456 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.262379 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.28869 \n \nIteration # 9 \nf1 = 3.507856 \nf2 = 3.506851 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.27864 \nNew Lower Test Point =  2.28869 \nNew Upper Test Point =  2.294902 \n \nIteration # 10 \nf1 = 3.506851 \nf2 = 3.509765 \nf2 &gt; f1 \nNew Upper Bound = 2.294902 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.28869 \nNew Lower Test Point =  2.284852 \n \nIteration # 11 \nf1 = 3.5064 \nf2 = 3.506851 \nf2 &gt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.282479 \n \nIteration # 12 \nf1 = 3.506637 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.282479 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.286318 \n \nIteration # 13 \nf1 = 3.5064 \nf2 = 3.50645 \nf2 &gt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.282479 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.283945 \n \nIteration # 14 \nf1 = 3.506444 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.283945 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285412 \n \nIteration # 15 \nf1 = 3.5064 \nf2 = 3.506401 \nf2 &gt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.283945 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.284506 \n \nIteration # 16 \nf1 = 3.50641 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284506 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285066 \n \nIteration # 17 \nf1 = 3.5064 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284852 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285198 \n \nIteration # 18 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284852 \nNew Upper Test Point =  2.285066 \nNew Lower Test Point =  2.284984 \n \nIteration # 19 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284984 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285116 \n \nIteration # 20 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.285066 \nNew Lower Test Point =  2.285116 \nNew Upper Test Point =  2.285147 \n \nIteration # 21 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285147 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285116 \nNew Lower Test Point =  2.285097 \n \nIteration # 22 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285085 \n \nIteration # 23 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285085 \nNew Lower Test Point =  2.285097 \nNew Upper Test Point =  2.285104 \n \nIteration # 24 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285104 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285092 \n \nIteration # 25 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285092 \nNew Lower Test Point =  2.285089 \n \nIteration # 26 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285089 \nNew Lower Test Point =  2.285092 \nNew Upper Test Point =  2.285094 \n \nFinal Lower Bound = 2.285089 \nFinal Upper Bound = 2.285097 \nEstimated Minimizer = 2.285093 \n\n#Check out optim function\n?optim\n\nAnd for more complicated problems with many parameters, there are more complex algorithms, e.g.:\nhttps://en.wikipedia.org/wiki/Simulated_annealing\n\n\n\n\nUnderstanding the basics of programming, mathematical modeling, and statistical model fitting are not only the basis to stock assessment, but also yields other transferable skills.\n\nknitr::include_graphics('imgs/Transferable_skills.png')\n\n\n\n\n\n\n\n\nThis process can be frustrating as you learn debugging, or quickly learning how to find your own mistakes. Some things to remember as you get better at this:\n\nknitr::include_graphics('imgs/Debugging.png')\n\n\n\n\n\n\n\n\n\n\nThis text came with the quarto package. Can you fit linear models to each of the three groups of penguins and add the lines to the plots?\n\n\n\n\n\nThe penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\nThe plot below shows the relationship between flipper and bill lengths of these penguins."
  },
  {
    "objectID": "Review-what are models.html#parameterization",
    "href": "Review-what are models.html#parameterization",
    "title": "Gr√≥-FTP - Stock assessment line - what are models?",
    "section": "",
    "text": "Parameterization is the process of choosing values for parameters in an equation. For example, in a line equation:\n\\(y = a + bx\\) ,\nthe intercept parameter \\(a\\) and the slope parameter \\(b\\) can be chosen to be certain values. For example, what value of \\(y\\) results from \\(a=0\\) and \\(b=1\\)? Any two values can be used to create a single line that yields a given set of \\(x\\) and \\(y\\) values. Plotting these yields a line:\n\na &lt;- 0; b&lt;-1; x&lt;-1:10; y&lt;- a + b*x\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nThis line is an example of a mathematical model:\n\nknitr::include_graphics('imgs/Mathematical_Models.png')\n\n\n\n\n\n\n\n\nWith a linear functional form:\n\nknitr::include_graphics('imgs/Linear_Models.png')\n\n\n\n\n\n\n\n\nOther functional forms are possible. For example, can you find an equation that roughly represents this process?\n\nknitr::include_graphics('imgs/Wave_model.png')\n\n\n\n\n\n\n\n\nThis is one option, but others exist:\n\nx &lt;- seq(0,3*pi,pi/10); y &lt;- sin(x)\n\ntibble(x,y) %&gt;% ggplot(aes(x,y)) + geom_line()\n\n\n\n\n\n\n\n\nAnother kind of model could describe circular processes:\n\nknitr::include_graphics('imgs/Circular_model.png')\n\n\n\n\n\n\n\n\nMathematical models therefore can have a variety of forms, with specific parameterizations defining a certain model‚Äôs exact shape."
  },
  {
    "objectID": "Review-what are models.html#statistical-models",
    "href": "Review-what are models.html#statistical-models",
    "title": "Gr√≥-FTP - Stock assessment line - what are models?",
    "section": "",
    "text": "Instead of describing operational linkages, statistical models describe variation surrounding a central process. The generation of this variation is often itself described of as a process, but it is instead a process describing how chance results in certain numbers more frequently than other numbers, given the type of process. The result of these processes are distributions around a central process, such as a binomial, Poisson, Gaussian (normal), or lognormal distributions.\n\nknitr::include_graphics('imgs/Statistical models.png')\n\n\n\n\n\n\n\n\nDistributions are well-used in the study of biological processes, because they are notoriously imperfect and therefore partially controlled by chance. Growth for example, can be partially determined by age, but is also simply the result of variation:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(age, size)) + geom_point() + geom_line(aes(age, pred_size))\n\n\n\n\n\n\n\ntibble(age, size, pred_size) %&gt;% ggplot(aes(size - pred_size)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Review-what are models.html#fitting",
    "href": "Review-what are models.html#fitting",
    "title": "Gr√≥-FTP - Stock assessment line - what are models?",
    "section": "",
    "text": "If we do not know the process behind a pattern observed in nature, how do we best describe the process using parameters, or parameterize our model?\nThis is done using statistical model fitting. For example:\n\nage &lt;- 6:115; pred_size &lt;- 6:115; size &lt;-pred_size + rnorm(110, sd = 10)\n\ntest.lm &lt;- lm(size~age)\nsummary(test.lm)\n\n\nCall:\nlm(formula = size ~ age)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.5942  -5.2334  -0.3966   6.3316  24.3620 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.73259    2.00460   2.361     0.02 *  \nage          0.92313    0.02934  31.465   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.771 on 108 degrees of freedom\nMultiple R-squared:  0.9016,    Adjusted R-squared:  0.9007 \nF-statistic:   990 on 1 and 108 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that the fitting process accurately achieved our simulated parameters? How did it do that? It used a fitting algorithm to find the line that best fit all data points simultaneously. This is done by minimizing the objective function.\nA general form for an algorithm goes something like:\n1 - Pick or jump to new values for parameters (e.g., \\(a\\) and \\(b\\) in a linear model).\n2- Calculate predictions\n3 - Evaluate the objection function, which is usually a statistic representing model fit (e.g., sum of squares or likelihood function)\n4 - Repeat 1-3, then compare the fit to the previous value.\n5 - If the fit is better, keep new parameter values. If worse, keep old values. Some algorithms add an element of chance here to avoid local minima (e.g., keep the better values only 90% of the time)\n6 - Repeat until no better answer can be found (i.e., essentially the same answer is found again and again, indicating convergence)\n\n\nBy comparison, let‚Äôs try to do something similar by hand:\n\n\n####Exercise 3 - fit linear model----###\nline_f&lt;-function(b, a = 0, x = xx){\n  y&lt;-a + b*x\n  return(y)\n}\n\n\nset.seed(100)\n?rnorm\nrnorm(100)\n\n  [1] -0.50219235  0.13153117 -0.07891709  0.88678481  0.11697127  0.31863009\n  [7] -0.58179068  0.71453271 -0.82525943 -0.35986213  0.08988614  0.09627446\n [13] -0.20163395  0.73984050  0.12337950 -0.02931671 -0.38885425  0.51085626\n [19] -0.91381419  2.31029682 -0.43808998  0.76406062  0.26196129  0.77340460\n [25] -0.81437912 -0.43845057 -0.72022155  0.23094453 -1.15772946  0.24707599\n [31] -0.09111356  1.75737562 -0.13792961 -0.11119350 -0.69001432 -0.22179423\n [37]  0.18290768  0.41732329  1.06540233  0.97020202 -0.10162924  1.40320349\n [43] -1.77677563  0.62286739 -0.52228335  1.32223096 -0.36344033  1.31906574\n [49]  0.04377907 -1.87865588 -0.44706218 -1.73859795  0.17886485  1.89746570\n [55] -2.27192549  0.98046414 -1.39882562  1.82487242  1.38129873 -0.83885188\n [61] -0.26199577 -0.06884403 -0.37888356  2.58195893  0.12983414 -0.71302498\n [67]  0.63799424  0.20169159 -0.06991695 -0.09248988  0.44890327 -1.06435567\n [73] -1.16241932  1.64852175 -2.06209602  0.01274972 -1.08752835  0.27053949\n [79]  1.00845187 -2.07440475  0.89682227 -0.04999577 -1.34534931 -1.93121153\n [85]  0.70958158 -0.15790503  0.21636787  0.81736208  1.72717575 -0.10377029\n [91] -0.55712229  1.42830143 -0.89295740 -1.15757124 -0.53029645  2.44568276\n [97] -0.83249580  0.41351985 -1.17868314 -1.17403476\n\nhist(rnorm(100))\n\n\n\n\n\n\n\nxx &lt;- seq(0.5,3.5,0.5)\nline_f(xx)\n\n[1]  0.25  1.00  2.25  4.00  6.25  9.00 12.25\n\nobs &lt;- 0 + 2.3*xx + rnorm(length(xx))\n\n\npred&lt;-line_f(1)\n\npar(mfrow = c(1,1))\nplot(obs~xx)\nlines(pred~xx)\n\n\n\n\n\n\n\nSS1&lt;- sum((obs-pred)^2)\n\npred&lt;-line_f(1.5)\nSS1.5&lt;- sum((obs-pred)^2)\npred1.5&lt;-line_f(1.5)\n\npred&lt;-line_f(2)\nSS2&lt;- sum((obs-pred)^2)\npred2&lt;-line_f(2)\n\npred&lt;-line_f(2.5)\nSS2.5&lt;- sum((obs-pred)^2)\npred2.5&lt;-line_f(2.5)\n\npar(mfrow = c(1,1))\nplot(obs~xx)\nlines(pred1.5~xx)\nlines(pred2~xx, col = 2)\nlines(pred2.5~xx, col = 3)\n\n\n\n\n\n\n\npred&lt;-line_f( 3)\nSS3&lt;- sum((obs-pred)^2)\n\nSS1; SS1.5; SS2; SS2.5; SS3\n\n[1] 61.30771\n\n\n[1] 25.07943\n\n\n[1] 6.351145\n\n\n[1] 5.122861\n\n\n[1] 21.39458\n\nplot(c(SS1, SS1.5, SS2, SS2.5, SS3)~c(1, 1.5, 2, 2.5, 3), type = 'l')\n\n\n\n\n\n\n\nSS_f&lt;-function(b, a = 0, obs1 = obs){\n  pred&lt;-line_f(b)\n  SS&lt;- sum((obs1-pred)^2)\n  return(SS)\n}\n\nSS_f(2.5)\n\n[1] 5.122861\n\n\nThere must be a better way to do this‚Ä¶ try https://www.r-bloggers.com/2013/04/the-golden-section-search-method-modifying-the-bisection-method-with-the-golden-ratio-for-numerical-optimization/\nhttps://www.youtube.com/watch?v=VBFuqglVW3c\n\nsource('goldensectionsearch.r')\n\ngolden.section.search(SS_f, 1, 3, 0.00001)\n\n \nIteration # 1 \nf1 = 13.01273 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 3 \nNew Lower Bound = 1.763932 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.527864 \n \nIteration # 2 \nf1 = 3.590522 \nf2 = 5.569206 \nf2 &gt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 1.763932 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.055728 \n \nIteration # 3 \nf1 = 5.3477 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.527864 \nNew Lower Bound = 2.055728 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.347524 \n \nIteration # 4 \nf1 = 3.590522 \nf2 = 3.642812 \nf2 &gt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.055728 \nNew Upper Test Point =  2.236068 \nNew Lower Test Point =  2.167184 \n \nIteration # 5 \nf1 = 3.992991 \nf2 = 3.590522 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.167184 \nNew Lower Test Point =  2.236068 \nNew Upper Test Point =  2.27864 \n \nIteration # 6 \nf1 = 3.590522 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.347524 \nNew Lower Bound = 2.236068 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.304952 \n \nIteration # 7 \nf1 = 3.507856 \nf2 = 3.5202 \nf2 &gt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.236068 \nNew Upper Test Point =  2.27864 \nNew Lower Test Point =  2.262379 \n \nIteration # 8 \nf1 = 3.524456 \nf2 = 3.507856 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.262379 \nNew Lower Test Point =  2.27864 \nNew Upper Test Point =  2.28869 \n \nIteration # 9 \nf1 = 3.507856 \nf2 = 3.506851 \nf2 &lt; f1 \nNew Upper Bound = 2.304952 \nNew Lower Bound = 2.27864 \nNew Lower Test Point =  2.28869 \nNew Upper Test Point =  2.294902 \n \nIteration # 10 \nf1 = 3.506851 \nf2 = 3.509765 \nf2 &gt; f1 \nNew Upper Bound = 2.294902 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.28869 \nNew Lower Test Point =  2.284852 \n \nIteration # 11 \nf1 = 3.5064 \nf2 = 3.506851 \nf2 &gt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.27864 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.282479 \n \nIteration # 12 \nf1 = 3.506637 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.28869 \nNew Lower Bound = 2.282479 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.286318 \n \nIteration # 13 \nf1 = 3.5064 \nf2 = 3.50645 \nf2 &gt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.282479 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.283945 \n \nIteration # 14 \nf1 = 3.506444 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.286318 \nNew Lower Bound = 2.283945 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285412 \n \nIteration # 15 \nf1 = 3.5064 \nf2 = 3.506401 \nf2 &gt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.283945 \nNew Upper Test Point =  2.284852 \nNew Lower Test Point =  2.284506 \n \nIteration # 16 \nf1 = 3.50641 \nf2 = 3.5064 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284506 \nNew Lower Test Point =  2.284852 \nNew Upper Test Point =  2.285066 \n \nIteration # 17 \nf1 = 3.5064 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285412 \nNew Lower Bound = 2.284852 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285198 \n \nIteration # 18 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284852 \nNew Upper Test Point =  2.285066 \nNew Lower Test Point =  2.284984 \n \nIteration # 19 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.284984 \nNew Lower Test Point =  2.285066 \nNew Upper Test Point =  2.285116 \n \nIteration # 20 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285198 \nNew Lower Bound = 2.285066 \nNew Lower Test Point =  2.285116 \nNew Upper Test Point =  2.285147 \n \nIteration # 21 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285147 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285116 \nNew Lower Test Point =  2.285097 \n \nIteration # 22 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285066 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285085 \n \nIteration # 23 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285116 \nNew Lower Bound = 2.285085 \nNew Lower Test Point =  2.285097 \nNew Upper Test Point =  2.285104 \n \nIteration # 24 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285104 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285097 \nNew Lower Test Point =  2.285092 \n \nIteration # 25 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &gt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285085 \nNew Upper Test Point =  2.285092 \nNew Lower Test Point =  2.285089 \n \nIteration # 26 \nf1 = 3.506398 \nf2 = 3.506398 \nf2 &lt; f1 \nNew Upper Bound = 2.285097 \nNew Lower Bound = 2.285089 \nNew Lower Test Point =  2.285092 \nNew Upper Test Point =  2.285094 \n \nFinal Lower Bound = 2.285089 \nFinal Upper Bound = 2.285097 \nEstimated Minimizer = 2.285093 \n\n#Check out optim function\n?optim\n\nAnd for more complicated problems with many parameters, there are more complex algorithms, e.g.:\nhttps://en.wikipedia.org/wiki/Simulated_annealing"
  },
  {
    "objectID": "Review-what are models.html#conclusion",
    "href": "Review-what are models.html#conclusion",
    "title": "Gr√≥-FTP - Stock assessment line - what are models?",
    "section": "",
    "text": "Understanding the basics of programming, mathematical modeling, and statistical model fitting are not only the basis to stock assessment, but also yields other transferable skills.\n\nknitr::include_graphics('imgs/Transferable_skills.png')\n\n\n\n\n\n\n\n\nThis process can be frustrating as you learn debugging, or quickly learning how to find your own mistakes. Some things to remember as you get better at this:\n\nknitr::include_graphics('imgs/Debugging.png')\n\n\n\n\n\n\n\n\n\n\nThis text came with the quarto package. Can you fit linear models to each of the three groups of penguins and add the lines to the plots?"
  },
  {
    "objectID": "Review-what are models.html#meet-the-penguins",
    "href": "Review-what are models.html#meet-the-penguins",
    "title": "Gr√≥-FTP - Stock assessment line - what are models?",
    "section": "",
    "text": "The penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\nThe plot below shows the relationship between flipper and bill lengths of these penguins."
  }
]